{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laraAkg/Data-Science-Project/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aced0048"
      },
      "source": [
        "# Deep Learning for Heavy-Tailed Distributions - Model Training and Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ea2368"
      },
      "source": [
        "## 0. Colab/Drive Setup & Project Paths\n",
        "\n",
        "This code block initializes the project environment and manages file paths, with support for both **Google Colab (with Google Drive)** and **local execution**.\n",
        "\n",
        "**Key Functions:**\n",
        "- **Colab Detection:** Automatically detects whether the notebook is running in a Google Colab environment.\n",
        "- **Google Drive Integration:** Mounts Google Drive when executed in Colab to enable persistent storage.\n",
        "- **Directory Structure:** Defines a central `BASE_DIR` and creates all required subdirectories (e.g. `datasets`, `plots`, `models_tf`, `reports`) to ensure a consistent project structure.\n",
        "- **Folder Reset Logic:** Cleans and recreates selected output directories to guarantee reproducible and clean experiment runs.\n",
        "- **Global Image Configuration:** Sets a standard image size (`IMG_SIZE = 128×128`) used consistently throughout the project.\n",
        "- **Runtime Feedback:** Prints the resolved base directory to verify the execution environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fd779f6",
        "outputId": "6762c71a-9e80-48b0-81a5-277348e94882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[INFO] Entferne alten Ordner: /content/drive/MyDrive/Generated Data for Data science project/models_tf\n",
            "[INFO] Neu erstellt: /content/drive/MyDrive/Generated Data for Data science project/models_tf\n",
            "[INFO] Entferne alten Ordner: /content/drive/MyDrive/Generated Data for Data science project/reports\n",
            "[INFO] Neu erstellt: /content/drive/MyDrive/Generated Data for Data science project/reports\n",
            "BASE_DIR: /content/drive/MyDrive/Generated Data for Data science project\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DEFAULT_PROJECT_DIR = \"MyDrive/Generated Data for Data science project\"\n",
        "BASE_DIR = Path(\"/content/drive\") / DEFAULT_PROJECT_DIR if IN_COLAB else Path(\"./project_outputs\")\n",
        "\n",
        "DATA_DIR   = BASE_DIR / \"datasets\"\n",
        "PLOTS_DIR  = BASE_DIR / \"plots\"\n",
        "META_DIR   = BASE_DIR / \"metadata\"\n",
        "MODELS_DIR = BASE_DIR / \"models_tf\"\n",
        "REPORTS_DIR= BASE_DIR / \"reports\"\n",
        "REAL_DIR   = BASE_DIR / \"real\"\n",
        "BEST_MODEL_PATH = MODELS_DIR / \"best_model_keras.h5\"\n",
        "BEST_MODEL_META = REPORTS_DIR / \"best_model_meta.json\"\n",
        "\n",
        "folders_to_reset = [MODELS_DIR, REPORTS_DIR]\n",
        "\n",
        "for folder in folders_to_reset:\n",
        "    if folder.exists():\n",
        "        print(f\"[INFO] Remove old folders: {folder}\")\n",
        "        shutil.rmtree(folder)\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[INFO] Newly created: {folder}\")\n",
        "\n",
        "for p in [DATA_DIR, PLOTS_DIR, META_DIR, MODELS_DIR, REPORTS_DIR, REAL_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = (128, 128)  # (H, W)\n",
        "\n",
        "print(\"BASE_DIR:\", BASE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Imports & Global Configuration (TensorFlow / Keras)\n",
        "\n",
        "This code block initializes the core libraries and global settings required for model training and evaluation.\n",
        "\n",
        "- **Library Imports:** Loads standard Python libraries as well as NumPy, image processing, visualization, TensorFlow/Keras, and scikit-learn components used throughout the notebook.\n",
        "- **Reproducibility:** Sets a global random seed for Python, NumPy, and TensorFlow to ensure consistent and comparable results across runs.\n",
        "- **Hardware Detection:** Automatically detects whether a GPU is available and selects the appropriate computation device.\n",
        "- **Training Defaults:** Defines global default values for batch size and number of training epochs.\n",
        "- **Runtime Feedback:** Prints the detected TensorFlow device to confirm the execution environment."
      ],
      "metadata": {
        "id": "ShYazSkOXYVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "717edf07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c0cb93-9f6f-474f-e556-98daeb825793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF device: GPU\n"
          ]
        }
      ],
      "source": [
        "import os, json, math, time, random, csv\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "DEVICE = \"GPU\" if len(tf.config.list_physical_devices('GPU'))>0 else \"CPU\"\n",
        "BATCH_SIZE_DEFAULT = 32\n",
        "EPOCHS_DEFAULT = 8\n",
        "\n",
        "print(\"TF device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d23701"
      },
      "source": [
        "## 2. Load Metadata & Build Samples Table (Including Augmentations)\n",
        "\n",
        "This code block loads the previously generated metadata and constructs a unified table of samples used for model training and evaluation.  \n",
        "Each sample represents a set of plots (original and augmented) together with its corresponding label.\n",
        "\n",
        "- **Metadata Loading:** Reads the dataset metadata from the `datasets_index.json` file.\n",
        "- **Sample Construction:** Builds a list of sample entries, including original plots and all available augmentations.\n",
        "- **Label Assignment:** Assigns the corresponding binary label (heavy-tailed or not) to each sample.\n",
        "- **Dataset-Level Tracking:** Creates a list of unique dataset identifiers to enable dataset-level cross-validation in later stages.\n",
        "- **Runtime Statistics:** Prints the number of unique datasets and the total number of samples including augmentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba316e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe489dc5-38db-421c-cff3-1afa6aefa549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique dataset_ids: 600\n",
            "Total samples (incl. augs): 2400\n"
          ]
        }
      ],
      "source": [
        "INDEX_JSON = META_DIR / \"datasets_index.json\"\n",
        "with open(INDEX_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    records = json.load(f)\n",
        "\n",
        "samples = []\n",
        "uniq_rows = []\n",
        "for r in records:\n",
        "    ds_id = r[\"dataset_id\"]\n",
        "    label = int(r[\"heavy_tailed\"])\n",
        "    uniq_rows.append({\"dataset_id\": ds_id, \"label\": label})\n",
        "    samples.append({\"dataset_id\": ds_id, \"variant\": \"original\", \"paths\": r[\"plots\"][\"original\"], \"label\": label})\n",
        "    for aug_name, aug_paths in r[\"plots\"][\"aug\"].items():\n",
        "        samples.append({\"dataset_id\": ds_id, \"variant\": aug_name, \"paths\": aug_paths, \"label\": label})\n",
        "\n",
        "print(\"Unique dataset_ids:\", len(uniq_rows))\n",
        "print(\"Total samples (incl. augs):\", len(samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Image Helpers & TensorFlow Dataset Builder\n",
        "\n",
        "This cell contains helper functions for loading and preprocessing image data (plots) and for creating TensorFlow `Dataset` objects used for training.\n",
        "\n",
        "- **`load_gray_resized`**: Loads an image, converts it to grayscale, resizes it to the global image size, and normalizes pixel values.\n",
        "- **`stack_zipf_qq_me`**: Loads the three plot types (Zipf, QQ, ME) for a sample, converts them to grayscale arrays, and stacks them into a single multi-channel image.\n",
        "- **`sample_to_example`**: Converts a sample entry into the input image tensor (`X`) and its corresponding label (`y`).\n",
        "- **`rows_for_ids`**: Filters sample entries based on a given set of dataset identifiers.\n",
        "- **`make_tf_dataset`**: Creates an optimized TensorFlow `Dataset` from a list of samples, including batching, optional shuffling, and prefetching."
      ],
      "metadata": {
        "id": "EYJa4EtAZlii"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7aff3c0"
      },
      "outputs": [],
      "source": [
        "def load_gray_resized(path):\n",
        "    img = Image.open(path).convert(\"L\")\n",
        "    img = img.resize((IMG_SIZE[1], IMG_SIZE[0]))\n",
        "    arr = np.asarray(img).astype(\"float32\") / 255.0\n",
        "    return arr\n",
        "\n",
        "def stack_zipf_qq_me(paths_dict):\n",
        "    z = load_gray_resized(paths_dict[\"zipf\"])\n",
        "    q = load_gray_resized(paths_dict[\"qq_exp\"])\n",
        "    m = load_gray_resized(paths_dict[\"me\"])\n",
        "    return np.stack([z,q,m], axis=-1)\n",
        "\n",
        "def sample_to_example(row):\n",
        "    x = stack_zipf_qq_me(row[\"paths\"])\n",
        "    y = np.float32(row[\"label\"])\n",
        "    return x, y\n",
        "\n",
        "def rows_for_ids(id_set):\n",
        "    s = set(id_set)\n",
        "    return [row for row in samples if row[\"dataset_id\"] in s]\n",
        "\n",
        "def make_tf_dataset(rows, batch_size=BATCH_SIZE_DEFAULT, shuffle=False):\n",
        "    xs, ys = [], []\n",
        "    for r in rows:\n",
        "        x,y = sample_to_example(r); xs.append(x); ys.append(y)\n",
        "    xs = np.stack(xs, axis=0); ys = np.array(ys, dtype=np.float32)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((xs, ys))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(xs), seed=SEED, reshuffle_each_iteration=True)\n",
        "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Keras Model Architectures (Baseline & Separable CNN)\n",
        "\n",
        "This code block defines the neural network architectures used for binary image classification.  \n",
        "Two model variants are implemented: a standard convolutional **Baseline CNN** and a more parameter-efficient **Separable CNN**. Dropout and L2 regularization are used to reduce overfitting.\n",
        "\n",
        "- **`ConvBlock`**: Defines a convolutional building block consisting of convolution, batch normalization, and ReLU activation.\n",
        "- **`SepConvBlock`**: Defines a separable convolutional block with batch normalization and ReLU activation for improved computational efficiency.\n",
        "- **`build_baseline`**: Constructs the baseline CNN architecture using standard convolutional layers.\n",
        "- **`build_separable`**: Constructs the separable CNN architecture using depthwise separable convolutions.\n",
        "- **Model Output & Regularization:** Both models end with global average pooling, dropout, and a single-unit dense layer for binary classification, with configurable L2 regularization and dropout rates."
      ],
      "metadata": {
        "id": "bPLVi4jVaeJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efc6b0e6"
      },
      "outputs": [],
      "source": [
        "def ConvBlock(x, filters, k=3, s=1, l2=0.0):\n",
        "    x = layers.Conv2D(filters, k, strides=s, padding=\"same\",\n",
        "                      use_bias=False, kernel_regularizer=regularizers.l2(l2))(x)\n",
        "    x = layers.BatchNormalization()(x); x = layers.ReLU()(x); return x\n",
        "\n",
        "def SepConvBlock(x, filters, k=3, s=1, l2=0.0):\n",
        "    x = layers.SeparableConv2D(filters, k, strides=s, padding=\"same\", use_bias=False,\n",
        "                               depthwise_regularizer=regularizers.l2(l2), pointwise_regularizer=regularizers.l2(l2))(x)\n",
        "    x = layers.BatchNormalization()(x); x = layers.ReLU()(x); return x\n",
        "\n",
        "def build_baseline(input_shape=(128,128,3), l2reg=0.0, dropout=0.0):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = ConvBlock(inp, 16, l2=l2reg)\n",
        "    x = ConvBlock(x, 32, s=2, l2=l2reg)\n",
        "    x = ConvBlock(x, 64, s=2, l2=l2reg)\n",
        "    x = ConvBlock(x, 128, s=2, l2=l2reg)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    logit = layers.Dense(1, kernel_regularizer=regularizers.l2(l2reg))(x)\n",
        "    return keras.Model(inp, logit, name=\"CNNBaseline\")\n",
        "\n",
        "def build_separable(input_shape=(128,128,3), l2reg=0.0, dropout=0.0):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = SepConvBlock(inp, 16, l2=l2reg)\n",
        "    x = SepConvBlock(x, 32, s=2, l2=l2reg)\n",
        "    x = SepConvBlock(x, 64, s=2, l2=l2reg)\n",
        "    x = SepConvBlock(x, 128, s=2, l2=l2reg)\n",
        "    x = SepConvBlock(x, 128, s=1, l2=l2reg)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    logit = layers.Dense(1, kernel_regularizer=regularizers.l2(l2reg))(x)\n",
        "    return keras.Model(inp, logit, name=\"CNNSeparable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Training & Evaluation Utilities + Temperature Scaling\n",
        "\n",
        "This cell provides helper functions for training and evaluating models, and implements temperature scaling to improve probability calibration.  \n",
        "It supports both binary and multi-class outputs and includes utilities for threshold tuning and dataset-level evaluation.\n",
        "\n",
        "- **`bce_logits`**: Defines the binary cross-entropy loss with logits (`from_logits=True`) for model compilation.\n",
        "- **`compile_model`**: Compiles a Keras model using the Adam optimizer and the defined loss function.\n",
        "- **`predict_logits`**: Runs inference on a TensorFlow dataset and returns raw logits (pre-sigmoid/softmax) along with the true labels.\n",
        "- **`evaluate_numpy`**: Computes metrics (e.g., accuracy, F1, ROC-AUC, PR-AUC, confusion matrix) from logits and returns logits, probabilities, and labels.\n",
        "- **`evaluate_from_probs`**: Evaluates binary classification metrics starting directly from probabilities (useful after calibration).\n",
        "- **`aggregate_by_dataset`**: Aggregates predictions across image variants (original + augmentations) per `dataset_id` using mean probability.\n",
        "- **`evaluate_dataset_level`**: Computes evaluation metrics after aggregating predictions at the dataset level.\n",
        "- **`find_best_threshold_f1`**: Grid-searches decision thresholds to maximize F1-score on validation data.\n",
        "- **`expected_calibration_error`**: Calculates the Expected Calibration Error (ECE) to measure probability calibration quality.\n",
        "- **`TemperatureScalerTF`**: Implements a lightweight calibration model with a single trainable temperature parameter `T`.\n",
        "- **`fit_temperature_tf`**: Fits the temperature parameter on validation logits to improve calibration and returns the optimal `T`.\n",
        "- **`ternary_from_probs`**: Converts binary probabilities into a three-state decision (negative / uncertain / positive) using `t_low` and `t_high`.\n",
        "- **`evaluate_ternary`**: Evaluates ternary decision behavior, including abstention rate and accuracy on decided samples.\n",
        "- **`find_gray_zone_thresholds`**: Searches for `(t_low, t_high)` thresholds that meet a target precision while minimizing abstention."
      ],
      "metadata": {
        "id": "Tv8801K9a8cr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0faf61bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, precision_score, recall_score\n",
        "\n",
        "bce_logits = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def compile_model(model, lr=1e-3):\n",
        "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=opt, loss=bce_logits)\n",
        "    return model\n",
        "\n",
        "def predict_logits(model, ds):\n",
        "    y_true, logits = [], []\n",
        "    for x, y in ds:\n",
        "        y_true.append(y.numpy())\n",
        "        logit_batch = model(x, training=False)\n",
        "        if logit_batch.shape[-1] == 1:\n",
        "            logit_batch = tf.squeeze(logit_batch, axis=-1)\n",
        "        logits.append(logit_batch.numpy())\n",
        "    logits = np.concatenate(logits)\n",
        "    y_true = np.concatenate(y_true)\n",
        "    return logits, y_true\n",
        "\n",
        "def evaluate_numpy(logits, y_true, threshold=0.5):\n",
        "    if logits.ndim == 1:  # binary\n",
        "        probs = 1/(1+np.exp(-logits))\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        yb = (y_true > 0.5).astype(int)\n",
        "        out = {\n",
        "            \"acc\": float(accuracy_score(yb, preds)),\n",
        "            \"f1\": float(f1_score(yb, preds)),\n",
        "            \"roc_auc\": float(roc_auc_score(yb, probs)) if len(np.unique(yb))>1 else float(\"nan\"),\n",
        "            \"pr_auc\": float(average_precision_score(yb, probs)) if len(np.unique(yb))>1 else float(\"nan\"),\n",
        "            \"cm\": confusion_matrix(yb, preds).tolist(),\n",
        "            \"logits\": logits, \"probs\": probs, \"y_true\": yb\n",
        "        }\n",
        "        return out\n",
        "    else:\n",
        "        y_true_int = y_true.astype(int)\n",
        "        probs = tf.nn.softmax(logits, axis=-1).numpy()\n",
        "        preds = probs.argmax(axis=-1)\n",
        "        out = {\n",
        "            \"acc\": float(accuracy_score(y_true_int, preds)),\n",
        "            \"f1\": float(f1_score(y_true_int, preds, average=\"macro\")),\n",
        "            \"cm\": confusion_matrix(y_true_int, preds).tolist(),\n",
        "            \"logits\": logits, \"probs\": probs, \"y_true\": y_true_int\n",
        "        }\n",
        "        return out\n",
        "\n",
        "def evaluate_from_probs(probs, y_true, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate binary classification metrics starting from probabilities (already sigmoid-ed),\n",
        "    not logits. This is useful for calibrated probabilities.\n",
        "    \"\"\"\n",
        "    probs = np.asarray(probs).ravel()\n",
        "    y = (np.asarray(y_true) > 0.5).astype(int)\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    out = {\n",
        "        \"acc\": float(accuracy_score(y, preds)),\n",
        "        \"f1\": float(f1_score(y, preds)),\n",
        "        \"roc_auc\": float(roc_auc_score(y, probs)) if len(np.unique(y)) > 1 else float(\"nan\"),\n",
        "        \"pr_auc\": float(average_precision_score(y, probs)) if len(np.unique(y)) > 1 else float(\"nan\"),\n",
        "        \"cm\": confusion_matrix(y, preds).tolist(),\n",
        "        \"probs\": probs,\n",
        "        \"y_true\": y,\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "def aggregate_by_dataset(probs, y_true, ds_ids):\n",
        "    \"\"\"\n",
        "    Aggregate probabilities and labels per dataset_id.\n",
        "\n",
        "    For each dataset_id we take:\n",
        "      - mean probability over all its image variants (original + augmentations)\n",
        "      - label: we assume all variants have the same label, so we take the first.\n",
        "    \"\"\"\n",
        "    probs = np.asarray(probs).ravel()\n",
        "    y_true = np.asarray(y_true)\n",
        "    ds_ids = np.asarray(ds_ids)\n",
        "\n",
        "    assert len(probs) == len(y_true) == len(ds_ids), \"Lengths of probs, y_true and ds_ids must match.\"\n",
        "\n",
        "    uniq = np.unique(ds_ids)\n",
        "    probs_ds = []\n",
        "    y_ds = []\n",
        "    for ds in uniq:\n",
        "        m = (ds_ids == ds)\n",
        "        if not np.any(m):\n",
        "            continue\n",
        "        probs_ds.append(probs[m].mean())\n",
        "        y_ds.append(y_true[m][0])\n",
        "\n",
        "    return np.asarray(probs_ds), np.asarray(y_ds)\n",
        "\n",
        "\n",
        "def evaluate_dataset_level(probs, y_true, ds_ids, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compute metrics after aggregating probabilities per dataset_id.\n",
        "    \"\"\"\n",
        "    probs_ds, y_ds = aggregate_by_dataset(probs, y_true, ds_ids)\n",
        "    return evaluate_from_probs(probs_ds, y_ds, threshold=threshold)\n",
        "\n",
        "\n",
        "def find_best_threshold_f1(probs, y_true, thresholds=None):\n",
        "    \"\"\"Grid-search decision threshold between 0.01 and 0.99 to maximize F1 on a validation set.\n",
        "\n",
        "    Returns a dict with the best threshold and the corresponding accuracy, precision,\n",
        "    recall and F1-score. The true labels are assumed to be binary (0/1 or probabilities).\"\"\"\n",
        "    probs = np.asarray(probs).ravel()\n",
        "    y = (np.asarray(y_true) > 0.5).astype(int)\n",
        "\n",
        "    if thresholds is None:\n",
        "        thresholds = np.linspace(0.01, 0.99, 99)\n",
        "\n",
        "    best_f1 = None\n",
        "    best_info = {\n",
        "        \"threshold\": 0.5,\n",
        "        \"precision\": float(\"nan\"),\n",
        "        \"recall\": float(\"nan\"),\n",
        "        \"f1\": float(\"nan\"),\n",
        "        \"acc\": float(\"nan\"),\n",
        "    }\n",
        "\n",
        "    for t in thresholds:\n",
        "        preds = (probs >= t).astype(int)\n",
        "        prec = precision_score(y, preds, zero_division=0)\n",
        "        rec = recall_score(y, preds, zero_division=0)\n",
        "        f1 = f1_score(y, preds, zero_division=0)\n",
        "        acc = accuracy_score(y, preds)\n",
        "\n",
        "        if (best_f1 is None) or (f1 > best_f1):\n",
        "            best_f1 = f1\n",
        "            best_info = {\n",
        "                \"threshold\": float(t),\n",
        "                \"precision\": float(prec),\n",
        "                \"recall\": float(rec),\n",
        "                \"f1\": float(f1),\n",
        "                \"acc\": float(acc),\n",
        "            }\n",
        "\n",
        "    return best_info\n",
        "\n",
        "\n",
        "def expected_calibration_error(probs, labels, n_bins=15\n",
        "):\n",
        "    bins = np.linspace(0,1,n_bins+1)\n",
        "    idx = np.digitize(probs, bins) - 1\n",
        "    ece = 0.0\n",
        "    labels = (labels > 0.5).astype(int)\n",
        "    for b in range(n_bins):\n",
        "        m = (idx == b)\n",
        "        if not np.any(m):\n",
        "            continue\n",
        "        conf = probs[m].mean()\n",
        "        acc = ((probs[m] >= 0.5).astype(int) == labels[m]).mean()\n",
        "        ece += (np.sum(m)/len(probs)) * abs(acc - conf)\n",
        "    return float(ece)\n",
        "\n",
        "class TemperatureScalerTF(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.logT = tf.Variable(0.0, dtype=tf.float32)\n",
        "\n",
        "    def call(self, logits):\n",
        "        return logits / tf.exp(self.logT)\n",
        "\n",
        "def fit_temperature_tf(logits, labels, steps=200, lr=0.01):\n",
        "    logits = tf.convert_to_tensor(logits, dtype=tf.float32)\n",
        "    if logits.ndim == 1:\n",
        "        logits = tf.expand_dims(logits, axis=-1)\n",
        "    n_classes = logits.shape[-1]\n",
        "\n",
        "    scaler = TemperatureScalerTF()\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    if n_classes == 1:\n",
        "        labels = (np.asarray(labels) > 0.5).astype(int)\n",
        "        lb = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
        "        loss_fn = lambda s, y: tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=s[...,0]))\n",
        "    else:\n",
        "        if labels.ndim == 2:\n",
        "            y_int = labels.argmax(axis=-1)\n",
        "        else:\n",
        "            y_int = labels.astype(np.int32)\n",
        "        lb = tf.convert_to_tensor(y_int)\n",
        "        loss_fn = lambda s, y: tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=s))\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            s = scaler(logits)\n",
        "            loss = loss_fn(s, lb)\n",
        "        grads = tape.gradient(loss, scaler.trainable_variables)\n",
        "        if not grads or any(g is None for g in grads):\n",
        "            return 1.0\n",
        "        opt.apply_gradients(zip(grads, scaler.trainable_variables))\n",
        "\n",
        "    T = float(tf.exp(scaler.logT).numpy())\n",
        "    return max(0.5, min(T, 10.0))\n",
        "\n",
        "\n",
        "def ternary_from_probs(probs, t_low=0.30, t_high=0.70):\n",
        "    \"\"\"\n",
        "    Maps binary probability to 3 classes:\n",
        "      0 = 'certainly no fat tails'    (p <= t_low)\n",
        "      1 = 'uncertain / gray zone'       (t_low < p < t_high)\n",
        "      2 = 'surely fat tails'          (p >= t_high)\n",
        "    \"\"\"\n",
        "    probs = np.asarray(probs).ravel()\n",
        "    out = np.full(probs.shape, 1, dtype=int)\n",
        "    out[probs <= t_low] = 0\n",
        "    out[probs >= t_high] = 2\n",
        "    return out\n",
        "\n",
        "def evaluate_ternary(probs, y_true, t_low=0.30, t_high=0.70):\n",
        "    y = (np.asarray(y_true) > 0.5).astype(int)\n",
        "    p = np.asarray(probs).ravel()\n",
        "    pred3 = ternary_from_probs(p, t_low, t_high)\n",
        "    decided = pred3 != 1\n",
        "    abstain_rate = float((~decided).mean())\n",
        "    acc_decided = float(((pred3[decided] == (y[decided]*2)).mean()) if decided.any() else np.nan)\n",
        "    cm = np.zeros((2,3), dtype=int)\n",
        "    for yi, pi in zip(y, pred3):\n",
        "        cm[yi, pi] += 1\n",
        "    return {\n",
        "        \"t_low\": float(t_low), \"t_high\": float(t_high),\n",
        "        \"abstain_rate\": abstain_rate,\n",
        "        \"acc_decided\": acc_decided,\n",
        "        \"cm_2x3\": cm.tolist(),\n",
        "        \"pred3\": pred3\n",
        "    }\n",
        "\n",
        "def _prec_pos(p, y, thr):\n",
        "    m = p >= thr\n",
        "    if not m.any():\n",
        "        return np.nan, 0\n",
        "    tp = ((y == 1) & m).sum()\n",
        "    fp = ((y == 0) & m).sum()\n",
        "    prec = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
        "    return float(prec), int(m.sum())\n",
        "\n",
        "def _prec_neg(p, y, thr):\n",
        "    m = p <= thr\n",
        "    if not m.any():\n",
        "        return np.nan, 0\n",
        "    tn = ((y == 0) & m).sum()\n",
        "    fn = ((y == 1) & m).sum()\n",
        "    prec = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
        "    return float(prec), int(m.sum())\n",
        "\n",
        "def find_gray_zone_thresholds(probs, y_true, target_precision=0.90, min_points_each_side=5, grid_quantiles=99):\n",
        "    p = np.asarray(probs).ravel()\n",
        "    y = (np.asarray(y_true) > 0.5).astype(int)\n",
        "\n",
        "    qs = np.linspace(0.01, 0.99, grid_quantiles)\n",
        "    pts = np.unique(np.quantile(p, qs))\n",
        "    lefts  = pts[pts < 0.5]\n",
        "    rights = pts[pts > 0.5]\n",
        "\n",
        "    best = None\n",
        "    for tl in lefts:\n",
        "        prec0, n0 = _prec_neg(p, y, tl)\n",
        "        if np.isnan(prec0) or n0 < min_points_each_side or prec0 < target_precision:\n",
        "            continue\n",
        "        for th in rights:\n",
        "            if th <= tl:\n",
        "                continue\n",
        "            prec1, n1 = _prec_pos(p, y, th)\n",
        "            if np.isnan(prec1) or n1 < min_points_each_side or prec1 < target_precision:\n",
        "                continue\n",
        "            pred3 = ternary_from_probs(p, tl, th)\n",
        "            decided = pred3 != 1\n",
        "            abstain = 1.0 - decided.mean()\n",
        "            acc_decided = ((pred3[decided] == (y[decided]*2)).mean()) if decided.any() else np.nan\n",
        "            width = th - tl\n",
        "            key = (abstain, -np.nan_to_num(acc_decided, nan=-1.0), width, tl, th,\n",
        "                   {\"prec0\": float(prec0), \"n0\": int(n0), \"prec1\": float(prec1), \"n1\": int(n1)})\n",
        "            if (best is None) or (key < best):\n",
        "                best = key\n",
        "\n",
        "    if best is None:\n",
        "        print(\"[warn] No (t_low, t_high) found that satisfy target_precision. Fallback to (0.30, 0.70).\")\n",
        "        return 0.30, 0.70, {\"fallback\": True}\n",
        "\n",
        "    _, _, _, tl, th, extra = best\n",
        "    info = {\"fallback\": False, \"target_precision\": float(target_precision), **extra}\n",
        "    return float(tl), float(th), info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 5-Fold Cross-Validation + Hyperparameter Search (Including Dropout)\n",
        "\n",
        "This cell runs cross-validation at the dataset level and performs a small hyperparameter search.  \n",
        "For each fold and hyperparameter setting, models are trained, calibrated (temperature scaling), evaluated (image- and dataset-level), and saved. Results are logged to a CSV report.\n",
        "\n",
        "- **`DEFAULT_GRAY_T_LOW`, `DEFAULT_GRAY_T_HIGH`**: Default gray-zone thresholds used as fallback values.\n",
        "- **`TARGET_PRECISION`**: Target precision constraint used when searching gray-zone thresholds.\n",
        "- **`EpochTimer`**: Keras callback that measures per-epoch training time and provides an average epoch duration.\n",
        "- **`DEBUG_MODE`**: Switch to reduce folds, search space, and dataset size for fast debugging runs.\n",
        "- **`MODEL_CHOICES`, `LR_CHOICES`, `WD_CHOICES`, `EPOCH_CHOICES`, `BS_CHOICES`, `DROPOUT_CHOICES`**: Defines the hyperparameter search space (with smaller defaults when `DEBUG_MODE=True`).\n",
        "- **`search_space`**: Cartesian product of all hyperparameter choices used to iterate over trials.\n",
        "- **`build_model_keras`**: Factory function that instantiates the selected architecture (`baseline` or `separable`) with the given `l2reg` and `dropout`.\n",
        "- **`skf`**: `StratifiedKFold` splitter used for dataset-level outer cross-validation (2 folds in debug mode, otherwise 5).\n",
        "- **`uniq_ids`, `uniq_label`**: Arrays of unique dataset identifiers and their labels used for stratified splitting.\n",
        "- **`rows_for_ids`**: Collects all sample rows (including augmentations) belonging to a given set of dataset IDs.\n",
        "- **`make_tf_dataset`**: Builds the `tf.data` pipelines for `ds_train`, `ds_val`, and `ds_test` (batching, optional shuffling, prefetching).\n",
        "- **`compile_model`**: Compiles each model instance with the chosen learning rate before training.\n",
        "- **`predict_logits`**: Extracts validation/test logits and labels for calibration and metric computation.\n",
        "- **`fit_temperature_tf`**: Fits the temperature `T` on validation logits to calibrate probabilities.\n",
        "- **`find_best_threshold_f1`**: Searches the decision threshold that maximizes F1 on calibrated validation probabilities.\n",
        "- **`expected_calibration_error`**: Computes ECE to quantify calibration quality on validation/test probabilities.\n",
        "- **`find_gray_zone_thresholds`**: Searches `(t_low, t_high)` thresholds that meet `TARGET_PRECISION` for a three-state (positive/uncertain/negative) decision.\n",
        "- **`evaluate_numpy`, `evaluate_dataset_level`, `evaluate_ternary`**: Computes metrics for image-level, dataset-level, and ternary (abstention) evaluation.\n",
        "- **`fold_tuning_results`, `trial_results`, `saved_models`**: Stores trial-level and fold-level results, including model paths and calibration parameters.\n",
        "- **`soft_vote`**: Aggregates probabilities across multiple models via mean probability (soft voting).\n",
        "- **`report_csv`**: Writes all tuned CV results (val/test, including dataset-level metrics and calibration info) to a CSV report file."
      ],
      "metadata": {
        "id": "B91idjbUbxGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775e2030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de920afe-eb77-405c-a6db-9db3224c520f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Fold 1 / 2 — 1 Trials ==========\n",
            "\n",
            "[Trial 1/1] fold1 | model=baseline, lr=0.001, l2=0.0, ep=2, bs=16, dr=0.0\n",
            "[fold1 | model=baseline, lr=0.001, l2=0.0, ep=2, bs=16, dr=0.0] finished: 2 ep, avg 6.9s/ep, wall 13.8s\n",
            "[Fold 1] progress: 1/1 trials done (avg 13.8s/trial) — ETA fold ~0.0s\n",
            "[warn] Keine (t_low,t_high) gefunden, die target_precision erfüllen. Fallback auf (0.30, 0.70).\n",
            "Saved CV report: /content/drive/MyDrive/Generated Data for Data science project/reports/cv_results_tuned_dropout_keras.csv\n"
          ]
        }
      ],
      "source": [
        "from itertools import product\n",
        "import time, math\n",
        "\n",
        "DEFAULT_GRAY_T_LOW  = 0.30\n",
        "DEFAULT_GRAY_T_HIGH = 0.70\n",
        "TARGET_PRECISION    = 0.90\n",
        "\n",
        "class EpochTimer(keras.callbacks.Callback):\n",
        "    def __init__(self, total_epochs:int, label:str=\"\"):\n",
        "        super().__init__()\n",
        "        self.total_epochs = int(total_epochs)\n",
        "        self.label = label\n",
        "        self.epoch_durations = []\n",
        "        self._epoch_t0 = None\n",
        "        self._t0 = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self._t0 = time.time()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self._epoch_t0 = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self._epoch_t0 is None:\n",
        "            return\n",
        "        dur = time.time() - self._epoch_t0\n",
        "        self.epoch_durations.append(dur)\n",
        "\n",
        "    @property\n",
        "    def avg_epoch_seconds(self):\n",
        "        return (sum(self.epoch_durations) / len(self.epoch_durations)) if self.epoch_durations else float(\"nan\")\n",
        "\n",
        "DEBUG_MODE = True  # <--- for quick testing; set to False later\n",
        "\n",
        "# Original search spaces as reference (and for DEBUG_MODE=False)\n",
        "MODEL_CHOICES_FULL    = [\"baseline\",\"separable\"]\n",
        "LR_CHOICES_FULL       = [1e-3, 3e-4]\n",
        "WD_CHOICES_FULL       = [0.0, 1e-4]\n",
        "EPOCH_CHOICES_FULL    = [15, 30]\n",
        "BS_CHOICES_FULL       = [32]\n",
        "DROPOUT_CHOICES_FULL  = [0.0, 0.3]\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    # Radically smaller search space for quick testing\n",
        "    MODEL_CHOICES   = [\"baseline\"]\n",
        "    LR_CHOICES      = [1e-3]\n",
        "    WD_CHOICES      = [0.0]\n",
        "    EPOCH_CHOICES   = [2]\n",
        "    BS_CHOICES      = [16]\n",
        "    DROPOUT_CHOICES = [0.0]\n",
        "else:\n",
        "    # 1:1 your original search space\n",
        "    MODEL_CHOICES   = MODEL_CHOICES_FULL\n",
        "    LR_CHOICES      = LR_CHOICES_FULL\n",
        "    WD_CHOICES      = WD_CHOICES_FULL\n",
        "    EPOCH_CHOICES   = EPOCH_CHOICES_FULL\n",
        "    BS_CHOICES      = BS_CHOICES_FULL\n",
        "    DROPOUT_CHOICES = DROPOUT_CHOICES_FULL\n",
        "\n",
        "def build_model_keras(name, l2reg=0.0, dropout=0.0):\n",
        "    if name == \"baseline\":\n",
        "        return build_baseline(l2reg=l2reg, dropout=dropout)\n",
        "    elif name == \"separable\":\n",
        "        return build_separable(l2reg=l2reg, dropout=dropout)\n",
        "    else:\n",
        "        raise ValueError(name)\n",
        "\n",
        "search_space = list(product(MODEL_CHOICES, LR_CHOICES, WD_CHOICES, EPOCH_CHOICES, BS_CHOICES, DROPOUT_CHOICES))\n",
        "\n",
        "fold_tuning_results = []\n",
        "saved_models = []\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)  # faster: just 2 Folds\n",
        "else:\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)  # your original setting\n",
        "\n",
        "uniq_ids   = np.array([r[\"dataset_id\"] for r in uniq_rows])\n",
        "uniq_label = np.array([r[\"label\"] for r in uniq_rows])\n",
        "\n",
        "for fold_idx, (train_val_idx, test_idx) in enumerate(skf.split(uniq_ids, uniq_label), start=1):\n",
        "    if DEBUG_MODE and fold_idx > 1:\n",
        "        break\n",
        "\n",
        "    tv_ids = uniq_ids[train_val_idx]; tv_lab = uniq_label[train_val_idx]\n",
        "    test_ids = uniq_ids[test_idx]\n",
        "    skf_tv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    inner_train_ids, val_ids = next(skf_tv.split(tv_ids, tv_lab))\n",
        "    train_ids = tv_ids[inner_train_ids]; val_ids = tv_ids[val_ids]\n",
        "\n",
        "    train_rows = rows_for_ids(train_ids)\n",
        "    val_rows   = rows_for_ids(val_ids)\n",
        "    test_rows  = rows_for_ids(test_ids)\n",
        "\n",
        "    # In Debug mode: only a subset of data for faster testing (list-based)\n",
        "    if DEBUG_MODE:\n",
        "        rng = np.random.default_rng(SEED)\n",
        "\n",
        "        def subsample(rows, n):\n",
        "            if len(rows) <= n:\n",
        "                return rows\n",
        "            idx = rng.choice(len(rows), size=n, replace=False)\n",
        "            return [rows[i] for i in idx]\n",
        "\n",
        "        train_rows = subsample(train_rows, 200)\n",
        "        val_rows   = subsample(val_rows, 100)\n",
        "        test_rows  = subsample(test_rows, 100)\n",
        "\n",
        "\n",
        "    trial_results = []\n",
        "    total_trials = len(search_space)\n",
        "\n",
        "    completed_trials = 0\n",
        "    trial_wall_times = []\n",
        "\n",
        "    print(f\"\\n========== Fold {fold_idx} / {skf.get_n_splits()} — {total_trials} Trials ==========\")\n",
        "\n",
        "    for (model_name, lr, wd, epochs, bs, dr) in search_space:\n",
        "        completed_trials += 1\n",
        "        trial_label = f\"fold{fold_idx} | model={model_name}, lr={lr}, l2={wd}, ep={epochs}, bs={bs}, dr={dr}\"\n",
        "        print(f\"\\n[Trial {completed_trials}/{total_trials}] {trial_label}\")\n",
        "\n",
        "        # Copies of the lists (so we don't modify them in-place)\n",
        "        train_rows_f = list(train_rows)\n",
        "        val_rows_f   = list(val_rows)\n",
        "        test_rows_f  = list(test_rows)\n",
        "\n",
        "        # Dataset-IDs for dataset-level metrics (val/test)\n",
        "        val_ids_all  = [r[\"dataset_id\"] for r in val_rows_f]\n",
        "        test_ids_all = [r[\"dataset_id\"] for r in test_rows_f]\n",
        "\n",
        "        # tf.data-Datasets with your existing pipeline from 2.2\n",
        "        ds_train = make_tf_dataset(train_rows_f, batch_size=bs, shuffle=True)\n",
        "        ds_val   = make_tf_dataset(val_rows_f,   batch_size=bs, shuffle=False)\n",
        "        ds_test  = make_tf_dataset(test_rows_f,  batch_size=bs, shuffle=False)\n",
        "\n",
        "        model = build_model_keras(model_name, l2reg=wd, dropout=dr)\n",
        "        compile_model(model, lr=lr)\n",
        "\n",
        "        et_cb = EpochTimer(total_epochs=epochs, label=trial_label)\n",
        "        cb = [et_cb]\n",
        "\n",
        "        _t0 = time.time()\n",
        "        model.fit(ds_train, validation_data=ds_val, epochs=epochs, verbose=0, callbacks=cb)\n",
        "        train_wall = time.time() - _t0\n",
        "        trial_wall_times.append(train_wall)\n",
        "\n",
        "        avg_ep = et_cb.avg_epoch_seconds\n",
        "        done_ep = len(et_cb.epoch_durations)\n",
        "        print(f\"[{trial_label}] finished: {done_ep} ep, \"\n",
        "              f\"avg {avg_ep:.1f}s/ep, wall {train_wall:.1f}s\")\n",
        "\n",
        "        remaining_trials = max(total_trials - completed_trials, 0)\n",
        "        mean_trial = (sum(trial_wall_times) / len(trial_wall_times)) if trial_wall_times else float(\"nan\")\n",
        "        eta_fold = remaining_trials * mean_trial if math.isfinite(mean_trial) else float(\"nan\")\n",
        "        print(f\"[Fold {fold_idx}] progress: {completed_trials}/{total_trials} trials done \"\n",
        "              f\"(avg {mean_trial:.1f}s/trial) — ETA fold ~{eta_fold:.1f}s\")\n",
        "\n",
        "        # --- Validation logits / calibration / threshold tuning ---\n",
        "        v_logits, v_true = predict_logits(model, ds_val)\n",
        "\n",
        "        # Temperature scaling always on raw logits\n",
        "        T = fit_temperature_tf(v_logits, v_true, steps=200, lr=0.05)\n",
        "\n",
        "        if v_logits.ndim == 1:\n",
        "            # Binary case: calibrate logits and work with calibrated probabilities\n",
        "            v_logits_cal = v_logits / T\n",
        "            v_probs_cal = 1.0 / (1.0 + np.exp(-v_logits_cal))\n",
        "\n",
        "            # F1-optimal threshold on calibrated probabilities\n",
        "            thr_info = find_best_threshold_f1(v_probs_cal, v_true)\n",
        "            best_thr = float(thr_info[\"threshold\"])\n",
        "\n",
        "            # Metrics using calibrated logits (so that probs & decisions are consistent)\n",
        "            val_m = evaluate_numpy(v_logits_cal, v_true, threshold=best_thr)\n",
        "            val_ece = expected_calibration_error(v_probs_cal, v_true)\n",
        "\n",
        "            # Gray-zone thresholds on calibrated probabilities\n",
        "            tl, th, tlth_info = find_gray_zone_thresholds(\n",
        "                v_probs_cal,\n",
        "                v_true,\n",
        "                target_precision=TARGET_PRECISION,\n",
        "                min_points_each_side=5,\n",
        "                grid_quantiles=99,\n",
        "            )\n",
        "            val_tern = evaluate_ternary(v_probs_cal, v_true, tl, th)\n",
        "\n",
        "            # Store threshold & ternary metrics\n",
        "            val_m.update({\n",
        "                \"threshold\": best_thr,\n",
        "                \"thr_prec\": float(thr_info[\"precision\"]),\n",
        "                \"thr_rec\": float(thr_info[\"recall\"]),\n",
        "                \"thr_f1\": float(thr_info[\"f1\"]),\n",
        "                \"ece\": val_ece,\n",
        "                \"abstain\": val_tern[\"abstain_rate\"],\n",
        "                \"acc_decided\": val_tern[\"acc_decided\"],\n",
        "            })\n",
        "\n",
        "            # Dataset-level metrics on calibrated probabilities\n",
        "            val_ds_metrics = evaluate_dataset_level(\n",
        "                probs=v_probs_cal,\n",
        "                y_true=v_true,\n",
        "                ds_ids=val_ids_all,\n",
        "                threshold=best_thr,\n",
        "            )\n",
        "            for k, v in val_ds_metrics.items():\n",
        "                val_m[f\"ds_{k}\"] = v\n",
        "\n",
        "        else:\n",
        "            # Multiclass fallback (not expected for this project, but kept for safety)\n",
        "            v_logits_cal = v_logits / T\n",
        "            v_probs_cal = tf.nn.softmax(v_logits_cal, axis=-1).numpy()\n",
        "            best_thr = 0.5\n",
        "            thr_info = None\n",
        "\n",
        "            val_m = evaluate_numpy(v_logits_cal, v_true, threshold=best_thr)\n",
        "            val_ece = expected_calibration_error(v_probs_cal.max(axis=1), v_true)\n",
        "            val_m.update({\"ece\": val_ece})\n",
        "\n",
        "            tl, th, tlth_info = DEFAULT_GRAY_T_LOW, DEFAULT_GRAY_T_HIGH, {\"fallback\": True, \"multiclass\": True}\n",
        "\n",
        "        # --- Test split: use same T, thresholds etc. ---\n",
        "        t_logits, t_true = predict_logits(model, ds_test)\n",
        "\n",
        "        if t_logits.ndim == 1:\n",
        "            t_logits_cal = t_logits / T\n",
        "            t_probs_cal = 1.0 / (1.0 + np.exp(-t_logits_cal))\n",
        "\n",
        "            test_m = evaluate_numpy(t_logits_cal, t_true, threshold=best_thr)\n",
        "            test_ece = expected_calibration_error(t_probs_cal, t_true)\n",
        "\n",
        "            test_tern = evaluate_ternary(t_probs_cal, t_true, tl, th)\n",
        "            test_m.update({\n",
        "                \"ece\": test_ece,\n",
        "                \"abstain\": test_tern[\"abstain_rate\"],\n",
        "                \"acc_decided\": test_tern[\"acc_decided\"],\n",
        "                \"threshold\": float(best_thr),\n",
        "            })\n",
        "\n",
        "            # Dataset-level metrics on calibrated probabilities for test\n",
        "            test_ds_metrics = evaluate_dataset_level(\n",
        "                probs=t_probs_cal,\n",
        "                y_true=t_true,\n",
        "                ds_ids=test_ids_all,\n",
        "                threshold=best_thr,\n",
        "            )\n",
        "            for k, v in test_ds_metrics.items():\n",
        "                test_m[f\"ds_{k}\"] = v\n",
        "\n",
        "        else:\n",
        "            t_logits_cal = t_logits / T\n",
        "            t_probs_cal = tf.nn.softmax(t_logits_cal, axis=-1).numpy()\n",
        "\n",
        "            test_m = evaluate_numpy(t_logits_cal, t_true, threshold=best_thr)\n",
        "            test_ece = expected_calibration_error(t_probs_cal.max(axis=1), t_true)\n",
        "            test_m.update({\"ece\": test_ece})\n",
        "\n",
        "        hp = {\"model\": model_name, \"lr\": lr, \"l2\": wd, \"epochs\": epochs, \"bs\": bs, \"dropout\": dr}\n",
        "        path = MODELS_DIR / f\"fold{fold_idx}_{model_name}_lr{lr}_l2{wd}_ep{epochs}_dr{dr}.keras\"\n",
        "        model.save(path, include_optimizer=False)\n",
        "\n",
        "        trial_results.append({\n",
        "            \"fold\": fold_idx,\n",
        "            \"hparams\": hp,\n",
        "            \"val\": val_m,\n",
        "            \"test\": test_m,\n",
        "            \"path\": str(path),\n",
        "            \"T\": float(T),\n",
        "            \"t_low\": float(tl),\n",
        "            \"t_high\": float(th),\n",
        "            \"t_opt\": float(best_thr),      # <--- New\n",
        "            \"tlth_info\": tlth_info\n",
        "        })\n",
        "\n",
        "        saved_models.append((fold_idx, model_name, hp, str(path), float(T)))\n",
        "\n",
        "    best = sorted(trial_results, key=lambda r: r[\"val\"].get(\"roc_auc\", -1), reverse=True)[:3]\n",
        "    fold_tuning_results.append({\"fold\": fold_idx, \"trials\": trial_results, \"topk\": best})\n",
        "\n",
        "def soft_vote(*probs_list):\n",
        "    return np.mean(np.stack(probs_list, axis=0), axis=0)\n",
        "\n",
        "report_csv = REPORTS_DIR / \"cv_results_tuned_dropout_keras.csv\"\n",
        "with open(report_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    import csv, json\n",
        "    header = [\n",
        "      \"fold\",\"model\",\"lr\",\"l2\",\"epochs\",\"bs\",\"dropout\",\n",
        "      \"split\",\n",
        "      \"acc\",\"f1\",\"roc_auc\",\"pr_auc\",\"ece\",\"abstain\",\"acc_decided\",\"threshold\",\n",
        "      \"ds_acc\",\"ds_f1\",\"ds_roc_auc\",\"ds_pr_auc\",\n",
        "      \"model_path\",\"T\",\"t_low\",\"t_high\",\"target_precision\",\"fallback\"\n",
        "    ]\n",
        "    w = csv.writer(f); w.writerow(header)\n",
        "    for fr in fold_tuning_results:\n",
        "      for r in fr[\"trials\"]:\n",
        "          hp = r[\"hparams\"]; info = r.get(\"tlth_info\", {})\n",
        "          fallback = info.get(\"fallback\", False)\n",
        "          tprec = info.get(\"target_precision\", TARGET_PRECISION)\n",
        "          for split in [\"val\",\"test\"]:\n",
        "              m = r[split]\n",
        "\n",
        "              ds_acc     = m.get(\"ds_acc\", \"\")\n",
        "              ds_f1      = m.get(\"ds_f1\", \"\")\n",
        "              ds_roc_auc = m.get(\"ds_roc_auc\", \"\")\n",
        "              ds_pr_auc  = m.get(\"ds_pr_auc\", \"\")\n",
        "\n",
        "              w.writerow([\n",
        "                  fr[\"fold\"], hp[\"model\"], hp[\"lr\"], hp[\"l2\"], hp[\"epochs\"], hp[\"bs\"], hp[\"dropout\"],\n",
        "                  split,\n",
        "                  m.get(\"acc\",\"\"), m.get(\"f1\",\"\"), m.get(\"roc_auc\",\"\"), m.get(\"pr_auc\",\"\"),\n",
        "                  m.get(\"ece\",\"\"), m.get(\"abstain\",\"\"), m.get(\"acc_decided\",\"\"), m.get(\"threshold\",\"\"),\n",
        "                  ds_acc, ds_f1, ds_roc_auc, ds_pr_auc,\n",
        "                  r[\"path\"], r[\"T\"], r[\"t_low\"], r[\"t_high\"], tprec, int(fallback)\n",
        "              ])\n",
        "print(\"Saved CV report:\", report_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Best-Model Visualizations: Cross-Validation Analysis & Dashboard\n",
        "\n",
        "This cell provides utilities for analyzing cross-validation results and generating visual summaries for the best-performing models.  \n",
        "It focuses on selecting optimal configurations, aggregating metrics across folds, and creating plots for performance and calibration analysis.\n",
        "\n",
        "- **`load_cv_results`**: Loads the cross-validation results CSV file and converts relevant columns to numeric values for analysis.\n",
        "- **`best_row`**: Selects the best-performing validation result based on a primary metric (e.g. `roc_auc`) and a secondary tie-breaker metric.\n",
        "- **`summarize_hparams`**: Aggregates validation metrics across folds for each hyperparameter configuration and computes mean and standard deviation.\n",
        "- **`select_best_config`**: Identifies the hyperparameter configuration with the best average validation performance across folds.\n",
        "- **`plot_confusion_matrix`**: Generates and saves a binary confusion matrix for the best model using a fixed decision threshold.\n",
        "- **`plot_confusion_matrix_ternary`**: Creates a ternary confusion matrix (negative / uncertain / positive) based on lower and upper probability thresholds.\n",
        "- **`plot_roc`**: Plots the ROC curve and computes the AUC for the best validation model.\n",
        "- **`plot_reliability`**: Generates a reliability diagram to assess probability calibration of the best model.\n",
        "- **`plot_cv_bars`**: Visualizes the best validation metric per fold as a bar chart to summarize cross-validation performance."
      ],
      "metadata": {
        "id": "Xv3Q9KDicLfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_cv_results(csv_path: str | Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.dropna(how=\"all\")\n",
        "    for col in [\n",
        "        \"acc\",\"f1\",\"roc_auc\",\"pr_auc\",\"ece\",\"abstain\",\"acc_decided\",\"threshold\",\n",
        "        \"ds_acc\",\"ds_f1\",\"ds_roc_auc\",\"ds_pr_auc\",\n",
        "        \"T\",\"t_low\",\"t_high\",\"target_precision\"\n",
        "    ]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def best_row(df_val: pd.DataFrame, primary=\"roc_auc\", secondary=\"f1\", higher_is_better=None):\n",
        "    p, s = primary.lower(), secondary.lower()\n",
        "    hib = {\"roc_auc\": True, \"f1\": True, \"acc\": True, \"pr_auc\": True, \"ece\": False}\n",
        "    if higher_is_better:\n",
        "        hib.update({k.lower(): v for k, v in higher_is_better.items()})\n",
        "    if df_val.empty:\n",
        "        raise ValueError(\"No validation rows found in CSV (split=='val').\")\n",
        "    def sort_key(row):\n",
        "        pk = row.get(p, np.nan); sk = row.get(s, np.nan)\n",
        "        pk = pk if hib.get(p, True) else -pk\n",
        "        sk = sk if hib.get(s, True) else -sk\n",
        "        return (np.nan_to_num(pk, nan=-np.inf), np.nan_to_num(sk, nan=-np.inf))\n",
        "    best_idx = max(df_val.index, key=lambda i: sort_key(df_val.loc[i]))\n",
        "    return df_val.loc[best_idx]\n",
        "\n",
        "def summarize_hparams(df_val: pd.DataFrame,\n",
        "                      primary: str = \"roc_auc\",\n",
        "                      secondary: str = \"f1\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregate validation metrics across folds for each hyperparameter configuration.\n",
        "    \"\"\"\n",
        "    cfg_cols = [\"model\",\"lr\",\"l2\",\"epochs\",\"bs\",\"dropout\"]\n",
        "    g = df_val.groupby(cfg_cols)\n",
        "\n",
        "    rows = []\n",
        "    for cfg, sub in g:\n",
        "        row = dict(zip(cfg_cols, cfg))\n",
        "        row[\"n_folds\"] = sub[\"fold\"].nunique()\n",
        "        for col in [primary, secondary]:\n",
        "            row[f\"mean_{col}\"] = sub[col].mean()\n",
        "            row[f\"std_{col}\"]  = sub[col].std(ddof=0)\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def select_best_config(hp_df: pd.DataFrame,\n",
        "                       primary: str = \"roc_auc\",\n",
        "                       secondary: str = \"f1\") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Select the hyperparameter configuration with best mean primary metric,\n",
        "    breaking ties by mean secondary metric.\n",
        "    \"\"\"\n",
        "    p = f\"mean_{primary}\"\n",
        "    s = f\"mean_{secondary}\"\n",
        "\n",
        "    if hp_df.empty:\n",
        "        raise ValueError(\"Hyperparameter summary is empty.\")\n",
        "\n",
        "    best_idx = max(\n",
        "        hp_df.index,\n",
        "        key=lambda i: (hp_df.loc[i, p], hp_df.loc[i, s])\n",
        "    )\n",
        "    return hp_df.loc[best_idx]\n",
        "\n",
        "def _ensure_np(a): return a if isinstance(a, np.ndarray) else np.asarray(a)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_prob, threshold=0.5, save_path: str | Path = \"cm.png\"):\n",
        "    y_true = _ensure_np(y_true).astype(int)\n",
        "    y_pred = (_ensure_np(y_prob).ravel() >= threshold).astype(int)\n",
        "    tp = int(((y_true==1)&(y_pred==1)).sum())\n",
        "    tn = int(((y_true==0)&(y_pred==0)).sum())\n",
        "    fp = int(((y_true==0)&(y_pred==1)).sum())\n",
        "    fn = int(((y_true==1)&(y_pred==0)).sum())\n",
        "    M = np.array([[tn, fp],[fn, tp]])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
        "    im = ax.imshow(M, aspect=\"equal\")\n",
        "    for (i,j), v in np.ndenumerate(M):\n",
        "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels([\"Pred 0\",\"Pred 1\"]); ax.set_yticklabels([\"True 0\",\"True 1\"])\n",
        "    ax.set_title(\"Confusion Matrix (best model, split=val)\")\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n",
        "def plot_confusion_matrix_ternary(y_true, y_prob, t_low=0.3, t_high=0.7, save_path: str | Path = \"cm_ternary.png\"):\n",
        "    y = _ensure_np(y_true).astype(int).ravel()\n",
        "    p = _ensure_np(y_prob).ravel()\n",
        "    pred3 = np.full_like(p, 1, dtype=int)\n",
        "    pred3[p <= t_low] = 0\n",
        "    pred3[p >= t_high] = 2\n",
        "    M = np.zeros((2,3), dtype=int)\n",
        "    for yi, pi in zip(y, pred3):\n",
        "        M[yi, pi] += 1\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6.5,4.5))\n",
        "    im = ax.imshow(M, aspect=\"equal\")\n",
        "    for (i,j), v in np.ndenumerate(M):\n",
        "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    ax.set_xticks([0,1,2]); ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels([\"Pred: no fat\",\"Pred: uncertain\",\"Pred: fat\"], rotation=15, ha=\"right\")\n",
        "    ax.set_yticklabels([\"True 0 (no fat)\",\"True 1 (fat)\"])\n",
        "    ax.set_title(\"Ternary Confusion (best model, split=val)\")\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n",
        "def plot_roc(y_true, y_prob, save_path: str | Path = \"roc.png\"):\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    y_true = _ensure_np(y_true).astype(int)\n",
        "    y_prob = _ensure_np(y_prob).ravel()\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    A = auc(fpr, tpr)\n",
        "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
        "    ax.plot(fpr, tpr, label=f\"AUC={A:.3f}\")\n",
        "    ax.plot([0,1],[0,1],'--',lw=1)\n",
        "    ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\"); ax.legend()\n",
        "    ax.set_title(\"ROC (best model, split=val)\")\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n",
        "def plot_reliability(y_true, y_prob, n_bins=10, save_path: str | Path = \"reliability.png\"):\n",
        "    y_true = _ensure_np(y_true).astype(int)\n",
        "    y_prob = _ensure_np(y_prob).ravel()\n",
        "    bins = np.linspace(0,1,n_bins+1)\n",
        "    idx = np.digitize(y_prob, bins)-1\n",
        "    xs, ys = [], []\n",
        "    for b in range(n_bins):\n",
        "        m = (idx==b)\n",
        "        if not np.any(m):\n",
        "            continue\n",
        "        xs.append(y_prob[m].mean())\n",
        "        ys.append(y_true[m].mean())\n",
        "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
        "    ax.plot([0,1],[0,1],'--', lw=1)\n",
        "    ax.plot(xs, ys, marker='o')\n",
        "    ax.set_xlabel(\"confidence\"); ax.set_ylabel(\"empirical accuracy\")\n",
        "    ax.set_title(\"Reliability (best model, split=val)\")\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n",
        "def plot_cv_bars(df_val: pd.DataFrame, metric: str, save_path: str | Path):\n",
        "    metric = metric.lower()\n",
        "    g = df_val.groupby(\"fold\", as_index=False)[metric].max().sort_values(\"fold\")\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.bar(g[\"fold\"].astype(str), g[metric].values)\n",
        "    ax.set_xlabel(\"Fold\"); ax.set_ylabel(metric); ax.set_title(f\"Val {metric} (best per Fold)\")\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "sz3P_qJw2rqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Best-Model Selection, Final Training & Export\n",
        "\n",
        "This cell selects the best hyperparameter configuration from the CV results, retrains a final model on all available data (with an internal validation split), calibrates it, and exports both the model and its metadata for downstream use (e.g., in the Evaluation notebook).\n",
        "\n",
        "- **`BEST_MODEL_PATH`**: Target path where the final retrained best model is saved.\n",
        "- **`BEST_MODEL_META`**: Target path where the final best model metadata (JSON) is written.\n",
        "- **`show_best_model_dashboard`**: End-to-end pipeline that loads CV results, selects the best configuration, retrains the final model, calibrates probabilities, generates plots, and saves artifacts.\n",
        "- **`load_cv_results`**: Loads the CV results CSV and prepares numeric columns for reliable ranking and aggregation.\n",
        "- **`summarize_hparams`**: Aggregates validation metrics across folds for each hyperparameter configuration (mean/std).\n",
        "- **`select_best_config`**: Selects the best hyperparameter configuration based on mean CV performance (primary and secondary metrics).\n",
        "- **`plot_cv_bars`**: Generates a fold-wise bar chart of the chosen primary validation metric for quick CV overview.\n",
        "- **`StratifiedKFold`**: Creates an internal train/validation split for retraining the final model while preserving class balance.\n",
        "- **`rows_for_ids`**: Collects all sample rows (including augmentations) for the selected train/validation dataset IDs.\n",
        "- **`make_tf_dataset`**: Builds `ds_train_final` and `ds_val_final` TensorFlow datasets (batching/shuffling/prefetching).\n",
        "- **`build_model_keras`**: Instantiates the model architecture using the selected best hyperparameters.\n",
        "- **`compile_model`**: Compiles the final model with the selected learning rate before retraining.\n",
        "- **`predict_logits`**: Computes validation logits and labels used for calibration and threshold selection.\n",
        "- **`fit_temperature_tf`**: Fits the calibration temperature `T_final` on validation logits for improved probability calibration.\n",
        "- **`find_best_threshold_f1`**: Determines the optimal decision threshold `best_thr_final` using calibrated validation probabilities.\n",
        "- **`expected_calibration_error`**: Computes calibration quality (ECE) for the final model on the validation split.\n",
        "- **`find_gray_zone_thresholds`**: Finds `(t_low_final, t_high_final)` thresholds to support a ternary decision with an uncertainty zone.\n",
        "- **`evaluate_numpy`**: Computes final validation metrics at sample level using calibrated logits and the selected threshold.\n",
        "- **`evaluate_dataset_level`**: Computes final validation metrics aggregated at dataset level (per `dataset_id`).\n",
        "- **`model.save`**: Saves the final retrained best model to `BEST_MODEL_PATH` (without optimizer state).\n",
        "- **`best_meta`**: Collects calibration parameters, thresholds, and key metrics (sample-level + dataset-level + CV aggregates) for reproducible evaluation.\n",
        "- **`json.dump`**: Writes `best_meta` to `BEST_MODEL_META`, which is later consumed by the Evaluation notebook.\n",
        "- **`plot_roc`, `plot_confusion_matrix`, `plot_reliability`, `plot_confusion_matrix_ternary`**: Optionally creates diagnostic plots if `_Y_TRUE_` and `_Y_PROB_` are provided externally."
      ],
      "metadata": {
        "id": "_0U5xnYzcWII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "BEST_MODEL_PATH = MODELS_DIR / \"best_model_keras.h5\"\n",
        "BEST_MODEL_META = REPORTS_DIR / \"best_model_meta.json\"\n",
        "\n",
        "def show_best_model_dashboard(csv_path: str | Path,\n",
        "                              out_dir: str | Path,\n",
        "                              primary: str = \"roc_auc\",\n",
        "                              secondary: str = \"f1\",\n",
        "                              with_curves: bool = True,\n",
        "                              ternary_thresholds=None):\n",
        "    \"\"\"\n",
        "    Read CV results, aggregate metrics across folds per hyperparameter configuration,\n",
        "    select the best configuration, train one final model on all data (with internal\n",
        "    validation split), and save this final model + metadata to:\n",
        "\n",
        "      - BEST_MODEL_PATH  (e.g. models_tf/best_model_keras.h5)\n",
        "      - BEST_MODEL_META  (e.g. reports/best_model_meta.json)\n",
        "    \"\"\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[DEBUG] Starting show_best_model_dashboard with csv_path={csv_path}, out_dir={out_dir}\")\n",
        "\n",
        "    # 1) Load CSV & aggregate hyperparameters across folds\n",
        "    print(f\"[DEBUG] Loading CV results from: {csv_path}\")\n",
        "    df = load_cv_results(csv_path)\n",
        "    df_val = df[df[\"split\"] == \"val\"].copy()\n",
        "    if df_val.empty:\n",
        "        raise ValueError(\"Keine Zeilen mit split=='val' in CV-CSV gefunden.\")\n",
        "\n",
        "    print(\"[DEBUG] Loaded DataFrame head:\")\n",
        "    print(df.head())\n",
        "\n",
        "    hp_summary = summarize_hparams(df_val, primary=primary, secondary=secondary)\n",
        "    if hp_summary.empty:\n",
        "        raise ValueError(\"Keine Hyperparameter-Konfigurationen in den Validation-Ergebnissen gefunden.\")\n",
        "\n",
        "    best_cfg = select_best_config(hp_summary, primary=primary, secondary=secondary)\n",
        "\n",
        "    if hasattr(best_cfg, \"to_dict\"):\n",
        "        best_cfg_dict = best_cfg.to_dict()\n",
        "    else:\n",
        "        best_cfg_dict = dict(best_cfg)\n",
        "\n",
        "    print(\"[DEBUG] Best aggregated hyperparameters across folds:\")\n",
        "    print(best_cfg_dict)\n",
        "\n",
        "    # Bar-Plot pro Fold (as before, sample-level)\n",
        "    bars_png = plot_cv_bars(df_val, primary, out_dir / f\"cv_{primary}.png\")\n",
        "\n",
        "    # 2) Final training on all data (with internal validation split)\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    # uniq_ids / uniq_label were defined in 2.6 global\n",
        "    skf_final = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    final_train_idx, final_val_idx = next(skf_final.split(uniq_ids, uniq_label))\n",
        "    final_train_ids = uniq_ids[final_train_idx]\n",
        "    final_val_ids   = uniq_ids[final_val_idx]\n",
        "\n",
        "    final_train_rows = rows_for_ids(final_train_ids)\n",
        "    final_val_rows   = rows_for_ids(final_val_ids)\n",
        "\n",
        "    # Get hyperparameters from the best configuration\n",
        "    model_name = best_cfg_dict[\"model\"]\n",
        "    lr         = best_cfg_dict[\"lr\"]\n",
        "    wd         = best_cfg_dict[\"l2\"]\n",
        "    epochs     = int(best_cfg_dict[\"epochs\"])\n",
        "    bs         = int(best_cfg_dict[\"bs\"])\n",
        "    dropout    = best_cfg_dict[\"dropout\"]\n",
        "\n",
        "    if DEBUG_MODE:\n",
        "        final_train_rows = final_train_rows[:500]\n",
        "        final_val_rows   = final_val_rows[:200]\n",
        "        # epochs is already defined here -> no UnboundLocalError anymore\n",
        "        epochs = min(epochs, 2)\n",
        "\n",
        "    final_val_ids_all = [r[\"dataset_id\"] for r in final_val_rows]\n",
        "\n",
        "    ds_train_final = make_tf_dataset(final_train_rows, batch_size=bs, shuffle=True)\n",
        "    ds_val_final   = make_tf_dataset(final_val_rows,   batch_size=bs, shuffle=False)\n",
        "\n",
        "    model = build_model_keras(model_name, l2reg=wd, dropout=dropout)\n",
        "    compile_model(model, lr=lr)\n",
        "\n",
        "    print(\"[DEBUG] Fitting final model with best hyperparameters on all data (with internal validation split)...\")\n",
        "    model.fit(ds_train_final, validation_data=ds_val_final, epochs=epochs, verbose=0)\n",
        "\n",
        "    # 3) Calibration, threshold & gray zone for the final model\n",
        "    v_logits_final, v_true_final = predict_logits(model, ds_val_final)\n",
        "    T_final = fit_temperature_tf(v_logits_final, v_true_final, steps=200, lr=0.05)\n",
        "\n",
        "    if v_logits_final.ndim == 1:\n",
        "        v_logits_cal_final = v_logits_final / T_final\n",
        "        v_probs_cal_final = 1.0 / (1.0 + np.exp(-v_logits_cal_final))\n",
        "\n",
        "        thr_info_final = find_best_threshold_f1(v_probs_cal_final, v_true_final)\n",
        "        best_thr_final = float(thr_info_final[\"threshold\"])\n",
        "\n",
        "        val_m_final = evaluate_numpy(v_logits_cal_final, v_true_final, threshold=best_thr_final)\n",
        "        val_ece_final = expected_calibration_error(v_probs_cal_final, v_true_final)\n",
        "\n",
        "        t_low_final, t_high_final, tlth_info_final = find_gray_zone_thresholds(\n",
        "            v_probs_cal_final,\n",
        "            v_true_final,\n",
        "            target_precision=TARGET_PRECISION,\n",
        "            min_points_each_side=5,\n",
        "            grid_quantiles=99,\n",
        "        )\n",
        "        val_tern_final = evaluate_ternary(v_probs_cal_final, v_true_final, t_low_final, t_high_final)\n",
        "\n",
        "        val_m_final.update({\n",
        "            \"threshold\": best_thr_final,\n",
        "            \"thr_prec\": float(thr_info_final[\"precision\"]),\n",
        "            \"thr_rec\": float(thr_info_final[\"recall\"]),\n",
        "            \"thr_f1\": float(thr_info_final[\"f1\"]),\n",
        "            \"ece\": val_ece_final,\n",
        "            \"abstain\": val_tern_final[\"abstain_rate\"],\n",
        "            \"acc_decided\": val_tern_final[\"acc_decided\"],\n",
        "        })\n",
        "\n",
        "        # Dataset-level metrics for final model\n",
        "        val_ds_metrics_final = evaluate_dataset_level(\n",
        "            probs=v_probs_cal_final,\n",
        "            y_true=v_true_final,\n",
        "            ds_ids=final_val_ids_all,\n",
        "            threshold=best_thr_final,\n",
        "        )\n",
        "        for k, v in val_ds_metrics_final.items():\n",
        "            val_m_final[f\"ds_{k}\"] = v\n",
        "\n",
        "    else:\n",
        "        # Multiclass fallback (not expected here)\n",
        "        v_logits_cal_final = v_logits_final / T_final\n",
        "        v_probs_cal_final = tf.nn.softmax(v_logits_cal_final, axis=-1).numpy()\n",
        "        best_thr_final = 0.5\n",
        "\n",
        "        val_m_final = evaluate_numpy(v_logits_cal_final, v_true_final, threshold=best_thr_final)\n",
        "        val_ece_final = expected_calibration_error(v_probs_cal_final.max(axis=1), v_true_final)\n",
        "        val_m_final.update({\"ece\": val_ece_final})\n",
        "\n",
        "        t_low_final, t_high_final, tlth_info_final = DEFAULT_GRAY_T_LOW, DEFAULT_GRAY_T_HIGH, {\"fallback\": True, \"multiclass\": True}\n",
        "\n",
        "    # 4) Save final best model\n",
        "    BEST_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    model.save(BEST_MODEL_PATH, include_optimizer=False)\n",
        "    print(f\"[DEBUG] Final best model saved to: {BEST_MODEL_PATH}\")\n",
        "\n",
        "    # 5) Write meta JSON (used by Evaluation notebook)\n",
        "    best_meta = {\n",
        "        \"fold\": -1,  # no single fold; this is the final retrained model\n",
        "        \"model\": model_name,\n",
        "        \"original_model_path\": None,  # we no longer copy a single fold model\n",
        "        \"model_path\": str(BEST_MODEL_PATH),\n",
        "        \"T\": float(T_final),\n",
        "        \"threshold\": float(best_thr_final),\n",
        "        \"t_opt\": float(best_thr_final),\n",
        "        \"t_low\": float(t_low_final),\n",
        "        \"t_high\": float(t_high_final),\n",
        "        \"target_precision\": float(TARGET_PRECISION),\n",
        "        # final validation metrics (sample-level)\n",
        "        \"acc\": float(val_m_final.get(\"acc\", np.nan)),\n",
        "        \"f1\": float(val_m_final.get(\"f1\", np.nan)),\n",
        "        \"roc_auc\": float(val_m_final.get(\"roc_auc\", np.nan)),\n",
        "        \"pr_auc\": float(val_m_final.get(\"pr_auc\", np.nan)),\n",
        "        # final validation metrics (dataset-level)\n",
        "        \"ds_acc\": float(val_m_final.get(\"ds_acc\", np.nan)),\n",
        "        \"ds_f1\": float(val_m_final.get(\"ds_f1\", np.nan)),\n",
        "        \"ds_roc_auc\": float(val_m_final.get(\"ds_roc_auc\", np.nan)),\n",
        "        \"ds_pr_auc\": float(val_m_final.get(\"ds_pr_auc\", np.nan)),\n",
        "        # aggregated CV metrics over folds for this configuration\n",
        "        \"cv_mean_roc_auc\": float(best_cfg_dict.get(\"mean_roc_auc\", np.nan)),\n",
        "        \"cv_std_roc_auc\": float(best_cfg_dict.get(\"std_roc_auc\", np.nan)),\n",
        "        \"cv_mean_f1\": float(best_cfg_dict.get(\"mean_f1\", np.nan)),\n",
        "        \"cv_std_f1\": float(best_cfg_dict.get(\"std_f1\", np.nan)),\n",
        "    }\n",
        "\n",
        "    BEST_MODEL_META.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(BEST_MODEL_META, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(best_meta, f, indent=2)\n",
        "    print(f\"[DEBUG] Best model meta written to: {BEST_MODEL_META}\")\n",
        "    print(\"[DEBUG] best_meta content:\")\n",
        "    print(best_meta)\n",
        "\n",
        "    thresholds_used = (float(t_low_final), float(t_high_final))\n",
        "\n",
        "    results = {\n",
        "        \"bars\": bars_png,\n",
        "        \"best_row\": best_cfg_dict,\n",
        "        \"thresholds\": thresholds_used,\n",
        "        \"best_model_path\": str(BEST_MODEL_PATH),\n",
        "        \"best_model_meta\": str(BEST_MODEL_META),\n",
        "    }\n",
        "\n",
        "    # Optional plots if _Y_TRUE_ / _Y_PROB_ are defined externally\n",
        "    if with_curves and (\"_Y_TRUE_\" in globals()) and (\"_Y_PROB_\" in globals()):\n",
        "        roc_png = plot_roc(_Y_TRUE_, _Y_PROB_, out_dir / \"roc.png\")\n",
        "        cm_png  = plot_confusion_matrix(_Y_TRUE_, _Y_PROB_, 0.5, out_dir / \"cm.png\")\n",
        "        rel_png = plot_reliability(_Y_TRUE_, _Y_PROB_, 10, out_dir / \"reliability.png\")\n",
        "        tcm_png = plot_confusion_matrix_ternary(_Y_TRUE_, _Y_PROB_, t_low_final, t_high_final, out_dir / \"cm_ternary.png\")\n",
        "        results.update({\n",
        "            \"roc\": roc_png,\n",
        "            \"cm\": cm_png,\n",
        "            \"reliability\": rel_png,\n",
        "            \"cm_ternary\": tcm_png,\n",
        "        })\n",
        "\n",
        "    print(f\"Best model copied to:: {BEST_MODEL_PATH}\")\n",
        "    print(f\"Metadata saved to: {BEST_MODEL_META}\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ctwMSELXumDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Select Best Model & Generate Dashboard Artifacts\n",
        "\n",
        "This cell orchestrates the final best-model selection and dashboard generation using the cross-validation results produced earlier.  \n",
        "It triggers the end-to-end pipeline that selects the optimal configuration, retrains the final model, creates visual diagnostics, and stores all artifacts for later evaluation.\n",
        "\n",
        "- **`csv_path`**: Path to the cross-validation results CSV generated in Section 2.6.\n",
        "- **`dashboard_dir`**: Output directory used to store plots and dashboard visualizations for the best model.\n",
        "- **`show_best_model_dashboard`**: Executes the full best-model workflow, including configuration selection, final training, calibration, visualization, and artifact export.\n",
        "- **`primary=\"roc_auc\"`**: Specifies ROC-AUC as the main selection criterion, aligned with instructor feedback.\n",
        "- **`secondary=\"f1\"`**: Uses F1-score as a secondary tie-breaking criterion.\n",
        "- **`with_curves=True`**: Enables generation of diagnostic plots (ROC curve, confusion matrices, reliability diagram).\n",
        "- **`BEST_MODEL_PATH`**: Confirms the filesystem location where the final best model is stored.\n",
        "- **`BEST_MODEL_META`**: Confirms the location of the JSON file containing metadata, calibration parameters, and final metrics."
      ],
      "metadata": {
        "id": "d-ihI0klcyXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the CV-CSV written in 2.4\"\n",
        "csv_path = report_csv  # = REPORTS_DIR / \"cv_results_tuned_dropout_keras.csv\"\n",
        "\n",
        "# Folder for plots/dashboard output\n",
        "dashboard_dir = PLOTS_DIR / \"best_model_dashboard\"\n",
        "\n",
        "show_best_model_dashboard(\n",
        "    csv_path=csv_path,\n",
        "    out_dir=dashboard_dir,\n",
        "    primary=\"roc_auc\",\n",
        "    secondary=\"f1\",\n",
        "    with_curves=True,\n",
        ")\n",
        "\n",
        "print(\"Best model saved under:\", BEST_MODEL_PATH)\n",
        "print(\"Meta information saved in:\", BEST_MODEL_META)\n"
      ],
      "metadata": {
        "id": "NddOcIo5wEkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549d6b5c-7e90-4fc2-b11b-bbe5d3d065a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Starting show_best_model_dashboard with csv_path=/content/drive/MyDrive/Generated Data for Data science project/reports/cv_results_tuned_dropout_keras.csv, out_dir=/content/drive/MyDrive/Generated Data for Data science project/plots/best_model_dashboard\n",
            "[DEBUG] Loading CV results from: /content/drive/MyDrive/Generated Data for Data science project/reports/cv_results_tuned_dropout_keras.csv\n",
            "[DEBUG] Loaded DataFrame head:\n",
            "   fold     model     lr   l2  epochs  bs  dropout split   acc        f1  ...  \\\n",
            "0     1  baseline  0.001  0.0       2  16      0.0   val  0.68  0.698113  ...   \n",
            "1     1  baseline  0.001  0.0       2  16      0.0  test  0.66  0.701754  ...   \n",
            "\n",
            "     ds_acc     ds_f1  ds_roc_auc  ds_pr_auc  \\\n",
            "0  0.716981  0.727273    0.772989   0.776014   \n",
            "1  0.655556  0.693069    0.734196   0.626571   \n",
            "\n",
            "                                          model_path    T  t_low  t_high  \\\n",
            "0  /content/drive/MyDrive/Generated Data for Data...  1.0    0.3     0.7   \n",
            "1  /content/drive/MyDrive/Generated Data for Data...  1.0    0.3     0.7   \n",
            "\n",
            "   target_precision  fallback  \n",
            "0               0.9         1  \n",
            "1               0.9         1  \n",
            "\n",
            "[2 rows x 26 columns]\n",
            "[DEBUG] Best aggregated hyperparameters across folds:\n",
            "{'model': 'baseline', 'lr': 0.001, 'l2': 0.0, 'epochs': 2, 'bs': 16, 'dropout': 0.0, 'n_folds': 1, 'mean_roc_auc': 0.7623444399839421, 'std_roc_auc': 0.0, 'mean_f1': 0.6981132075471698, 'std_f1': 0.0}\n",
            "[DEBUG] Fitting final model with best hyperparameters on all data (with internal validation split)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[warn] Keine (t_low,t_high) gefunden, die target_precision erfüllen. Fallback auf (0.30, 0.70).\n",
            "[DEBUG] Final best model saved to: /content/drive/MyDrive/Generated Data for Data science project/models_tf/best_model_keras.h5\n",
            "[DEBUG] Best model meta written to: /content/drive/MyDrive/Generated Data for Data science project/reports/best_model_meta.json\n",
            "[DEBUG] best_meta content:\n",
            "{'fold': -1, 'model': 'baseline', 'original_model_path': None, 'model_path': '/content/drive/MyDrive/Generated Data for Data science project/models_tf/best_model_keras.h5', 'T': 1.0, 'threshold': 0.01, 't_opt': 0.01, 't_low': 0.3, 't_high': 0.7, 'target_precision': 0.9, 'acc': 0.28, 'f1': 0.4375, 'roc_auc': 0.5892857142857143, 'pr_auc': 0.46616560751094704, 'ds_acc': 0.28, 'ds_f1': 0.4375, 'ds_roc_auc': 0.6507936507936508, 'ds_pr_auc': 0.5700320208584327, 'cv_mean_roc_auc': 0.7623444399839421, 'cv_std_roc_auc': 0.0, 'cv_mean_f1': 0.6981132075471698, 'cv_std_f1': 0.0}\n",
            "Bestes Modell kopiert nach: /content/drive/MyDrive/Generated Data for Data science project/models_tf/best_model_keras.h5\n",
            "Metadaten gespeichert nach: /content/drive/MyDrive/Generated Data for Data science project/reports/best_model_meta.json\n",
            "Bestes Modell gespeichert unter: /content/drive/MyDrive/Generated Data for Data science project/models_tf/best_model_keras.h5\n",
            "Meta-Informationen gespeichert in: /content/drive/MyDrive/Generated Data for Data science project/reports/best_model_meta.json\n"
          ]
        }
      ]
    }
  ]
}