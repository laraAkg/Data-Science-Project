{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laraAkg/Data-Science-Project/blob/main/Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ea2368"
      },
      "source": [
        "This code block is designed to **initialize the project environment** and **manage file paths**, specifically for use within **Google Colab when integrated with Google Drive**.\n",
        "\n",
        "**Key Functions:**\n",
        "\n",
        "-   **Colab Detection:** Automatically checks if the notebook is executing in a Google Colab environment.\n",
        "-   **Google Drive Integration:** If in Colab, it mounts Google Drive to enable persistent storage and file operations.\n",
        "-   **Directory Structure:** Defines a `BASE_DIR` (either in Google Drive or a local directory) and creates essential subdirectories (e.g., `datasets`, `plots`, `models_tf`) to maintain an organized project output structure.\n",
        "-   **Standard Image Size:** Sets `IMG_SIZE` (128x128), establishing a global configuration for image processing tasks later in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fd779f6",
        "outputId": "6762c71a-9e80-48b0-81a5-277348e94882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[INFO] Entferne alten Ordner: /content/drive/MyDrive/Generated Data for Data science project/models_tf\n",
            "[INFO] Neu erstellt: /content/drive/MyDrive/Generated Data for Data science project/models_tf\n",
            "[INFO] Entferne alten Ordner: /content/drive/MyDrive/Generated Data for Data science project/reports\n",
            "[INFO] Neu erstellt: /content/drive/MyDrive/Generated Data for Data science project/reports\n",
            "BASE_DIR: /content/drive/MyDrive/Generated Data for Data science project\n"
          ]
        }
      ],
      "source": [
        "# === 0) Colab/Drive Setup & Project Paths ===\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DEFAULT_PROJECT_DIR = \"MyDrive/Generated Data for Data science project\"\n",
        "BASE_DIR = Path(\"/content/drive\") / DEFAULT_PROJECT_DIR if IN_COLAB else Path(\"./project_outputs\")\n",
        "\n",
        "DATA_DIR   = BASE_DIR / \"datasets\"\n",
        "PLOTS_DIR  = BASE_DIR / \"plots\"\n",
        "META_DIR   = BASE_DIR / \"metadata\"\n",
        "MODELS_DIR = BASE_DIR / \"models_tf\"\n",
        "REPORTS_DIR= BASE_DIR / \"reports\"\n",
        "REAL_DIR   = BASE_DIR / \"real\"\n",
        "BEST_MODEL_PATH = MODELS_DIR / \"best_model_keras.h5\"\n",
        "BEST_MODEL_META = REPORTS_DIR / \"best_model_meta.json\"\n",
        "\n",
        "folders_to_reset = [MODELS_DIR, REPORTS_DIR]\n",
        "\n",
        "for folder in folders_to_reset:\n",
        "    if folder.exists():\n",
        "        print(f\"[INFO] Entferne alten Ordner: {folder}\")\n",
        "        shutil.rmtree(folder)\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[INFO] Neu erstellt: {folder}\")\n",
        "\n",
        "for p in [DATA_DIR, PLOTS_DIR, META_DIR, MODELS_DIR, REPORTS_DIR, REAL_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = (128, 128)  # (H, W)\n",
        "\n",
        "print(\"BASE_DIR:\", BASE_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "717edf07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c0cb93-9f6f-474f-e556-98daeb825793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF device: GPU\n"
          ]
        }
      ],
      "source": [
        "# === 2.1) Imports & global config (TF/Keras) ===\n",
        "import os, json, math, time, random, csv\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
        "\n",
        "DEVICE = \"GPU\" if len(tf.config.list_physical_devices('GPU'))>0 else \"CPU\"\n",
        "BATCH_SIZE_DEFAULT = 32\n",
        "EPOCHS_DEFAULT = 8\n",
        "\n",
        "print(\"TF device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d23701"
      },
      "source": [
        "This cell loads the previously saved metadata and creates a table of samples used for training and evaluating the model. Each sample represents a set of plots (original and augmentations) and its corresponding label (heavy-tailed or not).\n",
        "\n",
        "- `INDEX_JSON`: Path to the metadata file.\n",
        "- `records`: Loads the metadata from the JSON file.\n",
        "- `samples`: A list of dictionaries containing each individual sample (plot set) and its label.\n",
        "- `uniq_rows`: A list of dictionaries representing each unique dataset, to perform cross-validation at the dataset level later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba316e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe489dc5-38db-421c-cff3-1afa6aefa549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique dataset_ids: 600\n",
            "Total samples (incl. augs): 2400\n"
          ]
        }
      ],
      "source": [
        "# === 2.2) Load metadata & build samples table (incl. augs) ===\n",
        "INDEX_JSON = META_DIR / \"datasets_index.json\"\n",
        "with open(INDEX_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    records = json.load(f)\n",
        "\n",
        "samples = []\n",
        "uniq_rows = []\n",
        "for r in records:\n",
        "    ds_id = r[\"dataset_id\"]\n",
        "    label = int(r[\"heavy_tailed\"])\n",
        "    uniq_rows.append({\"dataset_id\": ds_id, \"label\": label})\n",
        "    samples.append({\"dataset_id\": ds_id, \"variant\": \"original\", \"paths\": r[\"plots\"][\"original\"], \"label\": label})\n",
        "    for aug_name, aug_paths in r[\"plots\"][\"aug\"].items():\n",
        "        samples.append({\"dataset_id\": ds_id, \"variant\": aug_name, \"paths\": aug_paths, \"label\": label})\n",
        "\n",
        "print(\"Unique dataset_ids:\", len(uniq_rows))\n",
        "print(\"Total samples (incl. augs):\", len(samples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47af2c91"
      },
      "source": [
        "This cell contains helper functions for loading and preprocessing image data (plots) and for creating TensorFlow `Dataset` objects for training.\n",
        "\n",
        "- `load_gray_resized`: Loads an image, converts it to grayscale, resizes it, and normalizes pixel values.\n",
        "- `stack_zipf_qq_me`: Loads the three plots (Zipf, QQ, ME) for a sample, converts them to grayscale arrays, and stacks them into a single 3-channel image array.\n",
        "- `sample_to_example`: Takes a sample row and creates the input image array (`x`) and the label (`y`) for the neural network.\n",
        "- `rows_for_ids`: Filters the sample rows based on a list of dataset IDs.\n",
        "- `make_tf_dataset`: Creates a TensorFlow `Dataset` from a list of sample rows. It handles batching the data and optional shuffling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7aff3c0"
      },
      "outputs": [],
      "source": [
        "# === 2.3) Image helpers & tf.data builder ===\n",
        "def load_gray_resized(path):\n",
        "    img = Image.open(path).convert(\"L\")\n",
        "    img = img.resize((IMG_SIZE[1], IMG_SIZE[0]))\n",
        "    arr = np.asarray(img).astype(\"float32\") / 255.0\n",
        "    return arr\n",
        "\n",
        "def stack_zipf_qq_me(paths_dict):\n",
        "    z = load_gray_resized(paths_dict[\"zipf\"])\n",
        "    q = load_gray_resized(paths_dict[\"qq_exp\"])\n",
        "    m = load_gray_resized(paths_dict[\"me\"])\n",
        "    return np.stack([z,q,m], axis=-1)\n",
        "\n",
        "def sample_to_example(row):\n",
        "    x = stack_zipf_qq_me(row[\"paths\"])\n",
        "    y = np.float32(row[\"label\"])\n",
        "    return x, y\n",
        "\n",
        "def rows_for_ids(id_set):\n",
        "    s = set(id_set)\n",
        "    return [row for row in samples if row[\"dataset_id\"] in s]\n",
        "\n",
        "def make_tf_dataset(rows, batch_size=BATCH_SIZE_DEFAULT, shuffle=False):\n",
        "    xs, ys = [], []\n",
        "    for r in rows:\n",
        "        x,y = sample_to_example(r); xs.append(x); ys.append(y)\n",
        "    xs = np.stack(xs, axis=0); ys = np.array(ys, dtype=np.float32)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((xs, ys))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(xs), seed=SEED, reshuffle_each_iteration=True)\n",
        "    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec088f97"
      },
      "source": [
        "This cell defines the architectures of the neural networks (Keras models) used for classification. There are two variants: a \"Baseline\" with standard convolutional layers and a \"Separable\" with separable convolutional layers, which are often more efficient. Dropout is added for regularization.\n",
        "\n",
        "- `ConvBlock`: Defines a block consisting of convolution, batch normalization, and ReLU activation.\n",
        "- `SepConvBlock`: Defines a block consisting of separable convolution, batch normalization, and ReLU activation.\n",
        "- `build_baseline`: Creates the \"Baseline\" CNN model.\n",
        "- `build_separable`: Creates the \"Separable\" CNN model.\n",
        "- Both models end with GlobalAveragePooling2D, Dropout, and a Dense layer with one output for binary classification (heavy-tailed or not). L2 regularization and Dropout can be configured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efc6b0e6"
      },
      "outputs": [],
      "source": [
        "# === 2.4) Keras models (Baseline & Separable) with Dropout ===\n",
        "def ConvBlock(x, filters, k=3, s=1, l2=0.0):\n",
        "    x = layers.Conv2D(filters, k, strides=s, padding=\"same\",\n",
        "                      use_bias=False, kernel_regularizer=regularizers.l2(l2))(x)\n",
        "    x = layers.BatchNormalization()(x); x = layers.ReLU()(x); return x\n",
        "\n",
        "def SepConvBlock(x, filters, k=3, s=1, l2=0.0):\n",
        "    x = layers.SeparableConv2D(filters, k, strides=s, padding=\"same\", use_bias=False,\n",
        "                               depthwise_regularizer=regularizers.l2(l2), pointwise_regularizer=regularizers.l2(l2))(x)\n",
        "    x = layers.BatchNormalization()(x); x = layers.ReLU()(x); return x\n",
        "\n",
        "def build_baseline(input_shape=(128,128,3), l2reg=0.0, dropout=0.0):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = ConvBlock(inp, 16, l2=l2reg)\n",
        "    x = ConvBlock(x, 32, s=2, l2=l2reg)\n",
        "    x = ConvBlock(x, 64, s=2, l2=l2reg)\n",
        "    x = ConvBlock(x, 128, s=2, l2=l2reg)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    logit = layers.Dense(1, kernel_regularizer=regularizers.l2(l2reg))(x)\n",
        "    return keras.Model(inp, logit, name=\"CNNBaseline\")\n",
        "\n",
        "def build_separable(input_shape=(128,128,3), l2reg=0.0, dropout=0.0):\n",
        "    inp = keras.Input(shape=input_shape)\n",
        "    x = SepConvBlock(inp, 16, l2=l2reg)\n",
        "    x = SepConvBlock(x, 32, s=2, l2=l2reg)\n",
        "    x = SepConvBlock(x, 64, s=2, l2=l2reg)\n",
        "    x = SepConvBlock(x, 128, s=2, l2=l2reg)\n",
        "    x = SepConvBlock(x, 128, s=1, l2=l2reg)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    logit = layers.Dense(1, kernel_regularizer=regularizers.l2(l2reg))(x)\n",
        "    return keras.Model(inp, logit, name=\"CNNSeparable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14ff622c"
      },
      "source": [
        "This cell contains helper functions for training and evaluating the models, as well as an implementation for temperature scaling to improve the calibration of model probabilities.\n",
        "\n",
        "- `bce_logits`: Defines the loss function (Binary Crossentropy) for model compilation.\n",
        "- `compile_model`: Compiles a Keras model with an Adam optimizer and the defined loss function.\n",
        "- `predict_logits`: Performs predictions with the model and returns the logits (unscaled outputs before the sigmoid function) and the true labels.\n",
        "- `evaluate_numpy`: Calculates various metrics such as accuracy, F1-score, ROC-AUC, and PR-AUC based on the logits and true labels. It also returns the logits, probabilities, and true labels.\n",
        "- `expected_calibration_error`: Calculates the Expected Calibration Error (ECE), a metric to assess the calibration of prediction probabilities.\n",
        "- `TemperatureScalerTF`: A small Keras model that only has a trainable scaling factor (temperature T) for the logits.\n",
        "- `fit_temperature_tf`: Adjusts the temperature (`T`) by using a separate dataset (validation data) to improve the model's calibration. This is important so that the predicted probabilities better match the actual probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0faf61bf"
      },
      "outputs": [],
      "source": [
        "# === 2.5) Train/Eval utils + Temperature Scaling (binary & multiclass safe) ===\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, precision_score, recall_score\n",
        "\n",
        "bce_logits = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def compile_model(model, lr=1e-3):\n",
        "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=opt, loss=bce_logits)\n",
        "    return model\n",
        "\n",
        "def predict_logits(model, ds):\n",
        "    y_true, logits = [], []\n",
        "    for x, y in ds:\n",
        "        y_true.append(y.numpy())\n",
        "        logit_batch = model(x, training=False)\n",
        "        if logit_batch.shape[-1] == 1:\n",
        "            logit_batch = tf.squeeze(logit_batch, axis=-1)\n",
        "        logits.append(logit_batch.numpy())\n",
        "    logits = np.concatenate(logits)\n",
        "    y_true = np.concatenate(y_true)\n",
        "    return logits, y_true\n",
        "\n",
        "def evaluate_numpy(logits, y_true, threshold=0.5):\n",
        "    if logits.ndim == 1:  # binär\n",
        "        probs = 1/(1+np.exp(-logits))\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "        yb = (y_true > 0.5).astype(int)\n",
        "        out = {\n",
        "            \"acc\": float(accuracy_score(yb, preds)),\n",
        "            \"f1\": float(f1_score(yb, preds)),\n",
        "            \"roc_auc\": float(roc_auc_score(yb, probs)) if len(np.unique(yb))>1 else float(\"nan\"),\n",
        "            \"pr_auc\": float(average_precision_score(yb, probs)) if len(np.unique(yb))>1 else float(\"nan\"),\n",
        "            \"cm\": confusion_matrix(yb, preds).tolist(),\n",
        "            \"logits\": logits, \"probs\": probs, \"y_true\": yb\n",
        "        }\n",
        "        return out\n",
        "    else:\n",
        "        y_true_int = y_true.astype(int)\n",
        "        probs = tf.nn.softmax(logits, axis=-1).numpy()\n",
        "        preds = probs.argmax(axis=-1)\n",
        "        out = {\n",
        "            \"acc\": float(accuracy_score(y_true_int, preds)),\n",
        "            \"f1\": float(f1_score(y_true_int, preds, average=\"macro\")),\n",
        "            \"cm\": confusion_matrix(y_true_int, preds).tolist(),\n",
        "            \"logits\": logits, \"probs\": probs, \"y_true\": y_true_int\n",
        "        }\n",
        "        return out\n",
        "\n",
        "def evaluate_from_probs(probs, y_true, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate binary classification metrics starting from probabilities (already sigmoid-ed),\n",
        "    not logits. This is useful for calibrated probabilities.\n",
        "    \"\"\"\n",
        "    probs = np.asarray(probs).ravel()\n",
        "    y = (np.asarray(y_true) > 0.5).astype(int)\n",
        "    preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    out = {\n",
        "        \"acc\": float(accuracy_score(y, preds)),\n",
        "        \"f1\": float(f1_score(y, preds)),\n",
        "        \"roc_auc\": float(roc_auc_score(y, probs)) if len(np.unique(y)) > 1 else float(\"nan\"),\n",
        "        \"pr_auc\": float(average_precision_score(y, probs)) if len(np.unique(y)) > 1 else float(\"nan\"),\n",
        "        \"cm\": confusion_matrix(y, preds).tolist(),\n",
        "        \"probs\": probs,\n",
        "        \"y_true\": y,\n",
        "    }\n",
        "    return out\n",
        "\n",
        "\n",
        "def aggregate_by_dataset(probs, y_true, ds_ids):\n",
        "    \"\"\"\n",
        "    Aggregate probabilities and labels per dataset_id.\n",
        "\n",
        "    For each dataset_id we take:\n",
        "      - mean probability over all its image variants (original + augmentations)\n",
        "      - label: we assume all variants have the same label, so we take the first.\n",
        "    \"\"\"\n",
        "    probs = np.asarray(probs).ravel()\n",
        "    y_true = np.asarray(y_true)\n",
        "    ds_ids = np.asarray(ds_ids)\n",
        "\n",
        "    assert len(probs) == len(y_true) == len(ds_ids), \"Lengths of probs, y_true and ds_ids must match.\"\n",
        "\n",
        "    uniq = np.unique(ds_ids)\n",
        "    probs_ds = []\n",
        "    y_ds = []\n",
        "    for ds in uniq:\n",
        "        m = (ds_ids == ds)\n",
        "        if not np.any(m):\n",
        "            continue\n",
        "        probs_ds.append(probs[m].mean())\n",
        "        y_ds.append(y_true[m][0])\n",
        "\n",
        "    return np.asarray(probs_ds), np.asarray(y_ds)\n",
        "\n",
        "\n",
        "def evaluate_dataset_level(probs, y_true, ds_ids, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Compute metrics after aggregating probabilities per dataset_id.\n",
        "    \"\"\"\n",
        "    probs_ds, y_ds = aggregate_by_dataset(probs, y_true, ds_ids)\n",
        "    return evaluate_from_probs(probs_ds, y_ds, threshold=threshold)\n",
        "\n",
        "\n",
        "def find_best_threshold_f1(probs, y_true, thresholds=None):\n",
        "    \"\"\"Grid-search decision threshold between 0.01 and 0.99 to maximize F1 on a validation set.\n",
        "\n",
        "    Returns a dict with the best threshold and the corresponding accuracy, precision,\n",
        "    recall and F1-score. The true labels are assumed to be binary (0/1 or probabilities).\"\"\"\n",
        "    probs = np.asarray(probs).ravel()\n",
        "    y = (np.asarray(y_true) > 0.5).astype(int)\n",
        "\n",
        "    if thresholds is None:\n",
        "        thresholds = np.linspace(0.01, 0.99, 99)\n",
        "\n",
        "    best_f1 = None\n",
        "    best_info = {\n",
        "        \"threshold\": 0.5,\n",
        "        \"precision\": float(\"nan\"),\n",
        "        \"recall\": float(\"nan\"),\n",
        "        \"f1\": float(\"nan\"),\n",
        "        \"acc\": float(\"nan\"),\n",
        "    }\n",
        "\n",
        "    for t in thresholds:\n",
        "        preds = (probs >= t).astype(int)\n",
        "        prec = precision_score(y, preds, zero_division=0)\n",
        "        rec = recall_score(y, preds, zero_division=0)\n",
        "        f1 = f1_score(y, preds, zero_division=0)\n",
        "        acc = accuracy_score(y, preds)\n",
        "\n",
        "        if (best_f1 is None) or (f1 > best_f1):\n",
        "            best_f1 = f1\n",
        "            best_info = {\n",
        "                \"threshold\": float(t),\n",
        "                \"precision\": float(prec),\n",
        "                \"recall\": float(rec),\n",
        "                \"f1\": float(f1),\n",
        "                \"acc\": float(acc),\n",
        "            }\n",
        "\n",
        "    return best_info\n",
        "\n",
        "\n",
        "def expected_calibration_error(probs, labels, n_bins=15\n",
        "):\n",
        "    bins = np.linspace(0,1,n_bins+1)\n",
        "    idx = np.digitize(probs, bins) - 1\n",
        "    ece = 0.0\n",
        "    labels = (labels > 0.5).astype(int)\n",
        "    for b in range(n_bins):\n",
        "        m = (idx == b)\n",
        "        if not np.any(m):\n",
        "            continue\n",
        "        conf = probs[m].mean()\n",
        "        acc = ((probs[m] >= 0.5).astype(int) == labels[m]).mean()\n",
        "        ece += (np.sum(m)/len(probs)) * abs(acc - conf)\n",
        "    return float(ece)\n",
        "\n",
        "class TemperatureScalerTF(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.logT = tf.Variable(0.0, dtype=tf.float32)\n",
        "\n",
        "    def call(self, logits):\n",
        "        return logits / tf.exp(self.logT)\n",
        "\n",
        "def fit_temperature_tf(logits, labels, steps=200, lr=0.01):\n",
        "    logits = tf.convert_to_tensor(logits, dtype=tf.float32)\n",
        "    if logits.ndim == 1:\n",
        "        logits = tf.expand_dims(logits, axis=-1)\n",
        "    n_classes = logits.shape[-1]\n",
        "\n",
        "    scaler = TemperatureScalerTF()\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "    if n_classes == 1:\n",
        "        labels = (np.asarray(labels) > 0.5).astype(int)\n",
        "        lb = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
        "        loss_fn = lambda s, y: tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=s[...,0]))\n",
        "    else:\n",
        "        if labels.ndim == 2:\n",
        "            y_int = labels.argmax(axis=-1)\n",
        "        else:\n",
        "            y_int = labels.astype(np.int32)\n",
        "        lb = tf.convert_to_tensor(y_int)\n",
        "        loss_fn = lambda s, y: tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=s))\n",
        "\n",
        "    for _ in range(steps):\n",
        "        with tf.GradientTape() as tape:\n",
        "            s = scaler(logits)\n",
        "            loss = loss_fn(s, lb)\n",
        "        grads = tape.gradient(loss, scaler.trainable_variables)\n",
        "        if not grads or any(g is None for g in grads):\n",
        "            return 1.0\n",
        "        opt.apply_gradients(zip(grads, scaler.trainable_variables))\n",
        "\n",
        "    T = float(tf.exp(scaler.logT).numpy())\n",
        "    return max(0.5, min(T, 10.0))\n",
        "\n",
        "\n",
        "def ternary_from_probs(probs, t_low=0.30, t_high=0.70):\n",
        "    \"\"\"\n",
        "    Mappt Binär-Probability auf 3 Klassen:\n",
        "      0 = 'sicher keine fat tails'    (p <= t_low)\n",
        "      1 = 'unsicher / Grauzone'       (t_low < p < t_high)\n",
        "      2 = 'sicher fat tails'          (p >= t_high)\n",
        "    \"\"\"\n",
        "    probs = np.asarray(probs).ravel()\n",
        "    out = np.full(probs.shape, 1, dtype=int)\n",
        "    out[probs <= t_low] = 0\n",
        "    out[probs >= t_high] = 2\n",
        "    return out\n",
        "\n",
        "def evaluate_ternary(probs, y_true, t_low=0.30, t_high=0.70):\n",
        "    y = (np.asarray(y_true) > 0.5).astype(int)\n",
        "    p = np.asarray(probs).ravel()\n",
        "    pred3 = ternary_from_probs(p, t_low, t_high)\n",
        "    decided = pred3 != 1\n",
        "    abstain_rate = float((~decided).mean())\n",
        "    acc_decided = float(((pred3[decided] == (y[decided]*2)).mean()) if decided.any() else np.nan)\n",
        "    cm = np.zeros((2,3), dtype=int)\n",
        "    for yi, pi in zip(y, pred3):\n",
        "        cm[yi, pi] += 1\n",
        "    return {\n",
        "        \"t_low\": float(t_low), \"t_high\": float(t_high),\n",
        "        \"abstain_rate\": abstain_rate,\n",
        "        \"acc_decided\": acc_decided,\n",
        "        \"cm_2x3\": cm.tolist(),\n",
        "        \"pred3\": pred3\n",
        "    }\n",
        "\n",
        "def _prec_pos(p, y, thr):\n",
        "    m = p >= thr\n",
        "    if not m.any():\n",
        "        return np.nan, 0\n",
        "    tp = ((y == 1) & m).sum()\n",
        "    fp = ((y == 0) & m).sum()\n",
        "    prec = tp / (tp + fp) if (tp + fp) > 0 else np.nan\n",
        "    return float(prec), int(m.sum())\n",
        "\n",
        "def _prec_neg(p, y, thr):\n",
        "    m = p <= thr\n",
        "    if not m.any():\n",
        "        return np.nan, 0\n",
        "    tn = ((y == 0) & m).sum()\n",
        "    fn = ((y == 1) & m).sum()\n",
        "    prec = tn / (tn + fn) if (tn + fn) > 0 else np.nan\n",
        "    return float(prec), int(m.sum())\n",
        "\n",
        "def find_gray_zone_thresholds(probs, y_true, target_precision=0.90, min_points_each_side=5, grid_quantiles=99):\n",
        "    p = np.asarray(probs).ravel()\n",
        "    y = (np.asarray(y_true) > 0.5).astype(int)\n",
        "\n",
        "    qs = np.linspace(0.01, 0.99, grid_quantiles)\n",
        "    pts = np.unique(np.quantile(p, qs))\n",
        "    lefts  = pts[pts < 0.5]\n",
        "    rights = pts[pts > 0.5]\n",
        "\n",
        "    best = None\n",
        "    for tl in lefts:\n",
        "        prec0, n0 = _prec_neg(p, y, tl)\n",
        "        if np.isnan(prec0) or n0 < min_points_each_side or prec0 < target_precision:\n",
        "            continue\n",
        "        for th in rights:\n",
        "            if th <= tl:\n",
        "                continue\n",
        "            prec1, n1 = _prec_pos(p, y, th)\n",
        "            if np.isnan(prec1) or n1 < min_points_each_side or prec1 < target_precision:\n",
        "                continue\n",
        "            pred3 = ternary_from_probs(p, tl, th)\n",
        "            decided = pred3 != 1\n",
        "            abstain = 1.0 - decided.mean()\n",
        "            acc_decided = ((pred3[decided] == (y[decided]*2)).mean()) if decided.any() else np.nan\n",
        "            width = th - tl\n",
        "            key = (abstain, -np.nan_to_num(acc_decided, nan=-1.0), width, tl, th,\n",
        "                   {\"prec0\": float(prec0), \"n0\": int(n0), \"prec1\": float(prec1), \"n1\": int(n1)})\n",
        "            if (best is None) or (key < best):\n",
        "                best = key\n",
        "\n",
        "    if best is None:\n",
        "        print(\"[warn] Keine (t_low,t_high) gefunden, die target_precision erfüllen. Fallback auf (0.30, 0.70).\")\n",
        "        return 0.30, 0.70, {\"fallback\": True}\n",
        "\n",
        "    _, _, _, tl, th, extra = best\n",
        "    info = {\"fallback\": False, \"target_precision\": float(target_precision), **extra}\n",
        "    return float(tl), float(th), info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9012886"
      },
      "source": [
        "This cell performs 5-fold cross-validation (CV) and includes a small hyperparameter search, including optimizing the dropout rate.\n",
        "\n",
        "- `uniq_ids`, `uniq_label`: Arrays of unique dataset IDs and their labels for the CV.\n",
        "- `skf`: StratifiedKFold object for splitting the data into training and test folds while maintaining the label distribution.\n",
        "- `MODEL_CHOICES`, `LR_CHOICES`, etc.: Define the search space for hyperparameters.\n",
        "- `search_space`: Combines all hyperparameter options.\n",
        "- `MAX_TRIALS_PER_FOLD`: Limits the number of hyperparameter combinations per fold.\n",
        "- `build_model_keras`: A helper function to build the Keras model based on the name and hyperparameters.\n",
        "- The loop iterates through each fold of the CV. Within each fold, an inner CV is performed for hyperparameter search.\n",
        "- For each hyperparameter combination, a model is trained, evaluated on the validation data, and temperature scaling is performed.\n",
        "- The best models (based on ROC-AUC on the validation data) are saved and the results are logged.\n",
        "- `soft_vote`: A function for averaging the probabilities of multiple models (Soft Voting).\n",
        "- `voting_summary`: Summarizes the results of soft voting for each fold.\n",
        "- `report_csv`: Saves the detailed results of the CV and hyperparameter search in a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "775e2030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de920afe-eb77-405c-a6db-9db3224c520f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Fold 1 / 2 — 1 Trials ==========\n",
            "\n",
            "[Trial 1/1] fold1 | model=baseline, lr=0.001, l2=0.0, ep=2, bs=16, dr=0.0\n",
            "[fold1 | model=baseline, lr=0.001, l2=0.0, ep=2, bs=16, dr=0.0] finished: 2 ep, avg 6.9s/ep, wall 13.8s\n",
            "[Fold 1] progress: 1/1 trials done (avg 13.8s/trial) — ETA fold ~0.0s\n",
            "[warn] Keine (t_low,t_high) gefunden, die target_precision erfüllen. Fallback auf (0.30, 0.70).\n",
            "Saved CV report: /content/drive/MyDrive/Generated Data for Data science project/reports/cv_results_tuned_dropout_keras.csv\n"
          ]
        }
      ],
      "source": [
        "# === 2.6) 5-fold CV + kleine Hyperparameter-Suche (inkl. Dropout) ===\n",
        "from itertools import product\n",
        "import time, math\n",
        "\n",
        "DEFAULT_GRAY_T_LOW  = 0.30\n",
        "DEFAULT_GRAY_T_HIGH = 0.70\n",
        "TARGET_PRECISION    = 0.90\n",
        "\n",
        "class EpochTimer(keras.callbacks.Callback):\n",
        "    def __init__(self, total_epochs:int, label:str=\"\"):\n",
        "        super().__init__()\n",
        "        self.total_epochs = int(total_epochs)\n",
        "        self.label = label\n",
        "        self.epoch_durations = []\n",
        "        self._epoch_t0 = None\n",
        "        self._t0 = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self._t0 = time.time()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self._epoch_t0 = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if self._epoch_t0 is None:\n",
        "            return\n",
        "        dur = time.time() - self._epoch_t0\n",
        "        self.epoch_durations.append(dur)\n",
        "\n",
        "    @property\n",
        "    def avg_epoch_seconds(self):\n",
        "        return (sum(self.epoch_durations) / len(self.epoch_durations)) if self.epoch_durations else float(\"nan\")\n",
        "\n",
        "DEBUG_MODE = True  # <--- für schnellen Test; später auf False setzen\n",
        "\n",
        "# Originale Suchräume als Referenz (und für DEBUG_MODE=False)\n",
        "MODEL_CHOICES_FULL    = [\"baseline\",\"separable\"]\n",
        "LR_CHOICES_FULL       = [1e-3, 3e-4]\n",
        "WD_CHOICES_FULL       = [0.0, 1e-4]\n",
        "EPOCH_CHOICES_FULL    = [15, 30]\n",
        "BS_CHOICES_FULL       = [32]\n",
        "DROPOUT_CHOICES_FULL  = [0.0, 0.3]\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    # Radikal kleiner Suchraum für schnellen Test\n",
        "    MODEL_CHOICES   = [\"baseline\"]\n",
        "    LR_CHOICES      = [1e-3]\n",
        "    WD_CHOICES      = [0.0]\n",
        "    EPOCH_CHOICES   = [2]\n",
        "    BS_CHOICES      = [16]\n",
        "    DROPOUT_CHOICES = [0.0]\n",
        "else:\n",
        "    # 1:1 dein ursprünglicher Suchraum\n",
        "    MODEL_CHOICES   = MODEL_CHOICES_FULL\n",
        "    LR_CHOICES      = LR_CHOICES_FULL\n",
        "    WD_CHOICES      = WD_CHOICES_FULL\n",
        "    EPOCH_CHOICES   = EPOCH_CHOICES_FULL\n",
        "    BS_CHOICES      = BS_CHOICES_FULL\n",
        "    DROPOUT_CHOICES = DROPOUT_CHOICES_FULL\n",
        "\n",
        "def build_model_keras(name, l2reg=0.0, dropout=0.0):\n",
        "    if name == \"baseline\":\n",
        "        return build_baseline(l2reg=l2reg, dropout=dropout)\n",
        "    elif name == \"separable\":\n",
        "        return build_separable(l2reg=l2reg, dropout=dropout)\n",
        "    else:\n",
        "        raise ValueError(name)\n",
        "\n",
        "search_space = list(product(MODEL_CHOICES, LR_CHOICES, WD_CHOICES, EPOCH_CHOICES, BS_CHOICES, DROPOUT_CHOICES))\n",
        "\n",
        "fold_tuning_results = []\n",
        "saved_models = []\n",
        "\n",
        "if DEBUG_MODE:\n",
        "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=SEED)  # schneller: nur 2 Folds\n",
        "else:\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)  # dein ursprüngliches Setting\n",
        "\n",
        "uniq_ids   = np.array([r[\"dataset_id\"] for r in uniq_rows])\n",
        "uniq_label = np.array([r[\"label\"] for r in uniq_rows])\n",
        "\n",
        "for fold_idx, (train_val_idx, test_idx) in enumerate(skf.split(uniq_ids, uniq_label), start=1):\n",
        "    if DEBUG_MODE and fold_idx > 1:\n",
        "        break\n",
        "\n",
        "    tv_ids = uniq_ids[train_val_idx]; tv_lab = uniq_label[train_val_idx]\n",
        "    test_ids = uniq_ids[test_idx]\n",
        "    skf_tv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    inner_train_ids, val_ids = next(skf_tv.split(tv_ids, tv_lab))\n",
        "    train_ids = tv_ids[inner_train_ids]; val_ids = tv_ids[val_ids]\n",
        "\n",
        "    train_rows = rows_for_ids(train_ids)\n",
        "    val_rows   = rows_for_ids(val_ids)\n",
        "    test_rows  = rows_for_ids(test_ids)\n",
        "\n",
        "    # Im Debug-Modus: nur Teilmenge der Daten für schnelleren Test (auf Listen-Basis)\n",
        "    if DEBUG_MODE:\n",
        "        rng = np.random.default_rng(SEED)\n",
        "\n",
        "        def subsample(rows, n):\n",
        "            if len(rows) <= n:\n",
        "                return rows\n",
        "            idx = rng.choice(len(rows), size=n, replace=False)\n",
        "            return [rows[i] for i in idx]\n",
        "\n",
        "        train_rows = subsample(train_rows, 200)\n",
        "        val_rows   = subsample(val_rows, 100)\n",
        "        test_rows  = subsample(test_rows, 100)\n",
        "\n",
        "\n",
        "    trial_results = []\n",
        "    total_trials = len(search_space)\n",
        "\n",
        "    completed_trials = 0\n",
        "    trial_wall_times = []\n",
        "\n",
        "    print(f\"\\n========== Fold {fold_idx} / {skf.get_n_splits()} — {total_trials} Trials ==========\")\n",
        "\n",
        "    for (model_name, lr, wd, epochs, bs, dr) in search_space:\n",
        "        completed_trials += 1\n",
        "        trial_label = f\"fold{fold_idx} | model={model_name}, lr={lr}, l2={wd}, ep={epochs}, bs={bs}, dr={dr}\"\n",
        "        print(f\"\\n[Trial {completed_trials}/{total_trials}] {trial_label}\")\n",
        "\n",
        "        # Kopien der Listen (damit wir sie nicht in-place verändern)\n",
        "        train_rows_f = list(train_rows)\n",
        "        val_rows_f   = list(val_rows)\n",
        "        test_rows_f  = list(test_rows)\n",
        "\n",
        "        # Dataset-IDs für dataset-level metrics (val/test)\n",
        "        val_ids_all  = [r[\"dataset_id\"] for r in val_rows_f]\n",
        "        test_ids_all = [r[\"dataset_id\"] for r in test_rows_f]\n",
        "\n",
        "        # tf.data-Datasets mit deiner bestehenden Pipeline aus 2.3\n",
        "        ds_train = make_tf_dataset(train_rows_f, batch_size=bs, shuffle=True)\n",
        "        ds_val   = make_tf_dataset(val_rows_f,   batch_size=bs, shuffle=False)\n",
        "        ds_test  = make_tf_dataset(test_rows_f,  batch_size=bs, shuffle=False)\n",
        "\n",
        "        model = build_model_keras(model_name, l2reg=wd, dropout=dr)\n",
        "        compile_model(model, lr=lr)\n",
        "\n",
        "        et_cb = EpochTimer(total_epochs=epochs, label=trial_label)\n",
        "        cb = [et_cb]\n",
        "\n",
        "        _t0 = time.time()\n",
        "        model.fit(ds_train, validation_data=ds_val, epochs=epochs, verbose=0, callbacks=cb)\n",
        "        train_wall = time.time() - _t0\n",
        "        trial_wall_times.append(train_wall)\n",
        "\n",
        "        avg_ep = et_cb.avg_epoch_seconds\n",
        "        done_ep = len(et_cb.epoch_durations)\n",
        "        print(f\"[{trial_label}] finished: {done_ep} ep, \"\n",
        "              f\"avg {avg_ep:.1f}s/ep, wall {train_wall:.1f}s\")\n",
        "\n",
        "        remaining_trials = max(total_trials - completed_trials, 0)\n",
        "        mean_trial = (sum(trial_wall_times) / len(trial_wall_times)) if trial_wall_times else float(\"nan\")\n",
        "        eta_fold = remaining_trials * mean_trial if math.isfinite(mean_trial) else float(\"nan\")\n",
        "        print(f\"[Fold {fold_idx}] progress: {completed_trials}/{total_trials} trials done \"\n",
        "              f\"(avg {mean_trial:.1f}s/trial) — ETA fold ~{eta_fold:.1f}s\")\n",
        "\n",
        "        # --- Validation logits / calibration / threshold tuning ---\n",
        "        v_logits, v_true = predict_logits(model, ds_val)\n",
        "\n",
        "        # Temperature scaling always on raw logits\n",
        "        T = fit_temperature_tf(v_logits, v_true, steps=200, lr=0.05)\n",
        "\n",
        "        if v_logits.ndim == 1:\n",
        "            # Binary case: calibrate logits and work with calibrated probabilities\n",
        "            v_logits_cal = v_logits / T\n",
        "            v_probs_cal = 1.0 / (1.0 + np.exp(-v_logits_cal))\n",
        "\n",
        "            # F1-optimal threshold on calibrated probabilities\n",
        "            thr_info = find_best_threshold_f1(v_probs_cal, v_true)\n",
        "            best_thr = float(thr_info[\"threshold\"])\n",
        "\n",
        "            # Metrics using calibrated logits (so that probs & decisions are consistent)\n",
        "            val_m = evaluate_numpy(v_logits_cal, v_true, threshold=best_thr)\n",
        "            val_ece = expected_calibration_error(v_probs_cal, v_true)\n",
        "\n",
        "            # Gray-zone thresholds on calibrated probabilities\n",
        "            tl, th, tlth_info = find_gray_zone_thresholds(\n",
        "                v_probs_cal,\n",
        "                v_true,\n",
        "                target_precision=TARGET_PRECISION,\n",
        "                min_points_each_side=5,\n",
        "                grid_quantiles=99,\n",
        "            )\n",
        "            val_tern = evaluate_ternary(v_probs_cal, v_true, tl, th)\n",
        "\n",
        "            # Store threshold & ternary metrics\n",
        "            val_m.update({\n",
        "                \"threshold\": best_thr,\n",
        "                \"thr_prec\": float(thr_info[\"precision\"]),\n",
        "                \"thr_rec\": float(thr_info[\"recall\"]),\n",
        "                \"thr_f1\": float(thr_info[\"f1\"]),\n",
        "                \"ece\": val_ece,\n",
        "                \"abstain\": val_tern[\"abstain_rate\"],\n",
        "                \"acc_decided\": val_tern[\"acc_decided\"],\n",
        "            })\n",
        "\n",
        "            # Dataset-level metrics on calibrated probabilities\n",
        "            val_ds_metrics = evaluate_dataset_level(\n",
        "                probs=v_probs_cal,\n",
        "                y_true=v_true,\n",
        "                ds_ids=val_ids_all,\n",
        "                threshold=best_thr,\n",
        "            )\n",
        "            for k, v in val_ds_metrics.items():\n",
        "                val_m[f\"ds_{k}\"] = v\n",
        "\n",
        "        else:\n",
        "            # Multiclass fallback (not expected for this project, but kept for safety)\n",
        "            v_logits_cal = v_logits / T\n",
        "            v_probs_cal = tf.nn.softmax(v_logits_cal, axis=-1).numpy()\n",
        "            best_thr = 0.5\n",
        "            thr_info = None\n",
        "\n",
        "            val_m = evaluate_numpy(v_logits_cal, v_true, threshold=best_thr)\n",
        "            val_ece = expected_calibration_error(v_probs_cal.max(axis=1), v_true)\n",
        "            val_m.update({\"ece\": val_ece})\n",
        "\n",
        "            tl, th, tlth_info = DEFAULT_GRAY_T_LOW, DEFAULT_GRAY_T_HIGH, {\"fallback\": True, \"multiclass\": True}\n",
        "\n",
        "        # --- Test split: use same T, thresholds etc. ---\n",
        "        t_logits, t_true = predict_logits(model, ds_test)\n",
        "\n",
        "        if t_logits.ndim == 1:\n",
        "            t_logits_cal = t_logits / T\n",
        "            t_probs_cal = 1.0 / (1.0 + np.exp(-t_logits_cal))\n",
        "\n",
        "            test_m = evaluate_numpy(t_logits_cal, t_true, threshold=best_thr)\n",
        "            test_ece = expected_calibration_error(t_probs_cal, t_true)\n",
        "\n",
        "            test_tern = evaluate_ternary(t_probs_cal, t_true, tl, th)\n",
        "            test_m.update({\n",
        "                \"ece\": test_ece,\n",
        "                \"abstain\": test_tern[\"abstain_rate\"],\n",
        "                \"acc_decided\": test_tern[\"acc_decided\"],\n",
        "                \"threshold\": float(best_thr),\n",
        "            })\n",
        "\n",
        "            # Dataset-level metrics on calibrated probabilities for test\n",
        "            test_ds_metrics = evaluate_dataset_level(\n",
        "                probs=t_probs_cal,\n",
        "                y_true=t_true,\n",
        "                ds_ids=test_ids_all,\n",
        "                threshold=best_thr,\n",
        "            )\n",
        "            for k, v in test_ds_metrics.items():\n",
        "                test_m[f\"ds_{k}\"] = v\n",
        "\n",
        "        else:\n",
        "            t_logits_cal = t_logits / T\n",
        "            t_probs_cal = tf.nn.softmax(t_logits_cal, axis=-1).numpy()\n",
        "\n",
        "            test_m = evaluate_numpy(t_logits_cal, t_true, threshold=best_thr)\n",
        "            test_ece = expected_calibration_error(t_probs_cal.max(axis=1), t_true)\n",
        "            test_m.update({\"ece\": test_ece})\n",
        "\n",
        "        hp = {\"model\": model_name, \"lr\": lr, \"l2\": wd, \"epochs\": epochs, \"bs\": bs, \"dropout\": dr}\n",
        "        path = MODELS_DIR / f\"fold{fold_idx}_{model_name}_lr{lr}_l2{wd}_ep{epochs}_dr{dr}.keras\"\n",
        "        model.save(path, include_optimizer=False)\n",
        "\n",
        "        trial_results.append({\n",
        "            \"fold\": fold_idx,\n",
        "            \"hparams\": hp,\n",
        "            \"val\": val_m,\n",
        "            \"test\": test_m,\n",
        "            \"path\": str(path),\n",
        "            \"T\": float(T),\n",
        "            \"t_low\": float(tl),\n",
        "            \"t_high\": float(th),\n",
        "            \"t_opt\": float(best_thr),      # <--- NEU\n",
        "            \"tlth_info\": tlth_info\n",
        "        })\n",
        "\n",
        "        saved_models.append((fold_idx, model_name, hp, str(path), float(T)))\n",
        "\n",
        "    best = sorted(trial_results, key=lambda r: r[\"val\"].get(\"roc_auc\", -1), reverse=True)[:3]\n",
        "    fold_tuning_results.append({\"fold\": fold_idx, \"trials\": trial_results, \"topk\": best})\n",
        "\n",
        "def soft_vote(*probs_list):\n",
        "    return np.mean(np.stack(probs_list, axis=0), axis=0)\n",
        "\n",
        "report_csv = REPORTS_DIR / \"cv_results_tuned_dropout_keras.csv\"\n",
        "with open(report_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    import csv, json\n",
        "    header = [\n",
        "      \"fold\",\"model\",\"lr\",\"l2\",\"epochs\",\"bs\",\"dropout\",\n",
        "      \"split\",\n",
        "      \"acc\",\"f1\",\"roc_auc\",\"pr_auc\",\"ece\",\"abstain\",\"acc_decided\",\"threshold\",\n",
        "      \"ds_acc\",\"ds_f1\",\"ds_roc_auc\",\"ds_pr_auc\",\n",
        "      \"model_path\",\"T\",\"t_low\",\"t_high\",\"target_precision\",\"fallback\"\n",
        "    ]\n",
        "    w = csv.writer(f); w.writerow(header)\n",
        "    for fr in fold_tuning_results:\n",
        "      for r in fr[\"trials\"]:\n",
        "          hp = r[\"hparams\"]; info = r.get(\"tlth_info\", {})\n",
        "          fallback = info.get(\"fallback\", False)\n",
        "          tprec = info.get(\"target_precision\", TARGET_PRECISION)\n",
        "          for split in [\"val\",\"test\"]:\n",
        "              m = r[split]\n",
        "\n",
        "              ds_acc     = m.get(\"ds_acc\", \"\")\n",
        "              ds_f1      = m.get(\"ds_f1\", \"\")\n",
        "              ds_roc_auc = m.get(\"ds_roc_auc\", \"\")\n",
        "              ds_pr_auc  = m.get(\"ds_pr_auc\", \"\")\n",
        "\n",
        "              w.writerow([\n",
        "                  fr[\"fold\"], hp[\"model\"], hp[\"lr\"], hp[\"l2\"], hp[\"epochs\"], hp[\"bs\"], hp[\"dropout\"],\n",
        "                  split,\n",
        "                  m.get(\"acc\",\"\"), m.get(\"f1\",\"\"), m.get(\"roc_auc\",\"\"), m.get(\"pr_auc\",\"\"),\n",
        "                  m.get(\"ece\",\"\"), m.get(\"abstain\",\"\"), m.get(\"acc_decided\",\"\"), m.get(\"threshold\",\"\"),\n",
        "                  ds_acc, ds_f1, ds_roc_auc, ds_pr_auc,\n",
        "                  r[\"path\"], r[\"T\"], r[\"t_low\"], r[\"t_high\"], tprec, int(fallback)\n",
        "              ])\n",
        "print(\"Saved CV report:\", report_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Best-Model Visuals: CV-Auswertung & Dashboard ===\n",
        "import json, gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_cv_results(csv_path: str | Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df = df.dropna(how=\"all\")\n",
        "    for col in [\n",
        "        \"acc\",\"f1\",\"roc_auc\",\"pr_auc\",\"ece\",\"abstain\",\"acc_decided\",\"threshold\",\n",
        "        \"ds_acc\",\"ds_f1\",\"ds_roc_auc\",\"ds_pr_auc\",\n",
        "        \"T\",\"t_low\",\"t_high\",\"target_precision\"\n",
        "    ]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "def best_row(df_val: pd.DataFrame, primary=\"roc_auc\", secondary=\"f1\", higher_is_better=None):\n",
        "    p, s = primary.lower(), secondary.lower()\n",
        "    hib = {\"roc_auc\": True, \"f1\": True, \"acc\": True, \"pr_auc\": True, \"ece\": False}\n",
        "    if higher_is_better:\n",
        "        hib.update({k.lower(): v for k, v in higher_is_better.items()})\n",
        "    if df_val.empty:\n",
        "        raise ValueError(\"No validation rows found in CSV (split=='val').\")\n",
        "    def sort_key(row):\n",
        "        pk = row.get(p, np.nan); sk = row.get(s, np.nan)\n",
        "        pk = pk if hib.get(p, True) else -pk\n",
        "        sk = sk if hib.get(s, True) else -sk\n",
        "        return (np.nan_to_num(pk, nan=-np.inf), np.nan_to_num(sk, nan=-np.inf))\n",
        "    best_idx = max(df_val.index, key=lambda i: sort_key(df_val.loc[i]))\n",
        "    return df_val.loc[best_idx]\n",
        "\n",
        "def summarize_hparams(df_val: pd.DataFrame,\n",
        "                      primary: str = \"roc_auc\",\n",
        "                      secondary: str = \"f1\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregate validation metrics across folds for each hyperparameter configuration.\n",
        "    \"\"\"\n",
        "    cfg_cols = [\"model\",\"lr\",\"l2\",\"epochs\",\"bs\",\"dropout\"]\n",
        "    g = df_val.groupby(cfg_cols)\n",
        "\n",
        "    rows = []\n",
        "    for cfg, sub in g:\n",
        "        row = dict(zip(cfg_cols, cfg))\n",
        "        row[\"n_folds\"] = sub[\"fold\"].nunique()\n",
        "        for col in [primary, secondary]:\n",
        "            row[f\"mean_{col}\"] = sub[col].mean()\n",
        "            row[f\"std_{col}\"]  = sub[col].std(ddof=0)\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "def select_best_config(hp_df: pd.DataFrame,\n",
        "                       primary: str = \"roc_auc\",\n",
        "                       secondary: str = \"f1\") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Select the hyperparameter configuration with best mean primary metric,\n",
        "    breaking ties by mean secondary metric.\n",
        "    \"\"\"\n",
        "    p = f\"mean_{primary}\"\n",
        "    s = f\"mean_{secondary}\"\n",
        "\n",
        "    if hp_df.empty:\n",
        "        raise ValueError(\"Hyperparameter summary is empty.\")\n",
        "\n",
        "    best_idx = max(\n",
        "        hp_df.index,\n",
        "        key=lambda i: (hp_df.loc[i, p], hp_df.loc[i, s])\n",
        "    )\n",
        "    return hp_df.loc[best_idx]\n",
        "\n",
        "def _ensure_np(a): return a if isinstance(a, np.ndarray) else np.asarray(a)\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_prob, threshold=0.5, save_path: str | Path = \"cm.png\"):\n",
        "    y_true = _ensure_np(y_true).astype(int)\n",
        "    y_pred = (_ensure_np(y_prob).ravel() >= threshold).astype(int)\n",
        "    tp = int(((y_true==1)&(y_pred==1)).sum())\n",
        "    tn = int(((y_true==0)&(y_pred==0)).sum())\n",
        "    fp = int(((y_true==0)&(y_pred==1)).sum())\n",
        "    fn = int(((y_true==1)&(y_pred==0)).sum())\n",
        "    M = np.array([[tn, fp],[fn, tp]])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
        "    im = ax.imshow(M, aspect=\"equal\")\n",
        "    for (i,j), v in np.ndenumerate(M):\n",
        "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels([\"Pred 0\",\"Pred 1\"]); ax.set_yticklabels([\"True 0\",\"True 1\"])\n",
        "    ax.set_title(\"Confusion Matrix (best model, split=val)\")\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n",
        "def plot_confusion_matrix_ternary(y_true, y_prob, t_low=0.3, t_high=0.7, save_path: str | Path = \"cm_ternary.png\"):\n",
        "    y = _ensure_np(y_true).astype(int).ravel()\n",
        "    p = _ensure_np(y_prob).ravel()\n",
        "    pred3 = np.full_like(p, 1, dtype=int)\n",
        "    pred3[p <= t_low] = 0\n",
        "    pred3[p >= t_high] = 2\n",
        "    M = np.zeros((2,3), dtype=int)\n",
        "    for yi, pi in zip(y, pred3):\n",
        "        M[yi, pi] += 1\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6.5,4.5))\n",
        "    im = ax.imshow(M, aspect=\"equal\")\n",
        "    for (i,j), v in np.ndenumerate(M):\n",
        "        ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
        "    ax.set_xticks([0,1,2]); ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels([\"Pred: no fat\",\"Pred: uncertain\",\"Pred: fat\"], rotation=15, ha=\"right\")\n",
        "    ax.set_yticklabels([\"True 0 (no fat)\",\"True 1 (fat)\"])\n",
        "    ax.set_title(\"Ternary Confusion (best model, split=val)\")\n",
        "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n",
        "def plot_roc(y_true, y_prob, save_path: str | Path = \"roc.png\"):\n",
        "    from sklearn.metrics import roc_curve, auc\n",
        "    y_true = _ensure_np(y_true).astype(int)\n",
        "    y_prob = _ensure_np(y_prob).ravel()\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    A = auc(fpr, tpr)\n",
        "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
        "    ax.plot(fpr, tpr, label=f\"AUC={A:.3f}\")\n",
        "    ax.plot([0,1],[0,1],'--',lw=1)\n",
        "    ax.set_xlabel(\"FPR\"); ax.set_ylabel(\"TPR\"); ax.legend()\n",
        "    ax.set_title(\"ROC (best model, split=val)\")\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n",
        "def plot_reliability(y_true, y_prob, n_bins=10, save_path: str | Path = \"reliability.png\"):\n",
        "    y_true = _ensure_np(y_true).astype(int)\n",
        "    y_prob = _ensure_np(y_prob).ravel()\n",
        "    bins = np.linspace(0,1,n_bins+1)\n",
        "    idx = np.digitize(y_prob, bins)-1\n",
        "    xs, ys = [], []\n",
        "    for b in range(n_bins):\n",
        "        m = (idx==b)\n",
        "        if not np.any(m):\n",
        "            continue\n",
        "        xs.append(y_prob[m].mean())\n",
        "        ys.append(y_true[m].mean())\n",
        "    fig, ax = plt.subplots(figsize=(4.5,4))\n",
        "    ax.plot([0,1],[0,1],'--', lw=1)\n",
        "    ax.plot(xs, ys, marker='o')\n",
        "    ax.set_xlabel(\"confidence\"); ax.set_ylabel(\"empirical accuracy\")\n",
        "    ax.set_title(\"Reliability (best model, split=val)\")\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n",
        "def plot_cv_bars(df_val: pd.DataFrame, metric: str, save_path: str | Path):\n",
        "    metric = metric.lower()\n",
        "    g = df_val.groupby(\"fold\", as_index=False)[metric].max().sort_values(\"fold\")\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.bar(g[\"fold\"].astype(str), g[metric].values)\n",
        "    ax.set_xlabel(\"Fold\"); ax.set_ylabel(metric); ax.set_title(f\"Val {metric} (best per Fold)\")\n",
        "    fig.tight_layout(); fig.savefig(save_path, dpi=160, bbox_inches=\"tight\")\n",
        "    plt.close(fig); gc.collect()\n",
        "    return str(save_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "sz3P_qJw2rqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "BEST_MODEL_PATH = MODELS_DIR / \"best_model_keras.h5\"\n",
        "BEST_MODEL_META = REPORTS_DIR / \"best_model_meta.json\"\n",
        "\n",
        "def show_best_model_dashboard(csv_path: str | Path,\n",
        "                              out_dir: str | Path,\n",
        "                              primary: str = \"roc_auc\",\n",
        "                              secondary: str = \"f1\",\n",
        "                              with_curves: bool = True,\n",
        "                              ternary_thresholds=None):\n",
        "    \"\"\"\n",
        "    Read CV results, aggregate metrics across folds per hyperparameter configuration,\n",
        "    select the best configuration, train one final model on all data (with internal\n",
        "    validation split), and save this final model + metadata to:\n",
        "\n",
        "      - BEST_MODEL_PATH  (e.g. models_tf/best_model_keras.h5)\n",
        "      - BEST_MODEL_META  (e.g. reports/best_model_meta.json)\n",
        "    \"\"\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[DEBUG] Starting show_best_model_dashboard with csv_path={csv_path}, out_dir={out_dir}\")\n",
        "\n",
        "    # 1) Load CSV & aggregate hyperparameters across folds\n",
        "    print(f\"[DEBUG] Loading CV results from: {csv_path}\")\n",
        "    df = load_cv_results(csv_path)\n",
        "    df_val = df[df[\"split\"] == \"val\"].copy()\n",
        "    if df_val.empty:\n",
        "        raise ValueError(\"Keine Zeilen mit split=='val' in CV-CSV gefunden.\")\n",
        "\n",
        "    print(\"[DEBUG] Loaded DataFrame head:\")\n",
        "    print(df.head())\n",
        "\n",
        "    hp_summary = summarize_hparams(df_val, primary=primary, secondary=secondary)\n",
        "    if hp_summary.empty:\n",
        "        raise ValueError(\"Keine Hyperparameter-Konfigurationen in den Validation-Ergebnissen gefunden.\")\n",
        "\n",
        "    best_cfg = select_best_config(hp_summary, primary=primary, secondary=secondary)\n",
        "\n",
        "    if hasattr(best_cfg, \"to_dict\"):\n",
        "        best_cfg_dict = best_cfg.to_dict()\n",
        "    else:\n",
        "        best_cfg_dict = dict(best_cfg)\n",
        "\n",
        "    print(\"[DEBUG] Best aggregated hyperparameters across folds:\")\n",
        "    print(best_cfg_dict)\n",
        "\n",
        "    # Bar-Plot pro Fold (wie bisher, sample-level)\n",
        "    bars_png = plot_cv_bars(df_val, primary, out_dir / f\"cv_{primary}.png\")\n",
        "\n",
        "    # 2) Final training on all data (with internal validation split)\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "    # uniq_ids / uniq_label wurden in 2.6 global definiert\n",
        "    skf_final = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "    final_train_idx, final_val_idx = next(skf_final.split(uniq_ids, uniq_label))\n",
        "    final_train_ids = uniq_ids[final_train_idx]\n",
        "    final_val_ids   = uniq_ids[final_val_idx]\n",
        "\n",
        "    final_train_rows = rows_for_ids(final_train_ids)\n",
        "    final_val_rows   = rows_for_ids(final_val_ids)\n",
        "\n",
        "    # Hyperparameter aus der besten Konfiguration holen\n",
        "    model_name = best_cfg_dict[\"model\"]\n",
        "    lr         = best_cfg_dict[\"lr\"]\n",
        "    wd         = best_cfg_dict[\"l2\"]\n",
        "    epochs     = int(best_cfg_dict[\"epochs\"])\n",
        "    bs         = int(best_cfg_dict[\"bs\"])\n",
        "    dropout    = best_cfg_dict[\"dropout\"]\n",
        "\n",
        "    if DEBUG_MODE:\n",
        "        final_train_rows = final_train_rows[:500]\n",
        "        final_val_rows   = final_val_rows[:200]\n",
        "        # jetzt ist epochs bereits definiert -> kein UnboundLocalError mehr\n",
        "        epochs = min(epochs, 2)\n",
        "\n",
        "    final_val_ids_all = [r[\"dataset_id\"] for r in final_val_rows]\n",
        "\n",
        "    ds_train_final = make_tf_dataset(final_train_rows, batch_size=bs, shuffle=True)\n",
        "    ds_val_final   = make_tf_dataset(final_val_rows,   batch_size=bs, shuffle=False)\n",
        "\n",
        "    model = build_model_keras(model_name, l2reg=wd, dropout=dropout)\n",
        "    compile_model(model, lr=lr)\n",
        "\n",
        "    print(\"[DEBUG] Fitting final model with best hyperparameters on all data (with internal validation split)...\")\n",
        "    model.fit(ds_train_final, validation_data=ds_val_final, epochs=epochs, verbose=0)\n",
        "\n",
        "    # 3) Calibration, threshold & gray zone for the final model\n",
        "    v_logits_final, v_true_final = predict_logits(model, ds_val_final)\n",
        "    T_final = fit_temperature_tf(v_logits_final, v_true_final, steps=200, lr=0.05)\n",
        "\n",
        "    if v_logits_final.ndim == 1:\n",
        "        v_logits_cal_final = v_logits_final / T_final\n",
        "        v_probs_cal_final = 1.0 / (1.0 + np.exp(-v_logits_cal_final))\n",
        "\n",
        "        thr_info_final = find_best_threshold_f1(v_probs_cal_final, v_true_final)\n",
        "        best_thr_final = float(thr_info_final[\"threshold\"])\n",
        "\n",
        "        val_m_final = evaluate_numpy(v_logits_cal_final, v_true_final, threshold=best_thr_final)\n",
        "        val_ece_final = expected_calibration_error(v_probs_cal_final, v_true_final)\n",
        "\n",
        "        t_low_final, t_high_final, tlth_info_final = find_gray_zone_thresholds(\n",
        "            v_probs_cal_final,\n",
        "            v_true_final,\n",
        "            target_precision=TARGET_PRECISION,\n",
        "            min_points_each_side=5,\n",
        "            grid_quantiles=99,\n",
        "        )\n",
        "        val_tern_final = evaluate_ternary(v_probs_cal_final, v_true_final, t_low_final, t_high_final)\n",
        "\n",
        "        val_m_final.update({\n",
        "            \"threshold\": best_thr_final,\n",
        "            \"thr_prec\": float(thr_info_final[\"precision\"]),\n",
        "            \"thr_rec\": float(thr_info_final[\"recall\"]),\n",
        "            \"thr_f1\": float(thr_info_final[\"f1\"]),\n",
        "            \"ece\": val_ece_final,\n",
        "            \"abstain\": val_tern_final[\"abstain_rate\"],\n",
        "            \"acc_decided\": val_tern_final[\"acc_decided\"],\n",
        "        })\n",
        "\n",
        "        # Dataset-level metrics for final model\n",
        "        val_ds_metrics_final = evaluate_dataset_level(\n",
        "            probs=v_probs_cal_final,\n",
        "            y_true=v_true_final,\n",
        "            ds_ids=final_val_ids_all,\n",
        "            threshold=best_thr_final,\n",
        "        )\n",
        "        for k, v in val_ds_metrics_final.items():\n",
        "            val_m_final[f\"ds_{k}\"] = v\n",
        "\n",
        "    else:\n",
        "        # Multiclass fallback (not expected here)\n",
        "        v_logits_cal_final = v_logits_final / T_final\n",
        "        v_probs_cal_final = tf.nn.softmax(v_logits_cal_final, axis=-1).numpy()\n",
        "        best_thr_final = 0.5\n",
        "\n",
        "        val_m_final = evaluate_numpy(v_logits_cal_final, v_true_final, threshold=best_thr_final)\n",
        "        val_ece_final = expected_calibration_error(v_probs_cal_final.max(axis=1), v_true_final)\n",
        "        val_m_final.update({\"ece\": val_ece_final})\n",
        "\n",
        "        t_low_final, t_high_final, tlth_info_final = DEFAULT_GRAY_T_LOW, DEFAULT_GRAY_T_HIGH, {\"fallback\": True, \"multiclass\": True}\n",
        "\n",
        "    # 4) Save final best model\n",
        "    BEST_MODEL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "    model.save(BEST_MODEL_PATH, include_optimizer=False)\n",
        "    print(f\"[DEBUG] Final best model saved to: {BEST_MODEL_PATH}\")\n",
        "\n",
        "    # 5) Write meta JSON (used by Evaluation notebook)\n",
        "    best_meta = {\n",
        "        \"fold\": -1,  # no single fold; this is the final retrained model\n",
        "        \"model\": model_name,\n",
        "        \"original_model_path\": None,  # we no longer copy a single fold model\n",
        "        \"model_path\": str(BEST_MODEL_PATH),\n",
        "        \"T\": float(T_final),\n",
        "        \"threshold\": float(best_thr_final),\n",
        "        \"t_opt\": float(best_thr_final),\n",
        "        \"t_low\": float(t_low_final),\n",
        "        \"t_high\": float(t_high_final),\n",
        "        \"target_precision\": float(TARGET_PRECISION),\n",
        "        # final validation metrics (sample-level)\n",
        "        \"acc\": float(val_m_final.get(\"acc\", np.nan)),\n",
        "        \"f1\": float(val_m_final.get(\"f1\", np.nan)),\n",
        "        \"roc_auc\": float(val_m_final.get(\"roc_auc\", np.nan)),\n",
        "        \"pr_auc\": float(val_m_final.get(\"pr_auc\", np.nan)),\n",
        "        # final validation metrics (dataset-level)\n",
        "        \"ds_acc\": float(val_m_final.get(\"ds_acc\", np.nan)),\n",
        "        \"ds_f1\": float(val_m_final.get(\"ds_f1\", np.nan)),\n",
        "        \"ds_roc_auc\": float(val_m_final.get(\"ds_roc_auc\", np.nan)),\n",
        "        \"ds_pr_auc\": float(val_m_final.get(\"ds_pr_auc\", np.nan)),\n",
        "        # aggregated CV metrics over folds for this configuration\n",
        "        \"cv_mean_roc_auc\": float(best_cfg_dict.get(\"mean_roc_auc\", np.nan)),\n",
        "        \"cv_std_roc_auc\": float(best_cfg_dict.get(\"std_roc_auc\", np.nan)),\n",
        "        \"cv_mean_f1\": float(best_cfg_dict.get(\"mean_f1\", np.nan)),\n",
        "        \"cv_std_f1\": float(best_cfg_dict.get(\"std_f1\", np.nan)),\n",
        "    }\n",
        "\n",
        "    BEST_MODEL_META.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(BEST_MODEL_META, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(best_meta, f, indent=2)\n",
        "    print(f\"[DEBUG] Best model meta written to: {BEST_MODEL_META}\")\n",
        "    print(\"[DEBUG] best_meta content:\")\n",
        "    print(best_meta)\n",
        "\n",
        "    thresholds_used = (float(t_low_final), float(t_high_final))\n",
        "\n",
        "    results = {\n",
        "        \"bars\": bars_png,\n",
        "        \"best_row\": best_cfg_dict,\n",
        "        \"thresholds\": thresholds_used,\n",
        "        \"best_model_path\": str(BEST_MODEL_PATH),\n",
        "        \"best_model_meta\": str(BEST_MODEL_META),\n",
        "    }\n",
        "\n",
        "    # Optional plots if _Y_TRUE_ / _Y_PROB_ are defined externally\n",
        "    if with_curves and (\"_Y_TRUE_\" in globals()) and (\"_Y_PROB_\" in globals()):\n",
        "        roc_png = plot_roc(_Y_TRUE_, _Y_PROB_, out_dir / \"roc.png\")\n",
        "        cm_png  = plot_confusion_matrix(_Y_TRUE_, _Y_PROB_, 0.5, out_dir / \"cm.png\")\n",
        "        rel_png = plot_reliability(_Y_TRUE_, _Y_PROB_, 10, out_dir / \"reliability.png\")\n",
        "        tcm_png = plot_confusion_matrix_ternary(_Y_TRUE_, _Y_PROB_, t_low_final, t_high_final, out_dir / \"cm_ternary.png\")\n",
        "        results.update({\n",
        "            \"roc\": roc_png,\n",
        "            \"cm\": cm_png,\n",
        "            \"reliability\": rel_png,\n",
        "            \"cm_ternary\": tcm_png,\n",
        "        })\n",
        "\n",
        "    print(f\"Bestes Modell kopiert nach: {BEST_MODEL_PATH}\")\n",
        "    print(f\"Metadaten gespeichert nach: {BEST_MODEL_META}\")\n",
        "\n",
        "    return results\n",
        "\n"
      ],
      "metadata": {
        "id": "ctwMSELXumDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2.7) Bestes Modell auswählen & Meta-JSON schreiben ===\n",
        "\n",
        "# Pfad zur in 2.6 geschriebenen CV-CSV\n",
        "csv_path = report_csv  # = REPORTS_DIR / \"cv_results_tuned_dropout_keras.csv\"\n",
        "\n",
        "# Ordner für die Plots/Dashboard-Ausgabe\n",
        "dashboard_dir = PLOTS_DIR / \"best_model_dashboard\"\n",
        "\n",
        "show_best_model_dashboard(\n",
        "    csv_path=csv_path,\n",
        "    out_dir=dashboard_dir,\n",
        "    primary=\"roc_auc\",   # wie im Dozenten-Feedback: AUC als Hauptkriterium ok\n",
        "    secondary=\"f1\",      # F1 als sekundäres Kriterium\n",
        "    with_curves=True,\n",
        ")\n",
        "\n",
        "print(\"Bestes Modell gespeichert unter:\", BEST_MODEL_PATH)\n",
        "print(\"Meta-Informationen gespeichert in:\", BEST_MODEL_META)\n"
      ],
      "metadata": {
        "id": "NddOcIo5wEkq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "549d6b5c-7e90-4fc2-b11b-bbe5d3d065a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Starting show_best_model_dashboard with csv_path=/content/drive/MyDrive/Generated Data for Data science project/reports/cv_results_tuned_dropout_keras.csv, out_dir=/content/drive/MyDrive/Generated Data for Data science project/plots/best_model_dashboard\n",
            "[DEBUG] Loading CV results from: /content/drive/MyDrive/Generated Data for Data science project/reports/cv_results_tuned_dropout_keras.csv\n",
            "[DEBUG] Loaded DataFrame head:\n",
            "   fold     model     lr   l2  epochs  bs  dropout split   acc        f1  ...  \\\n",
            "0     1  baseline  0.001  0.0       2  16      0.0   val  0.68  0.698113  ...   \n",
            "1     1  baseline  0.001  0.0       2  16      0.0  test  0.66  0.701754  ...   \n",
            "\n",
            "     ds_acc     ds_f1  ds_roc_auc  ds_pr_auc  \\\n",
            "0  0.716981  0.727273    0.772989   0.776014   \n",
            "1  0.655556  0.693069    0.734196   0.626571   \n",
            "\n",
            "                                          model_path    T  t_low  t_high  \\\n",
            "0  /content/drive/MyDrive/Generated Data for Data...  1.0    0.3     0.7   \n",
            "1  /content/drive/MyDrive/Generated Data for Data...  1.0    0.3     0.7   \n",
            "\n",
            "   target_precision  fallback  \n",
            "0               0.9         1  \n",
            "1               0.9         1  \n",
            "\n",
            "[2 rows x 26 columns]\n",
            "[DEBUG] Best aggregated hyperparameters across folds:\n",
            "{'model': 'baseline', 'lr': 0.001, 'l2': 0.0, 'epochs': 2, 'bs': 16, 'dropout': 0.0, 'n_folds': 1, 'mean_roc_auc': 0.7623444399839421, 'std_roc_auc': 0.0, 'mean_f1': 0.6981132075471698, 'std_f1': 0.0}\n",
            "[DEBUG] Fitting final model with best hyperparameters on all data (with internal validation split)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[warn] Keine (t_low,t_high) gefunden, die target_precision erfüllen. Fallback auf (0.30, 0.70).\n",
            "[DEBUG] Final best model saved to: /content/drive/MyDrive/Generated Data for Data science project/models_tf/best_model_keras.h5\n",
            "[DEBUG] Best model meta written to: /content/drive/MyDrive/Generated Data for Data science project/reports/best_model_meta.json\n",
            "[DEBUG] best_meta content:\n",
            "{'fold': -1, 'model': 'baseline', 'original_model_path': None, 'model_path': '/content/drive/MyDrive/Generated Data for Data science project/models_tf/best_model_keras.h5', 'T': 1.0, 'threshold': 0.01, 't_opt': 0.01, 't_low': 0.3, 't_high': 0.7, 'target_precision': 0.9, 'acc': 0.28, 'f1': 0.4375, 'roc_auc': 0.5892857142857143, 'pr_auc': 0.46616560751094704, 'ds_acc': 0.28, 'ds_f1': 0.4375, 'ds_roc_auc': 0.6507936507936508, 'ds_pr_auc': 0.5700320208584327, 'cv_mean_roc_auc': 0.7623444399839421, 'cv_std_roc_auc': 0.0, 'cv_mean_f1': 0.6981132075471698, 'cv_std_f1': 0.0}\n",
            "Bestes Modell kopiert nach: /content/drive/MyDrive/Generated Data for Data science project/models_tf/best_model_keras.h5\n",
            "Metadaten gespeichert nach: /content/drive/MyDrive/Generated Data for Data science project/reports/best_model_meta.json\n",
            "Bestes Modell gespeichert unter: /content/drive/MyDrive/Generated Data for Data science project/models_tf/best_model_keras.h5\n",
            "Meta-Informationen gespeichert in: /content/drive/MyDrive/Generated Data for Data science project/reports/best_model_meta.json\n"
          ]
        }
      ]
    }
  ]
}