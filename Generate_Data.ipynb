{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laraAkg/Data-Science-Project/blob/main/Generate_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14bfee8"
      },
      "source": [
        "This cell imports necessary libraries for the project.\n",
        "\n",
        "It also configures Matplotlib to use a non-GUI backend and checks if the code is running in Google Colab. If in Colab, it mounts Google Drive to access files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCwfkeZyPkMu",
        "outputId": "50b97381-1934-46ba-ce79-7ac254860c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import gc\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import scipy.stats as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKxL8iy5wwQ5"
      },
      "source": [
        "This cell defines the directory structure for the project, creating paths for storing datasets, plots, metadata, models, reports, and real-world data.\n",
        "\n",
        "- `DEFAULT_PROJECT_DIR`: Sets the default directory name within Google Drive.\n",
        "- `BASE_DIR`: Determines the base directory for project outputs, either in Google Drive if running in Colab or a local `./project_outputs` directory otherwise.\n",
        "- `DATA_DIR`, `PLOTS_DIR`, `META_DIR`, `MODELS_DIR`, `REPORTS_DIR`, `REAL_DIR`: Define specific subdirectories within the `BASE_DIR` for different types of project artifacts.\n",
        "- The code then iterates through the defined directories and creates them if they don't already exist using `mkdir(parents=True, exist_ok=True)`.\n",
        "- `IMG_SIZE`, `DPI`, `FIGSIZE`: Define constants for image size, dots per inch for saving figures, and the figure size for matplotlib plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fd779f6",
        "outputId": "7e45f256-bc5f-4683-c6e6-4bd50ce08ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE_DIR: /content/drive/MyDrive/Generated Data for Data science project\n"
          ]
        }
      ],
      "source": [
        "DEFAULT_PROJECT_DIR = \"MyDrive/Generated Data for Data science project\"\n",
        "BASE_DIR = Path(\"/content/drive\") / DEFAULT_PROJECT_DIR if IN_COLAB else Path(\"./project_outputs\")\n",
        "\n",
        "DATA_DIR   = BASE_DIR / \"datasets\"\n",
        "PLOTS_DIR  = BASE_DIR / \"plots\"\n",
        "META_DIR   = BASE_DIR / \"metadata\"\n",
        "MODELS_DIR = BASE_DIR / \"models_tf\"\n",
        "REPORTS_DIR= BASE_DIR / \"reports\"\n",
        "REAL_DIR   = BASE_DIR / \"real\"\n",
        "\n",
        "for p in [DATA_DIR, PLOTS_DIR, META_DIR, MODELS_DIR, REPORTS_DIR, REAL_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = (128, 128)  # (H, W)\n",
        "DPI      = 150\n",
        "FIGSIZE  = (4.0, 4.0)\n",
        "\n",
        "print(\"BASE_DIR:\", BASE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "894a3600"
      },
      "source": [
        "This cell contains utility functions and configures Matplotlib for plot creation and saving.\n",
        "\n",
        "- `ensure_parent`: Ensures that the parent directory of a file exists.\n",
        "- `_patched_savefig`: A modified version of the Matplotlib `savefig` function that redirects relative paths to the `PLOTS_DIR` and ensures that the parent directories exist.\n",
        "- `save_dataset`: Saves a NumPy array as a `.npy` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ba0bcb"
      },
      "outputs": [],
      "source": [
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "plt.rcParams[\"figure.dpi\"] = DPI\n",
        "plt.rcParams[\"savefig.dpi\"] = DPI\n",
        "plt.rcParams[\"figure.figsize\"] = FIGSIZE\n",
        "\n",
        "def ensure_parent(path: Path):\n",
        "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True); return path\n",
        "\n",
        "_original_savefig = plt.savefig\n",
        "def _patched_savefig(*args, **kwargs):\n",
        "    if len(args) > 0 and isinstance(args[0], (str, Path)):\n",
        "        fname = Path(args[0])\n",
        "        if not fname.is_absolute():\n",
        "            fname = PLOTS_DIR / fname\n",
        "        ensure_parent(fname)\n",
        "        args = (str(fname),) + tuple(args[1:])\n",
        "    else:\n",
        "        auto = PLOTS_DIR / f\"plot_{int(time.time()*1000)}.png\"\n",
        "        ensure_parent(auto)\n",
        "        args = (str(auto),) + args\n",
        "    kwargs.setdefault(\"dpi\", DPI)\n",
        "    return _original_savefig(*args, **kwargs)\n",
        "plt.savefig = _patched_savefig\n",
        "\n",
        "def save_dataset(arr: np.ndarray, path: Path):\n",
        "    path = ensure_parent(path)\n",
        "    np.save(path, arr.astype(np.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "997f8483"
      },
      "source": [
        "This cell defines different probability distributions, functions to sample from them, and a function to determine if a distribution is considered \"heavy-tailed\" based on its parameters.\n",
        "\n",
        "- `DistSpec`: A data class to hold the name, parameter generation function, and sampling function for each distribution.\n",
        "- `rng_for`: Creates a new random number generator based on a distribution name and index.\n",
        "- `sample_*`: Functions to generate samples from specific distributions (normal, exponential, Pareto, Student's t, lognormal, and a mix of normal and Pareto).\n",
        "- `p_*`: Functions to generate random parameters for each distribution.\n",
        "- `DISTRIBUTIONS`: A list of `DistSpec` objects, one for each supported distribution.\n",
        "- `make_dataset_id`: Creates a unique ID for each generated dataset.\n",
        "- `is_heavy_tailed`: Determines if a distribution is heavy-tailed based on its name and parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cd446d4"
      },
      "outputs": [],
      "source": [
        "# === Distributions, seeding, RNG utilities, and dataset builder (fresh randomness each run) ===\n",
        "from dataclasses import dataclass\n",
        "import json, hashlib\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1) Fresh randomness each run (record the seed) ---\n",
        "# If you want reproducibility later, you can set SEED to a fixed int instead.\n",
        "SEED = int(np.random.SeedSequence().entropy)   # fresh each run\n",
        "RNG  = np.random.default_rng(SEED)\n",
        "\n",
        "# --- 2) Deterministic per-task RNGs derived from (seed, labels) ---\n",
        "def rng_for(seed, *labels) -> np.random.Generator:\n",
        "    \"\"\"\n",
        "    Create an independent RNG stream for any (seed, labels...) combination.\n",
        "    Labels can include: distribution name, replicate index, 'params'/'data', augmentation name, etc.\n",
        "    \"\"\"\n",
        "    h = hashlib.blake2b(digest_size=16)\n",
        "    h.update(f\"seed={seed}\".encode())\n",
        "    h.update(json.dumps(labels, separators=(',', ':'), default=str).encode())\n",
        "    entropy = int.from_bytes(h.digest(), \"big\")\n",
        "    return np.random.default_rng(entropy)\n",
        "\n",
        "# --- 3) Spec for a distribution: parameter generator + sampler (both receive an RNG) ---\n",
        "@dataclass\n",
        "class DistSpec:\n",
        "    name: str\n",
        "    param_fn: callable   # signature: param_fn(i: int, rng: np.random.Generator) -> dict\n",
        "    sample_fn: callable  # signature: sample_fn(n: int, rng: np.random.Generator, **params) -> np.ndarray\n",
        "\n",
        "# --- 4) Parameter generators (ranges unchanged; only RNG is now explicit) ---\n",
        "def p_normal(i, rng):       # μ in [-1, 1], σ in [0.5, 2.0]\n",
        "    mu    = rng.uniform(-1.0, 1.0)\n",
        "    sigma = rng.uniform(0.5, 2.0)\n",
        "    return {\"mu\": float(mu), \"sigma\": float(sigma)}\n",
        "\n",
        "def p_exponential(i, rng):  # λ in [0.5, 2.0]\n",
        "    lam = rng.uniform(0.5, 2.0)\n",
        "    return {\"lam\": float(lam)}\n",
        "\n",
        "def p_pareto(i, rng):       # α in [1.15, 1.9], xm in [0.7, 2.5]\n",
        "    alpha = rng.uniform(1.15, 1.9)\n",
        "    xm    = rng.uniform(0.7, 2.5)\n",
        "    return {\"alpha\": float(alpha), \"xm\": float(xm)}\n",
        "\n",
        "def p_student_t(i, rng):    # df in [2, 12], scale in [0.5, 2.0], loc in [-1,1]\n",
        "    df    = rng.integers(2, 13)\n",
        "    loc   = rng.uniform(-1.0, 1.0)\n",
        "    scale = rng.uniform(0.5, 2.0)\n",
        "    return {\"df\": int(df), \"loc\": float(loc), \"scale\": float(scale)}\n",
        "\n",
        "def p_lognormal(i, rng):    # μ in [-0.5, 1.0], σ in [0.4, 1.2]\n",
        "    mu    = rng.uniform(-0.5, 1.0)\n",
        "    sigma = rng.uniform(0.4, 1.2)\n",
        "    return {\"mu\": float(mu), \"sigma\": float(sigma)}\n",
        "\n",
        "def p_mix_norm_pareto(i, rng):  # mixture π in [0.02, 0.25]\n",
        "    pi    = rng.uniform(0.02, 0.25)  # heavy-tail weight\n",
        "    mu    = rng.uniform(-0.5, 0.5)\n",
        "    sigma = rng.uniform(0.6, 1.5)\n",
        "    alpha = rng.uniform(1.2, 1.8)\n",
        "    xm    = rng.uniform(0.8, 1.6)\n",
        "    return {\"pi\": float(pi), \"mu\": float(mu), \"sigma\": float(sigma), \"alpha\": float(alpha), \"xm\": float(xm)}\n",
        "\n",
        "# --- 5) Samplers (explicit RNG; no global sampling) ---\n",
        "def sample_normal(n, rng, mu, sigma):\n",
        "    return rng.normal(mu, sigma, size=n)\n",
        "\n",
        "def sample_exponential(n, rng, lam):\n",
        "    return rng.exponential(1.0/lam, size=n)\n",
        "\n",
        "def sample_pareto(n, rng, alpha, xm=1.0):\n",
        "    return (rng.pareto(alpha, size=n) + 1.0) * xm\n",
        "\n",
        "def sample_student_t(n, rng, df, loc=0.0, scale=1.0):\n",
        "    return loc + scale * rng.standard_t(df=df, size=n)\n",
        "\n",
        "def sample_lognormal(n, rng, mu, sigma):\n",
        "    return rng.lognormal(mean=mu, sigma=sigma, size=n)\n",
        "\n",
        "def sample_mix_norm_pareto(n, rng, pi, mu, sigma, alpha, xm=1.0):\n",
        "    k = rng.binomial(1, pi, size=n).astype(bool)\n",
        "    x = np.empty(n, dtype=np.float64)\n",
        "    x[k]  = (rng.pareto(alpha, size=k.sum()) + 1.0) * xm\n",
        "    x[~k] = rng.normal(mu, sigma, size=(~k).sum())\n",
        "    return x\n",
        "\n",
        "# --- 6) Distribution registry (unchanged names) ---\n",
        "DISTRIBUTIONS = [\n",
        "    DistSpec(\"normal\",          p_normal,          sample_normal),\n",
        "    DistSpec(\"exponential\",     p_exponential,     sample_exponential),\n",
        "    DistSpec(\"pareto\",          p_pareto,          sample_pareto),\n",
        "    DistSpec(\"student_t\",       p_student_t,       sample_student_t),\n",
        "    DistSpec(\"lognormal\",       p_lognormal,       sample_lognormal),\n",
        "    DistSpec(\"mix_norm_pareto\", p_mix_norm_pareto, sample_mix_norm_pareto),\n",
        "]\n",
        "\n",
        "# --- 7) Utility: dataset id, tail label heuristic (unchanged logic) ---\n",
        "def make_dataset_id(dist_name: str, idx: int) -> str:\n",
        "    timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S%f\")\n",
        "    return f\"{dist_name}_{idx:03d}_{timestamp}\"\n",
        "\n",
        "def is_heavy_tailed(dist_name: str, params: dict) -> bool:\n",
        "    name = dist_name.lower()\n",
        "    if name in [\"pareto\", \"cauchy\", \"mix_norm_pareto\"]:\n",
        "        return True\n",
        "    if name in [\"normal\", \"gaussian\", \"exponential\"]:\n",
        "        return False\n",
        "    if name in [\"student_t\", \"studentt\", \"t\"]:\n",
        "        return params.get(\"df\", 10) <= 5\n",
        "    if name in [\"lognormal\", \"lognorm\"]:\n",
        "        return params.get(\"sigma\", 1.0) >= 1.0\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npBk3b53nMVn"
      },
      "outputs": [],
      "source": [
        "# === Helper: safe_move for cross-device moves ===\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def safe_move(src: Path, dst: Path):\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.move(str(src), str(dst))  # copy+delete when needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERgeRN-YnqPW"
      },
      "outputs": [],
      "source": [
        "# === build_corpus (open_memmap -> schreibt gültige .npy-Dateien; cross-device-safe move) ===\n",
        "from numpy.lib.format import open_memmap  # wichtig: schreibt .npy-Header\n",
        "\n",
        "def build_corpus(n_per_dist=5, n_samples=2000, chunk_size=200_000, dtype=np.float32, seed: int = SEED):\n",
        "    \"\"\"\n",
        "    Builds datasets for all distributions defined in DISTRIBUTIONS.\n",
        "    - Lokales Staging mit open_memmap (erzeugt echte .npy-Dateien, header-integriert)\n",
        "    - Cross-device-sicheres Verschieben per safe_move\n",
        "    - Per-task RNGs (params vs data)\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    for dist in DISTRIBUTIONS:\n",
        "        print(\"Generating:\", dist.name)\n",
        "        for i in range(n_per_dist):\n",
        "            # unabhängige RNG-Streams\n",
        "            rng_params = rng_for(seed, dist.name, i, \"params\")\n",
        "            rng_data   = rng_for(seed, dist.name, i, \"data\")\n",
        "\n",
        "            ds_id  = make_dataset_id(dist.name, i)\n",
        "            params = dist.param_fn(i, rng_params)\n",
        "            label  = is_heavy_tailed(dist.name, params)\n",
        "\n",
        "            # --- lokal (.npy) schreiben\n",
        "            local_out_dir = LOCAL_TMP / dist.name\n",
        "            local_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "            local_data_path = local_out_dir / f\"{ds_id}.npy\"\n",
        "\n",
        "            remaining = int(n_samples)\n",
        "            pos = 0\n",
        "            # open_memmap erzeugt eine .npy-Datei mit Header (kompatibel zu np.load)\n",
        "            mm = open_memmap(filename=str(local_data_path), mode='w+', dtype=dtype, shape=(n_samples,))\n",
        "            try:\n",
        "                while remaining > 0:\n",
        "                    m = min(remaining, chunk_size)\n",
        "                    x = dist.sample_fn(m, rng_data, **params)\n",
        "                    if x.dtype != dtype:\n",
        "                        x = x.astype(dtype, copy=False)\n",
        "                    mm[pos:pos+m] = x\n",
        "                    pos += m\n",
        "                    remaining -= m\n",
        "                mm.flush()\n",
        "            finally:\n",
        "                del mm  # Handle freigeben\n",
        "\n",
        "            # --- cross-device-sicher nach Drive verschieben\n",
        "            final_data_path = DATA_DIR / dist.name / f\"{ds_id}.npy\"\n",
        "            safe_move(local_data_path, final_data_path)\n",
        "\n",
        "            # --- Plots rendern (np.load(..., allow_pickle=False) funktioniert jetzt)\n",
        "            plots = render_plots(ds_id, final_data_path)\n",
        "\n",
        "            records.append({\n",
        "                \"dataset_id\": ds_id,\n",
        "                \"distribution\": dist.name,\n",
        "                \"n\": int(n_samples),\n",
        "                \"heavy_tailed\": bool(label),\n",
        "                \"data_path\": str(final_data_path),\n",
        "                \"params\": params,\n",
        "                \"plots\": plots,\n",
        "            })\n",
        "\n",
        "    return records"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ws3coCsyPXg"
      },
      "source": [
        "This code cell contains several helper functions for processing data and generating plots:\n",
        "\n",
        "- `_sanitize`: Removes NaN and infinite values from an array.\n",
        "- `_downsample`: Reduces the number of data points in an array to a specified maximum, useful for performance with large datasets.\n",
        "- `_clamp_by_percentile`: Limits the data range by values at specified percentiles to reduce the impact of outliers.\n",
        "- `plot_zipf`: Generates a Zipf plot, which visualizes the distribution of data ranks against their values on a log-log scale.\n",
        "- `plot_me`: Generates a Mean Excess plot, which shows the expected value of data exceeding a certain threshold.\n",
        "- `_fit_lambda_mle_pos`: Calculates the maximum likelihood estimate for the lambda parameter of an exponential distribution for positive values.\n",
        "- `plot_qq_exponential`: Generates an Exponential Q-Q plot, comparing the quantiles of the data to the theoretical quantiles of an exponential distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24f7ba70"
      },
      "outputs": [],
      "source": [
        "# === Plotting helpers (sanitize, downsample uses explicit RNG) ===\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _sanitize(x):\n",
        "    x = np.asarray(x).ravel()\n",
        "    return x[~np.isnan(x) & ~np.isinf(x)]\n",
        "\n",
        "def _downsample(x, max_points=50_000, rng=None):\n",
        "    n = x.shape[0]\n",
        "    if n <= max_points:\n",
        "        return x\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    idx = rng.choice(n, size=max_points, replace=False)\n",
        "    return x[idx]\n",
        "\n",
        "def _clamp_by_percentile(x, pct=99.5):\n",
        "    lo = np.nanpercentile(x, 100 - pct)\n",
        "    hi = np.nanpercentile(x, pct)\n",
        "    if not np.isfinite(lo): lo = np.nanmin(x)\n",
        "    if not np.isfinite(hi): hi = np.nanmax(x)\n",
        "    x = np.clip(x, lo, hi)\n",
        "    return x, (lo, hi)\n",
        "\n",
        "# --- existing plot functions; only change: use _downsample(x, rng=...) ---\n",
        "def plot_zipf(x, save_path, rng=None):\n",
        "    x = _sanitize(np.abs(x))\n",
        "    x = _downsample(x, rng=rng)\n",
        "    x = np.sort(x)[::-1]\n",
        "    ranks = np.arange(1, x.size + 1)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.loglog(ranks, x, marker=\".\", linewidth=0)\n",
        "    ax.set_xlabel(\"rank\"); ax.set_ylabel(\"|x| (sorted)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_me(x, save_path, n_bins=200, rng=None):\n",
        "    x = _sanitize(np.abs(x))\n",
        "    x = _downsample(x, rng=rng)\n",
        "    x, (lo, hi) = _clamp_by_percentile(x, 99.5)\n",
        "    xs = np.sort(x)\n",
        "    us = np.linspace(lo, hi, n_bins)\n",
        "    e_vals = []\n",
        "    for u in us:\n",
        "        exceed = xs[xs > u]\n",
        "        e_vals.append(np.mean(exceed - u) if exceed.size else np.nan)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(us, e_vals, marker=\".\", linewidth=1)\n",
        "    ax.set_xlabel(\"u\"); ax.set_ylabel(\"e(u)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_qq_exp(x, save_path, rng=None):\n",
        "    \"\"\"\n",
        "    Exponential QQ-plot: compare sample quantiles of |x| to theoretical exponential(1) quantiles.\n",
        "    \"\"\"\n",
        "    x = _sanitize(np.abs(x))\n",
        "    x = _downsample(x, rng=rng)\n",
        "    x = np.sort(x)\n",
        "    n = x.size\n",
        "    if n == 0:\n",
        "        # create an empty plot if needed\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.set_title(\"Empty sample\")\n",
        "        plt.tight_layout(); plt.savefig(save_path); plt.close(fig); return\n",
        "    # theoretical quantiles for Exp(1): F^{-1}(p) = -ln(1-p)\n",
        "    p = (np.arange(1, n + 1) - 0.5) / n\n",
        "    q_theory = -np.log1p(-p)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(q_theory, x, marker=\".\", linewidth=0)\n",
        "    ax.set_xlabel(\"Exp(1) quantiles\"); ax.set_ylabel(\"Sample |x| quantiles\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aae22ae1"
      },
      "source": [
        "This cell defines simple augmentation functions that are applied to the generated data to increase the robustness of the model.\n",
        "\n",
        "- `jitter`: Adds a small amount of noise to the data.\n",
        "- `bootstrap_resample`: Creates a new sample by drawing with replacement from the original data.\n",
        "- `slight_scale`: Slightly scales the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e34b0737"
      },
      "outputs": [],
      "source": [
        "# === Augmentations (explicit RNG) ===\n",
        "import numpy as np\n",
        "\n",
        "def jitter(x, scale=0.02, rng=None):\n",
        "    \"\"\"\n",
        "    Add small Gaussian noise proportional to the sample std.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    x = np.asarray(x).ravel().astype(np.float32)\n",
        "    s = float(np.std(x)) if np.std(x) > 0 else 1.0\n",
        "    return x + rng.normal(0.0, scale * s, size=len(x)).astype(np.float32)\n",
        "\n",
        "def bootstrap_resample(x, rng=None):\n",
        "    \"\"\"\n",
        "    Sample with replacement from x.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    x = np.asarray(x).ravel()\n",
        "    idx = rng.integers(0, len(x), size=len(x))\n",
        "    return x[idx].astype(np.float32, copy=False)\n",
        "\n",
        "def slight_scale(x, factor=1.05, rng=None):\n",
        "    \"\"\"\n",
        "    Deterministic scaling; rng included for a uniform signature.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x).ravel()\n",
        "    return (x * factor).astype(np.float32, copy=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDIvA_mIy6rY"
      },
      "source": [
        "This cell contains functions for rendering the plots for each dataset and its augmentations, building a corpus of datasets, and storing metadata.\n",
        "\n",
        "- `render_plots`: Creates and saves the Zipf, ME, and QQ plots for a given dataset and its augmentations.\n",
        "- `build_corpus`: Generates datasets for each distribution and the specified number, saves the data, and renders the plots. It also collects metadata for each dataset.\n",
        "- `write_metadata`: Saves the collected metadata in JSON and CSV files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "587530fb"
      },
      "outputs": [],
      "source": [
        "# === Local temp dir (RAM-friendly, no Drive lag) ===\n",
        "from pathlib import Path\n",
        "import json, csv\n",
        "LOCAL_TMP = Path(\"/content/ds_tmp\")\n",
        "LOCAL_TMP.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def render_plots(ds_id: str, npy_path: Path):\n",
        "    \"\"\"\n",
        "    Render and save plots for a dataset.\n",
        "    Uses derived RNG streams for downsampling and augmentations to keep runs controlled.\n",
        "    \"\"\"\n",
        "    ds_dir = PLOTS_DIR / ds_id\n",
        "    ds_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    x = np.load(npy_path, mmap_mode=\"r\", allow_pickle=False)\n",
        "\n",
        "    paths = {}\n",
        "\n",
        "    # ORIGINAL\n",
        "    zipf_p = ds_dir / \"zipf.png\"\n",
        "    me_p   = ds_dir / \"me.png\"\n",
        "    qq_p   = ds_dir / \"qq_exp.png\"\n",
        "\n",
        "    # derive RNG for original downsampling (plotting)\n",
        "    rng_down = rng_for(SEED, \"render_plots\", ds_id, \"downsample\")\n",
        "    plot_zipf(x, zipf_p, rng=rng_down)\n",
        "    plot_me(x,   me_p,   rng=rng_down)\n",
        "    plot_qq_exp(x, qq_p, rng=rng_down)\n",
        "\n",
        "    paths[\"original\"] = {\"zipf\": str(zipf_p), \"me\": str(me_p), \"qq_exp\": str(qq_p)}\n",
        "\n",
        "    # AUGMENTATIONS: take a manageable subsample first (with explicit RNG)\n",
        "    sample_n = min(50_000, x.shape[0])\n",
        "    rng_pick = rng_for(SEED, \"render_plots\", ds_id, \"pick_for_aug\")\n",
        "    idx = rng_pick.integers(0, x.shape[0], size=sample_n)\n",
        "    x_small = np.asarray(x[idx])\n",
        "\n",
        "    paths[\"aug\"] = {}\n",
        "    # independent RNG streams per augmentation\n",
        "    aug_streams = {\n",
        "        \"jitter\":    rng_for(SEED, \"render_plots\", ds_id, \"aug\", \"jitter\"),\n",
        "        \"bootstrap\": rng_for(SEED, \"render_plots\", ds_id, \"aug\", \"bootstrap\"),\n",
        "        \"scale105\":  rng_for(SEED, \"render_plots\", ds_id, \"aug\", \"scale105\"),\n",
        "    }\n",
        "    aug_data = {\n",
        "        \"jitter\":    jitter(x_small, rng=aug_streams[\"jitter\"]),\n",
        "        \"bootstrap\": bootstrap_resample(x_small, rng=aug_streams[\"bootstrap\"]),\n",
        "        \"scale105\":  slight_scale(x_small, 1.05, rng=aug_streams[\"scale105\"]),\n",
        "    }\n",
        "\n",
        "    # plotting RNGs per augmentation\n",
        "    for name, arr in aug_data.items():\n",
        "        z_a = ds_dir / f\"zipf_{name}.png\"\n",
        "        m_a = ds_dir / f\"me_{name}.png\"\n",
        "        q_a = ds_dir / f\"qq_exp_{name}.png\"\n",
        "\n",
        "        rng_plot = rng_for(SEED, \"render_plots\", ds_id, \"plot_aug\", name)\n",
        "        plot_zipf(arr, z_a, rng=rng_plot)\n",
        "        plot_me(arr,   m_a, rng=rng_plot)\n",
        "        plot_qq_exp(arr, q_a, rng=rng_plot)\n",
        "\n",
        "        paths.setdefault(\"aug\", {})[name] = {\"zipf\": str(z_a), \"me\": str(m_a), \"qq_exp\": str(q_a)}\n",
        "\n",
        "    return paths\n",
        "\n",
        "def write_metadata(records):\n",
        "    \"\"\"\n",
        "    Write metadata to JSON and CSV; include the run SEED so results are traceable even with fresh randomness.\n",
        "    \"\"\"\n",
        "    META_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    meta_json = META_DIR / \"datasets_metadata.json\"\n",
        "    meta_csv  = META_DIR / \"datasets_metadata.csv\"\n",
        "\n",
        "    # JSON\n",
        "    payload = {\n",
        "        \"seed\": SEED,\n",
        "        \"count\": len(records),\n",
        "        \"records\": records,\n",
        "    }\n",
        "    with open(meta_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # CSV (flat view)\n",
        "    cols = [\"dataset_id\",\"distribution\",\"n\",\"heavy_tailed\",\"data_path\",\"params_json\",\n",
        "            \"plot_zipf\",\"plot_me\",\"plot_qq_exp\",\n",
        "            \"plot_zipf_jitter\",\"plot_me_jitter\",\"plot_qq_jitter\",\n",
        "            \"plot_zipf_bootstrap\",\"plot_me_bootstrap\",\"plot_qq_bootstrap\",\n",
        "            \"plot_zipf_scale105\",\"plot_me_scale105\",\"plot_qq_scale105\",\n",
        "            \"seed\"]\n",
        "    with open(meta_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f); w.writerow(cols)\n",
        "        for r in records:\n",
        "            p = r[\"plots\"]\n",
        "            w.writerow([\n",
        "                r[\"dataset_id\"], r[\"distribution\"], r[\"n\"], int(r[\"heavy_tailed\"]), r[\"data_path\"],\n",
        "                json.dumps(r[\"params\"], ensure_ascii=False),\n",
        "                p[\"original\"][\"zipf\"], p[\"original\"][\"me\"], p[\"original\"][\"qq_exp\"],\n",
        "                p[\"aug\"][\"jitter\"][\"zipf\"], p[\"aug\"][\"jitter\"][\"me\"], p[\"aug\"][\"jitter\"][\"qq_exp\"],\n",
        "                p[\"aug\"][\"bootstrap\"][\"zipf\"], p[\"aug\"][\"bootstrap\"][\"me\"], p[\"aug\"][\"bootstrap\"][\"qq_exp\"],\n",
        "                p[\"aug\"][\"scale105\"][\"zipf\"], p[\"aug\"][\"scale105\"][\"me\"], p[\"aug\"][\"scale105\"][\"qq_exp\"],\n",
        "                SEED,\n",
        "            ])\n",
        "    print(\"Wrote metadata:\", meta_json, \"and\", meta_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2867b357"
      },
      "source": [
        "This cell is the main trigger for data generation. It calls the `build_corpus` and `write_metadata` functions to create the datasets, render the plots, and save the metadata.\n",
        "\n",
        "- `build_corpus(n_per_dist=5, n_samples=2000)`: Generates 5 datasets per distribution with 2000 samples each.\n",
        "- `write_metadata(records)`: Saves the metadata of the generated datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cfb39a4",
        "outputId": "93c55838-f5b2-4ea7-d40b-aa0a5956ed77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating: normal\n",
            "Generating: exponential\n",
            "Generating: pareto\n",
            "Generating: student_t\n",
            "Generating: lognormal\n",
            "Generating: mix_norm_pareto\n",
            "Wrote metadata: /content/drive/MyDrive/Generated Data for Data science project/metadata/datasets_metadata.json and /content/drive/MyDrive/Generated Data for Data science project/metadata/datasets_metadata.csv\n",
            "SEED used for this run: 37280314975076901032992437105419038345\n",
            "Data generation complete. Artifacts in: /content/drive/MyDrive/Generated Data for Data science project\n"
          ]
        }
      ],
      "source": [
        "# === Run data generation ===\n",
        "records = build_corpus(\n",
        "    n_per_dist=200,\n",
        "    n_samples=2000,\n",
        "    chunk_size=250_000,\n",
        ")\n",
        "write_metadata(records)\n",
        "print(\"SEED used for this run:\", SEED)\n",
        "print(\"Data generation complete. Artifacts in:\", BASE_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}