{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laraAkg/Data-Science-Project/blob/main/Generate_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b3a35c5"
      },
      "source": [
        "# Dataset Generation and Analysis for Heavy-Tailed Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a14bfee8"
      },
      "source": [
        "## 0. Imports, Environment Setup & Colab Integration\n",
        "\n",
        "This cell imports the required libraries for data generation and preprocessing, configures the plotting backend, and prepares the execution environment.  \n",
        "It also detects whether the notebook is running in Google Colab and mounts Google Drive if needed.\n",
        "\n",
        "- **Standard Library Imports:** Loads core Python utilities for file handling, serialization, timing, garbage collection, and date/time management.\n",
        "- **`numpy`**: Provides numerical operations used throughout the data generation pipeline.\n",
        "- **`matplotlib`**: Configured to use a non-GUI backend (`Agg`) to enable plot generation in headless environments.\n",
        "- **`scipy.stats`**: Optionally imported for statistical computations (gracefully disabled if unavailable).\n",
        "- **Colab Detection (`IN_COLAB`)**: Checks whether the notebook is executed in a Google Colab environment.\n",
        "- **Google Drive Integration:** Automatically mounts Google Drive when running in Colab to enable persistent file access and storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCwfkeZyPkMu",
        "outputId": "50b97381-1934-46ba-ce79-7ac254860c7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import gc\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    import scipy.stats as st\n",
        "except ImportError:\n",
        "    st = None\n",
        "\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Project Directory Structure & Global Constants\n",
        "\n",
        "This cell defines the directory structure for the project and initializes paths for storing datasets, plots, metadata, models, reports, and real-world data.\n",
        "\n",
        "- **`DEFAULT_PROJECT_DIR`**: Specifies the default project directory name within Google Drive.\n",
        "- **`BASE_DIR`**: Determines the base directory for all project outputs, using Google Drive when running in Colab or a local `./project_outputs` directory otherwise.\n",
        "- **`DATA_DIR`, `PLOTS_DIR`, `META_DIR`, `MODELS_DIR`, `REPORTS_DIR`, `REAL_DIR`**: Define dedicated subdirectories under `BASE_DIR` for different types of project artifacts.\n",
        "- **Directory Creation Logic**: Iterates over all defined directories and creates them if they do not already exist using `mkdir(parents=True, exist_ok=True)`.\n",
        "- **`IMG_SIZE`, `DPI`, `FIGSIZE`**: Define global constants for image resolution, dots-per-inch for saved figures, and default figure size for Matplotlib plots.\n",
        "- **Runtime Feedback**: Prints the resolved `BASE_DIR` to confirm the active project root directory."
      ],
      "metadata": {
        "id": "km0yoUKEoO9-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fd779f6",
        "outputId": "7e45f256-bc5f-4683-c6e6-4bd50ce08ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BASE_DIR: /content/drive/MyDrive/Generated Data for Data science project\n"
          ]
        }
      ],
      "source": [
        "DEFAULT_PROJECT_DIR = \"MyDrive/Generated Data for Data science project\"\n",
        "BASE_DIR = Path(\"/content/drive\") / DEFAULT_PROJECT_DIR if IN_COLAB else Path(\"./project_outputs\")\n",
        "\n",
        "DATA_DIR   = BASE_DIR / \"datasets\"\n",
        "PLOTS_DIR  = BASE_DIR / \"plots\"\n",
        "META_DIR   = BASE_DIR / \"metadata\"\n",
        "MODELS_DIR = BASE_DIR / \"models_tf\"\n",
        "REPORTS_DIR= BASE_DIR / \"reports\"\n",
        "REAL_DIR   = BASE_DIR / \"real\"\n",
        "\n",
        "for p in [DATA_DIR, PLOTS_DIR, META_DIR, MODELS_DIR, REPORTS_DIR, REAL_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "IMG_SIZE = (128, 128)  # (H, W)\n",
        "DPI      = 150\n",
        "FIGSIZE  = (4.0, 4.0)\n",
        "\n",
        "print(\"BASE_DIR:\", BASE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Utility Functions & Matplotlib Configuration\n",
        "\n",
        "This cell defines utility functions for file handling and configures Matplotlib for consistent plot creation and saving in a headless environment.\n",
        "\n",
        "- **Matplotlib Configuration**: Sets global Matplotlib parameters (`figure.dpi`, `savefig.dpi`, `figure.figsize`) and enforces the non-GUI `Agg` backend for reliable plot generation.\n",
        "- **`ensure_parent`**: Ensures that the parent directory of a given file path exists before writing files to disk.\n",
        "- **`_patched_savefig`**: Wraps Matplotlib’s `savefig` function to automatically redirect relative paths to `PLOTS_DIR` and create missing directories if necessary.\n",
        "- **`plt.savefig` Override**: Replaces the default `savefig` with the patched version to enforce consistent plot saving behavior across the notebook.\n",
        "- **`save_dataset`**: Saves NumPy arrays as `.npy` files with enforced `float32` precision, ensuring reproducibility and storage efficiency."
      ],
      "metadata": {
        "id": "BbY9amPKofI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43ba0bcb"
      },
      "outputs": [],
      "source": [
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "plt.rcParams[\"figure.dpi\"] = DPI\n",
        "plt.rcParams[\"savefig.dpi\"] = DPI\n",
        "plt.rcParams[\"figure.figsize\"] = FIGSIZE\n",
        "\n",
        "def ensure_parent(path: Path):\n",
        "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True); return path\n",
        "\n",
        "_original_savefig = plt.savefig\n",
        "def _patched_savefig(*args, **kwargs):\n",
        "    if len(args) > 0 and isinstance(args[0], (str, Path)):\n",
        "        fname = Path(args[0])\n",
        "        if not fname.is_absolute():\n",
        "            fname = PLOTS_DIR / fname\n",
        "        ensure_parent(fname)\n",
        "        args = (str(fname),) + tuple(args[1:])\n",
        "    else:\n",
        "        auto = PLOTS_DIR / f\"plot_{int(time.time()*1000)}.png\"\n",
        "        ensure_parent(auto)\n",
        "        args = (str(auto),) + args\n",
        "    kwargs.setdefault(\"dpi\", DPI)\n",
        "    return _original_savefig(*args, **kwargs)\n",
        "plt.savefig = _patched_savefig\n",
        "\n",
        "def save_dataset(arr: np.ndarray, path: Path):\n",
        "    path = ensure_parent(path)\n",
        "    np.save(path, arr.astype(np.float32))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Distributions, RNG Utilities & Dataset Labeling\n",
        "\n",
        "This cell defines probability distributions, utilities for reproducible random sampling, and a helper function to determine whether a generated dataset should be labeled as “heavy-tailed” based on distribution parameters.\n",
        "\n",
        "- **`SEED`**: Generates a fresh random seed for each run (can be fixed for reproducibility) and is used to initialize randomness.\n",
        "- **`RNG`**: Global NumPy random generator initialized with `SEED`.\n",
        "- **`rng_for`**: Creates deterministic, independent RNG streams derived from (`seed`, `labels...`) to ensure reproducible sub-tasks (e.g., params vs. sampling, augmentations).\n",
        "- **`DistSpec`**: Dataclass holding the distribution `name`, a parameter generator (`param_fn`), and a sampling function (`sample_fn`).\n",
        "- **`p_*`**: Parameter generator functions that sample valid distribution parameters using an explicit RNG.\n",
        "- **`sample_*`**: Sampling functions that generate data from specific distributions using an explicit RNG (no hidden global randomness).\n",
        "- **`DISTRIBUTIONS`**: Registry of supported distributions as a list of `DistSpec` objects.\n",
        "- **`make_dataset_id`**: Generates a unique identifier for each dataset instance (distribution name + index + timestamp).\n",
        "- **`is_heavy_tailed`**: Assigns the heavy-tailed label based on distribution type and parameter thresholds (e.g., `df` for Student-t, `sigma` for lognormal)."
      ],
      "metadata": {
        "id": "8Td_VazSo17r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cd446d4"
      },
      "outputs": [],
      "source": [
        "# === Distributions, seeding, RNG utilities, and dataset builder (fresh randomness each run) ===\n",
        "from dataclasses import dataclass\n",
        "import json, hashlib\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# --- 1) Fresh randomness each run (record the seed) ---\n",
        "# If you want reproducibility later, you can set SEED to a fixed int instead.\n",
        "SEED = int(np.random.SeedSequence().entropy)   # fresh each run\n",
        "RNG  = np.random.default_rng(SEED)\n",
        "\n",
        "# --- 2) Deterministic per-task RNGs derived from (seed, labels) ---\n",
        "def rng_for(seed, *labels) -> np.random.Generator:\n",
        "    \"\"\"\n",
        "    Create an independent RNG stream for any (seed, labels...) combination.\n",
        "    Labels can include: distribution name, replicate index, 'params'/'data', augmentation name, etc.\n",
        "    \"\"\"\n",
        "    h = hashlib.blake2b(digest_size=16)\n",
        "    h.update(f\"seed={seed}\".encode())\n",
        "    h.update(json.dumps(labels, separators=(',', ':'), default=str).encode())\n",
        "    entropy = int.from_bytes(h.digest(), \"big\")\n",
        "    return np.random.default_rng(entropy)\n",
        "\n",
        "# --- 3) Spec for a distribution: parameter generator + sampler (both receive an RNG) ---\n",
        "@dataclass\n",
        "class DistSpec:\n",
        "    name: str\n",
        "    param_fn: callable   # signature: param_fn(i: int, rng: np.random.Generator) -> dict\n",
        "    sample_fn: callable  # signature: sample_fn(n: int, rng: np.random.Generator, **params) -> np.ndarray\n",
        "\n",
        "# --- 4) Parameter generators (ranges unchanged; only RNG is now explicit) ---\n",
        "def p_normal(i, rng):       # μ in [-1, 1], σ in [0.5, 2.0]\n",
        "    mu    = rng.uniform(-1.0, 1.0)\n",
        "    sigma = rng.uniform(0.5, 2.0)\n",
        "    return {\"mu\": float(mu), \"sigma\": float(sigma)}\n",
        "\n",
        "def p_exponential(i, rng):  # λ in [0.5, 2.0]\n",
        "    lam = rng.uniform(0.5, 2.0)\n",
        "    return {\"lam\": float(lam)}\n",
        "\n",
        "def p_pareto(i, rng):       # α in [1.15, 1.9], xm in [0.7, 2.5]\n",
        "    alpha = rng.uniform(1.15, 1.9)\n",
        "    xm    = rng.uniform(0.7, 2.5)\n",
        "    return {\"alpha\": float(alpha), \"xm\": float(xm)}\n",
        "\n",
        "def p_student_t(i, rng):    # df in [2, 12], scale in [0.5, 2.0], loc in [-1,1]\n",
        "    df    = rng.integers(2, 13)\n",
        "    loc   = rng.uniform(-1.0, 1.0)\n",
        "    scale = rng.uniform(0.5, 2.0)\n",
        "    return {\"df\": int(df), \"loc\": float(loc), \"scale\": float(scale)}\n",
        "\n",
        "def p_lognormal(i, rng):    # μ in [-0.5, 1.0], σ in [0.4, 1.2]\n",
        "    mu    = rng.uniform(-0.5, 1.0)\n",
        "    sigma = rng.uniform(0.4, 1.2)\n",
        "    return {\"mu\": float(mu), \"sigma\": float(sigma)}\n",
        "\n",
        "def p_mix_norm_pareto(i, rng):  # mixture π in [0.02, 0.25]\n",
        "    pi    = rng.uniform(0.02, 0.25)  # heavy-tail weight\n",
        "    mu    = rng.uniform(-0.5, 0.5)\n",
        "    sigma = rng.uniform(0.6, 1.5)\n",
        "    alpha = rng.uniform(1.2, 1.8)\n",
        "    xm    = rng.uniform(0.8, 1.6)\n",
        "    return {\"pi\": float(pi), \"mu\": float(mu), \"sigma\": float(sigma), \"alpha\": float(alpha), \"xm\": float(xm)}\n",
        "\n",
        "# --- 5) Samplers (explicit RNG; no global sampling) ---\n",
        "def sample_normal(n, rng, mu, sigma):\n",
        "    return rng.normal(mu, sigma, size=n)\n",
        "\n",
        "def sample_exponential(n, rng, lam):\n",
        "    return rng.exponential(1.0/lam, size=n)\n",
        "\n",
        "def sample_pareto(n, rng, alpha, xm=1.0):\n",
        "    return (rng.pareto(alpha, size=n) + 1.0) * xm\n",
        "\n",
        "def sample_student_t(n, rng, df, loc=0.0, scale=1.0):\n",
        "    return loc + scale * rng.standard_t(df=df, size=n)\n",
        "\n",
        "def sample_lognormal(n, rng, mu, sigma):\n",
        "    return rng.lognormal(mean=mu, sigma=sigma, size=n)\n",
        "\n",
        "def sample_mix_norm_pareto(n, rng, pi, mu, sigma, alpha, xm=1.0):\n",
        "    k = rng.binomial(1, pi, size=n).astype(bool)\n",
        "    x = np.empty(n, dtype=np.float64)\n",
        "    x[k]  = (rng.pareto(alpha, size=k.sum()) + 1.0) * xm\n",
        "    x[~k] = rng.normal(mu, sigma, size=(~k).sum())\n",
        "    return x\n",
        "\n",
        "# --- 6) Distribution registry (unchanged names) ---\n",
        "DISTRIBUTIONS = [\n",
        "    DistSpec(\"normal\",          p_normal,          sample_normal),\n",
        "    DistSpec(\"exponential\",     p_exponential,     sample_exponential),\n",
        "    DistSpec(\"pareto\",          p_pareto,          sample_pareto),\n",
        "    DistSpec(\"student_t\",       p_student_t,       sample_student_t),\n",
        "    DistSpec(\"lognormal\",       p_lognormal,       sample_lognormal),\n",
        "    DistSpec(\"mix_norm_pareto\", p_mix_norm_pareto, sample_mix_norm_pareto),\n",
        "]\n",
        "\n",
        "# --- 7) Utility: dataset id, tail label heuristic (unchanged logic) ---\n",
        "def make_dataset_id(dist_name: str, idx: int) -> str:\n",
        "    timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%d%H%M%S%f\")\n",
        "    return f\"{dist_name}_{idx:03d}_{timestamp}\"\n",
        "\n",
        "def is_heavy_tailed(dist_name: str, params: dict) -> bool:\n",
        "    name = dist_name.lower()\n",
        "    if name in [\"pareto\", \"cauchy\", \"mix_norm_pareto\"]:\n",
        "        return True\n",
        "    if name in [\"normal\", \"gaussian\", \"exponential\"]:\n",
        "        return False\n",
        "    if name in [\"student_t\", \"studentt\", \"t\"]:\n",
        "        return params.get(\"df\", 10) <= 5\n",
        "    if name in [\"lognormal\", \"lognorm\"]:\n",
        "        return params.get(\"sigma\", 1.0) >= 1.0\n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Helper: Safe File Move Utility\n",
        "\n",
        "This cell defines a small helper function for safely moving files across directories or file systems.\n",
        "\n",
        "- **`safe_move`**: Moves a file from `src` to `dst`, automatically creating the destination parent directory if it does not exist.\n",
        "- **Cross-Device Safe**: Uses `shutil.move`, which transparently handles cross-device moves by falling back to copy-and-delete when required.\n",
        "- **Path Safety**: Ensures that destination paths are always valid before moving files."
      ],
      "metadata": {
        "id": "ALH1PNLvpIA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npBk3b53nMVn"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "def safe_move(src: Path, dst: Path):\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.move(str(src), str(dst))  # copy+delete when needed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Dataset Construction with Memory-Mapped Arrays\n",
        "\n",
        "This cell defines the main dataset construction routine that generates numerical samples for all configured distributions and stores them as valid `.npy` files.\n",
        "\n",
        "- **`build_corpus`**: Main dataset builder that iterates over all distributions and generates multiple datasets per distribution.\n",
        "- **`open_memmap`**: Writes data incrementally into valid `.npy` files (including headers), enabling efficient handling of large arrays without loading everything into memory.\n",
        "- **Per-Task RNG Streams**: Uses `rng_for` to create independent random generators for parameter sampling (`\"params\"`) and data generation (`\"data\"`).\n",
        "- **Local Staging (`LOCAL_TMP`)**: Temporarily writes datasets to a local directory before moving them to the final destination.\n",
        "- **Chunked Writing (`chunk_size`)**: Generates and writes samples in chunks to control memory usage.\n",
        "- **`safe_move`**: Moves generated `.npy` files to `DATA_DIR` in a cross-device-safe manner (e.g. local disk → Google Drive).\n",
        "- **`render_plots`**: Generates diagnostic plots from the stored `.npy` files using standard `np.load` (no pickling required).\n",
        "- **Metadata Records**: Collects dataset metadata including distribution name, parameters, label (`heavy_tailed`), file paths, and generated plots.\n",
        "- **Return Value**: Returns a list of metadata records, one per generated dataset, to be used for indexing and downstream processing."
      ],
      "metadata": {
        "id": "EaJPq2N9pZwW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERgeRN-YnqPW"
      },
      "outputs": [],
      "source": [
        "# === build_corpus (open_memmap -> writes valid .npy-files; cross-device-safe move) ===\n",
        "from numpy.lib.format import open_memmap  # important: writes .npy-Header\n",
        "\n",
        "def build_corpus(n_per_dist=5, n_samples=2000, chunk_size=200_000, dtype=np.float32, seed: int = SEED):\n",
        "    \"\"\"\n",
        "    Builds datasets for all distributions defined in DISTRIBUTIONS.\n",
        "    - Local staging with open_memmap (creates real .npy files, header-integrated)\n",
        "    - Cross-device-safe moving via safe_move\n",
        "    - Per-task RNGs (params vs data)\n",
        "    \"\"\"\n",
        "    records = []\n",
        "\n",
        "    for dist in DISTRIBUTIONS:\n",
        "        print(\"Generating:\", dist.name)\n",
        "        for i in range(n_per_dist):\n",
        "            # independent RNG-Streams\n",
        "            rng_params = rng_for(seed, dist.name, i, \"params\")\n",
        "            rng_data   = rng_for(seed, dist.name, i, \"data\")\n",
        "\n",
        "            ds_id  = make_dataset_id(dist.name, i)\n",
        "            params = dist.param_fn(i, rng_params)\n",
        "            label  = is_heavy_tailed(dist.name, params)\n",
        "\n",
        "            # --- Write locally (.npy)\n",
        "            local_out_dir = LOCAL_TMP / dist.name\n",
        "            local_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "            local_data_path = local_out_dir / f\"{ds_id}.npy\"\n",
        "\n",
        "            remaining = int(n_samples)\n",
        "            pos = 0\n",
        "            # open_memmap creates an .npy file with a header (compatible with np.load)\n",
        "            mm = open_memmap(filename=str(local_data_path), mode='w+', dtype=dtype, shape=(n_samples,))\n",
        "            try:\n",
        "                while remaining > 0:\n",
        "                    m = min(remaining, chunk_size)\n",
        "                    x = dist.sample_fn(m, rng_data, **params)\n",
        "                    if x.dtype != dtype:\n",
        "                        x = x.astype(dtype, copy=False)\n",
        "                    mm[pos:pos+m] = x\n",
        "                    pos += m\n",
        "                    remaining -= m\n",
        "                mm.flush()\n",
        "            finally:\n",
        "                del mm  # Release handle\n",
        "\n",
        "            # --- cross-device-safe move to Drive\n",
        "            final_data_path = DATA_DIR / dist.name / f\"{ds_id}.npy\"\n",
        "            safe_move(local_data_path, final_data_path)\n",
        "\n",
        "            # --- Render plots (np.load(..., allow_pickle=False) now works)\n",
        "            plots = render_plots(ds_id, final_data_path)\n",
        "\n",
        "            records.append({\n",
        "                \"dataset_id\": ds_id,\n",
        "                \"distribution\": dist.name,\n",
        "                \"n\": int(n_samples),\n",
        "                \"heavy_tailed\": bool(label),\n",
        "                \"data_path\": str(final_data_path),\n",
        "                \"params\": params,\n",
        "                \"plots\": plots,\n",
        "            })\n",
        "\n",
        "    return records"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Plotting Helpers for Distribution Diagnostics\n",
        "\n",
        "This cell defines helper functions for preprocessing numerical data and generating diagnostic plots used to analyze distributional properties and tail behavior.\n",
        "\n",
        "- **`_sanitize`**: Removes NaN and infinite values from an input array and flattens it to a one-dimensional representation.\n",
        "- **`_downsample`**: Randomly reduces the number of data points to a specified maximum using an explicit RNG, improving performance for large datasets.\n",
        "- **`_clamp_by_percentile`**: Restricts the data range based on lower and upper percentiles to limit the influence of extreme outliers.\n",
        "- **`plot_zipf`**: Generates a Zipf plot on a log–log scale, visualizing the relationship between data ranks and sorted absolute values.\n",
        "- **`plot_me`**: Creates a Mean Excess (ME) plot, showing the expected excess above varying thresholds to assess heavy-tailed behavior.\n",
        "- **`plot_qq_exp`**: Generates an exponential Q–Q plot, comparing empirical quantiles of the data against theoretical quantiles of an Exp(1) distribution."
      ],
      "metadata": {
        "id": "YnnRjZA0rWvU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24f7ba70"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _sanitize(x):\n",
        "    x = np.asarray(x).ravel()\n",
        "    return x[~np.isnan(x) & ~np.isinf(x)]\n",
        "\n",
        "def _downsample(x, max_points=50_000, rng=None):\n",
        "    n = x.shape[0]\n",
        "    if n <= max_points:\n",
        "        return x\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    idx = rng.choice(n, size=max_points, replace=False)\n",
        "    return x[idx]\n",
        "\n",
        "def _clamp_by_percentile(x, pct=99.5):\n",
        "    lo = np.nanpercentile(x, 100 - pct)\n",
        "    hi = np.nanpercentile(x, pct)\n",
        "    if not np.isfinite(lo): lo = np.nanmin(x)\n",
        "    if not np.isfinite(hi): hi = np.nanmax(x)\n",
        "    x = np.clip(x, lo, hi)\n",
        "    return x, (lo, hi)\n",
        "\n",
        "# --- existing plot functions; only change: use _downsample(x, rng=...) ---\n",
        "def plot_zipf(x, save_path, rng=None):\n",
        "    x = _sanitize(np.abs(x))\n",
        "    x = _downsample(x, rng=rng)\n",
        "    x = np.sort(x)[::-1]\n",
        "    ranks = np.arange(1, x.size + 1)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.loglog(ranks, x, marker=\".\", linewidth=0)\n",
        "    ax.set_xlabel(\"rank\"); ax.set_ylabel(\"|x| (sorted)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_me(x, save_path, n_bins=200, rng=None):\n",
        "    x = _sanitize(np.abs(x))\n",
        "    x = _downsample(x, rng=rng)\n",
        "    x, (lo, hi) = _clamp_by_percentile(x, 99.5)\n",
        "    xs = np.sort(x)\n",
        "    us = np.linspace(lo, hi, n_bins)\n",
        "    e_vals = []\n",
        "    for u in us:\n",
        "        exceed = xs[xs > u]\n",
        "        e_vals.append(np.mean(exceed - u) if exceed.size else np.nan)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(us, e_vals, marker=\".\", linewidth=1)\n",
        "    ax.set_xlabel(\"u\"); ax.set_ylabel(\"e(u)\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n",
        "\n",
        "def plot_qq_exp(x, save_path, rng=None):\n",
        "    \"\"\"\n",
        "    Exponential QQ-plot: compare sample quantiles of |x| to theoretical exponential(1) quantiles.\n",
        "    \"\"\"\n",
        "    x = _sanitize(np.abs(x))\n",
        "    x = _downsample(x, rng=rng)\n",
        "    x = np.sort(x)\n",
        "    n = x.size\n",
        "    if n == 0:\n",
        "        # create an empty plot if needed\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.set_title(\"Empty sample\")\n",
        "        plt.tight_layout(); plt.savefig(save_path); plt.close(fig); return\n",
        "    # theoretical quantiles for Exp(1): F^{-1}(p) = -ln(1-p)\n",
        "    p = (np.arange(1, n + 1) - 0.5) / n\n",
        "    q_theory = -np.log1p(-p)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(q_theory, x, marker=\".\", linewidth=0)\n",
        "    ax.set_xlabel(\"Exp(1) quantiles\"); ax.set_ylabel(\"Sample |x| quantiles\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close(fig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Data Augmentation Functions (Explicit RNG)\n",
        "\n",
        "This cell defines simple augmentation functions that are applied to the generated datasets to increase robustness and variability during model training.  \n",
        "All functions support an explicit random number generator to ensure reproducibility and consistent interfaces.\n",
        "\n",
        "- **`jitter`**: Adds small Gaussian noise to the data, scaled proportionally to the sample’s standard deviation.\n",
        "- **`bootstrap_resample`**: Creates a new sample by drawing values with replacement from the original data.\n",
        "- **`slight_scale`**: Applies a deterministic scaling factor to the data; the RNG argument is included for a uniform function signature."
      ],
      "metadata": {
        "id": "wz8uK1gXrxLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e34b0737"
      },
      "outputs": [],
      "source": [
        "# === Augmentations (explicit RNG) ===\n",
        "import numpy as np\n",
        "\n",
        "def jitter(x, scale=0.02, rng=None):\n",
        "    \"\"\"\n",
        "    Add small Gaussian noise proportional to the sample std.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    x = np.asarray(x).ravel().astype(np.float32)\n",
        "    s = float(np.std(x)) if np.std(x) > 0 else 1.0\n",
        "    return x + rng.normal(0.0, scale * s, size=len(x)).astype(np.float32)\n",
        "\n",
        "def bootstrap_resample(x, rng=None):\n",
        "    \"\"\"\n",
        "    Sample with replacement from x.\n",
        "    \"\"\"\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "    x = np.asarray(x).ravel()\n",
        "    idx = rng.integers(0, len(x), size=len(x))\n",
        "    return x[idx].astype(np.float32, copy=False)\n",
        "\n",
        "def slight_scale(x, factor=1.05, rng=None):\n",
        "    \"\"\"\n",
        "    Deterministic scaling; rng included for a uniform signature.\n",
        "    \"\"\"\n",
        "    x = np.asarray(x).ravel()\n",
        "    return (x * factor).astype(np.float32, copy=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Plot Rendering & Metadata Export\n",
        "\n",
        "This cell defines functions for rendering diagnostic plots for each dataset (including augmentations) and for exporting the collected metadata in structured formats.\n",
        "\n",
        "- **`LOCAL_TMP`**: Local staging directory used for temporary files to avoid Google Drive latency during generation.\n",
        "- **`render_plots`**: Creates and saves Zipf, Mean Excess (ME), and exponential Q–Q plots for a dataset and its augmentations, returning a structured dictionary of plot file paths.\n",
        "- **`rng_for` Usage**: Derives deterministic RNG streams for downsampling, augmentation sampling, and augmentation plotting to keep results controlled and reproducible.\n",
        "- **`aug_streams` / `aug_data`**: Builds separate augmentation variants (`jitter`, `bootstrap`, `scale105`) using independent RNG streams and then generates plots for each variant.\n",
        "- **`write_metadata`**: Writes the collected dataset metadata to both JSON (nested, full record) and CSV (flat view) formats.\n",
        "- **`SEED`**: Stores the run seed inside the metadata outputs to keep runs traceable even when using fresh randomness per execution."
      ],
      "metadata": {
        "id": "ichJp4vjsXTz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "587530fb"
      },
      "outputs": [],
      "source": [
        "# === Local temp dir (RAM-friendly, no Drive lag) ===\n",
        "from pathlib import Path\n",
        "import json, csv\n",
        "LOCAL_TMP = Path(\"/content/ds_tmp\")\n",
        "LOCAL_TMP.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def render_plots(ds_id: str, npy_path: Path):\n",
        "    \"\"\"\n",
        "    Render and save plots for a dataset.\n",
        "    Uses derived RNG streams for downsampling and augmentations to keep runs controlled.\n",
        "    \"\"\"\n",
        "    ds_dir = PLOTS_DIR / ds_id\n",
        "    ds_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    x = np.load(npy_path, mmap_mode=\"r\", allow_pickle=False)\n",
        "\n",
        "    paths = {}\n",
        "\n",
        "    # ORIGINAL\n",
        "    zipf_p = ds_dir / \"zipf.png\"\n",
        "    me_p   = ds_dir / \"me.png\"\n",
        "    qq_p   = ds_dir / \"qq_exp.png\"\n",
        "\n",
        "    # derive RNG for original downsampling (plotting)\n",
        "    rng_down = rng_for(SEED, \"render_plots\", ds_id, \"downsample\")\n",
        "    plot_zipf(x, zipf_p, rng=rng_down)\n",
        "    plot_me(x,   me_p,   rng=rng_down)\n",
        "    plot_qq_exp(x, qq_p, rng=rng_down)\n",
        "\n",
        "    paths[\"original\"] = {\"zipf\": str(zipf_p), \"me\": str(me_p), \"qq_exp\": str(qq_p)}\n",
        "\n",
        "    # AUGMENTATIONS: take a manageable subsample first (with explicit RNG)\n",
        "    sample_n = min(50_000, x.shape[0])\n",
        "    rng_pick = rng_for(SEED, \"render_plots\", ds_id, \"pick_for_aug\")\n",
        "    idx = rng_pick.integers(0, x.shape[0], size=sample_n)\n",
        "    x_small = np.asarray(x[idx])\n",
        "\n",
        "    paths[\"aug\"] = {}\n",
        "    # independent RNG streams per augmentation\n",
        "    aug_streams = {\n",
        "        \"jitter\":    rng_for(SEED, \"render_plots\", ds_id, \"aug\", \"jitter\"),\n",
        "        \"bootstrap\": rng_for(SEED, \"render_plots\", ds_id, \"aug\", \"bootstrap\"),\n",
        "        \"scale105\":  rng_for(SEED, \"render_plots\", ds_id, \"aug\", \"scale105\"),\n",
        "    }\n",
        "    aug_data = {\n",
        "        \"jitter\":    jitter(x_small, rng=aug_streams[\"jitter\"]),\n",
        "        \"bootstrap\": bootstrap_resample(x_small, rng=aug_streams[\"bootstrap\"]),\n",
        "        \"scale105\":  slight_scale(x_small, 1.05, rng=aug_streams[\"scale105\"]),\n",
        "    }\n",
        "\n",
        "    # plotting RNGs per augmentation\n",
        "    for name, arr in aug_data.items():\n",
        "        z_a = ds_dir / f\"zipf_{name}.png\"\n",
        "        m_a = ds_dir / f\"me_{name}.png\"\n",
        "        q_a = ds_dir / f\"qq_exp_{name}.png\"\n",
        "\n",
        "        rng_plot = rng_for(SEED, \"render_plots\", ds_id, \"plot_aug\", name)\n",
        "        plot_zipf(arr, z_a, rng=rng_plot)\n",
        "        plot_me(arr,   m_a, rng=rng_plot)\n",
        "        plot_qq_exp(arr, q_a, rng=rng_plot)\n",
        "\n",
        "        paths.setdefault(\"aug\", {})[name] = {\"zipf\": str(z_a), \"me\": str(m_a), \"qq_exp\": str(q_a)}\n",
        "\n",
        "    return paths\n",
        "\n",
        "def write_metadata(records):\n",
        "    \"\"\"\n",
        "    Write metadata to JSON and CSV; include the run SEED so results are traceable even with fresh randomness.\n",
        "    \"\"\"\n",
        "    META_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    meta_json = META_DIR / \"datasets_metadata.json\"\n",
        "    meta_csv  = META_DIR / \"datasets_metadata.csv\"\n",
        "\n",
        "    # JSON\n",
        "    payload = {\n",
        "        \"seed\": SEED,\n",
        "        \"count\": len(records),\n",
        "        \"records\": records,\n",
        "    }\n",
        "    with open(meta_json, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # CSV (flat view)\n",
        "    cols = [\"dataset_id\",\"distribution\",\"n\",\"heavy_tailed\",\"data_path\",\"params_json\",\n",
        "            \"plot_zipf\",\"plot_me\",\"plot_qq_exp\",\n",
        "            \"plot_zipf_jitter\",\"plot_me_jitter\",\"plot_qq_jitter\",\n",
        "            \"plot_zipf_bootstrap\",\"plot_me_bootstrap\",\"plot_qq_bootstrap\",\n",
        "            \"plot_zipf_scale105\",\"plot_me_scale105\",\"plot_qq_scale105\",\n",
        "            \"seed\"]\n",
        "    with open(meta_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f); w.writerow(cols)\n",
        "        for r in records:\n",
        "            p = r[\"plots\"]\n",
        "            w.writerow([\n",
        "                r[\"dataset_id\"], r[\"distribution\"], r[\"n\"], int(r[\"heavy_tailed\"]), r[\"data_path\"],\n",
        "                json.dumps(r[\"params\"], ensure_ascii=False),\n",
        "                p[\"original\"][\"zipf\"], p[\"original\"][\"me\"], p[\"original\"][\"qq_exp\"],\n",
        "                p[\"aug\"][\"jitter\"][\"zipf\"], p[\"aug\"][\"jitter\"][\"me\"], p[\"aug\"][\"jitter\"][\"qq_exp\"],\n",
        "                p[\"aug\"][\"bootstrap\"][\"zipf\"], p[\"aug\"][\"bootstrap\"][\"me\"], p[\"aug\"][\"bootstrap\"][\"qq_exp\"],\n",
        "                p[\"aug\"][\"scale105\"][\"zipf\"], p[\"aug\"][\"scale105\"][\"me\"], p[\"aug\"][\"scale105\"][\"qq_exp\"],\n",
        "                SEED,\n",
        "            ])\n",
        "    print(\"Wrote metadata:\", meta_json, \"and\", meta_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Data Generation Pipeline\n",
        "\n",
        "This cell serves as the main execution entry point for data generation. It orchestrates the creation of datasets, rendering of diagnostic plots, and persistence of metadata.\n",
        "\n",
        "- **`build_corpus`**: Generates datasets for all configured distributions, including sampling, labeling (heavy-tailed vs. not), plot rendering, and artifact storage.\n",
        "  - **`n_per_dist`**: Number of datasets generated per distribution.\n",
        "  - **`n_samples`**: Number of samples per dataset.\n",
        "  - **`chunk_size`**: Chunk size used when writing large `.npy` files to control memory usage.\n",
        "- **`write_metadata`**: Persists dataset metadata to JSON and CSV files, including distribution parameters, labels, plot paths, and the run seed.\n",
        "- **`SEED`**: Printed at the end of execution to ensure the run can be traced and reproduced if needed.\n",
        "- **Artifacts Location**: All generated datasets, plots, and metadata are stored under **`BASE_DIR`**.\n",
        "\n",
        "This cell should be executed once all helper functions and configurations are defined."
      ],
      "metadata": {
        "id": "gXlB62fztCez"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cfb39a4",
        "outputId": "93c55838-f5b2-4ea7-d40b-aa0a5956ed77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating: normal\n",
            "Generating: exponential\n",
            "Generating: pareto\n",
            "Generating: student_t\n",
            "Generating: lognormal\n",
            "Generating: mix_norm_pareto\n",
            "Wrote metadata: /content/drive/MyDrive/Generated Data for Data science project/metadata/datasets_metadata.json and /content/drive/MyDrive/Generated Data for Data science project/metadata/datasets_metadata.csv\n",
            "SEED used for this run: 37280314975076901032992437105419038345\n",
            "Data generation complete. Artifacts in: /content/drive/MyDrive/Generated Data for Data science project\n"
          ]
        }
      ],
      "source": [
        "# === Run data generation ===\n",
        "records = build_corpus(\n",
        "    n_per_dist=200,\n",
        "    n_samples=2000,\n",
        "    chunk_size=250_000,\n",
        ")\n",
        "write_metadata(records)\n",
        "print(\"SEED used for this run:\", SEED)\n",
        "print(\"Data generation complete. Artifacts in:\", BASE_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}